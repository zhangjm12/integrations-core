{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Agent Integrations Welcome to the wonderful world of developing Agent Integrations for Datadog. Here we document how we do things, the processes for various tasks, coding conventions & best practices, the internals of our testing infrastructure, and so much more. If you are intrigued, continue reading. If not, continue all the same Getting started \u00b6 To work on any integration (a.k.a. Check ), you must setup your development environment. After that you may immediately begin testing or read through the best practices we strive to follow. Also, feel free to check out how ddev works and browse the API reference of the base package. Navigation \u00b6 Desktop readers can use keyboard shortcuts to navigate. Keys Action , (comma) p Navigate to the \"previous\" page . (period) n Navigate to the \"next\" page / s Display the search modal","title":"Home"},{"location":"#getting-started","text":"To work on any integration (a.k.a. Check ), you must setup your development environment. After that you may immediately begin testing or read through the best practices we strive to follow. Also, feel free to check out how ddev works and browse the API reference of the base package.","title":"Getting started"},{"location":"#navigation","text":"Desktop readers can use keyboard shortcuts to navigate. Keys Action , (comma) p Navigate to the \"previous\" page . (period) n Navigate to the \"next\" page / s Display the search modal","title":"Navigation"},{"location":"e2e/","text":"E2E Any integration that makes use of our pytest plugin in its test suite supports end-to-end testing on a live Datadog Agent . The entrypoint for E2E management is the command group ddev env . Discovery \u00b6 Use the ls command to see what environments are available, for example: $ ddev env ls envoy envoy: py27 py38 You'll notice that only environments that actually run tests are available. Running simply ddev env ls with no arguments will display the active environments. Creation \u00b6 To start an environment run ddev env start <INTEGRATION> <ENVIRONMENT> , for example: $ ddev env start envoy py38 Setting up environment `py38`... success! Updating `datadog/agent-dev:master`... success! Detecting the major version... Agent 7 detected Writing configuration for `py38`... success! Starting the Agent... success! Config file (copied to your clipboard): C:\\Users\\ofek\\AppData\\Local\\dd-checks-dev\\envs\\envoy\\py38\\config\\envoy.yaml To run this check, do: ddev env check envoy py38 To stop this check, do: ddev env stop envoy py38 This sets up the selected environment and an instance of the Agent running in a Docker container. The default configuration is defined by each environment's test suite and is saved to a file, which is then mounted to the Agent container so you may freely modify it. Let's see what we have running: $ docker ps --format \"table {{.Image}}\\t{{.Status}}\\t{{.Ports}}\\t{{.Names}}\" IMAGE STATUS PORTS NAMES datadog/agent-dev:master-py3 Up 4 seconds (health: starting) dd_envoy_py38 default_service2 Up 5 seconds 80/tcp, 10000/tcp default_service2_1 envoyproxy/envoy:latest Up 5 seconds 0.0.0.0:8001->8001/tcp, 10000/tcp, 0.0.0.0:8000->80/tcp default_front-envoy_1 default_xds Up 5 seconds 8080/tcp default_xds_1 default_service1 Up 5 seconds 80/tcp, 10000/tcp default_service1_1 Agent version \u00b6 You can select a particular build of the Agent to use with the --agent / -a option. Any Docker image is valid e.g. datadog/agent:7.17.0 . A custom nightly build will be used by default, which is re-built on every commit to the Datadog Agent repository . Integration version \u00b6 By default the version of the integration used will be the one shipped with the chosen Agent version, as if you had passed in the --prod flag. If you wish to modify an integration and test changes in real time, use the --dev flag. Doing so will mount and install the integration in the Agent container. All modifications to the integration's directory will be propagated to the Agent, whether it be a code change or switching to a different Git branch. If you modify the base package then you will need to mount that with the --base flag, which implicitly activates --dev . Testing \u00b6 To run tests against the live Agent, use the ddev env test command. It is similar to the test command except it is capable of running tests marked as E2E , and only runs such tests. Automation \u00b6 You can use the --new-env / -ne flag to automate environment management. For example running: ddev env test apache:py38 vault:py38 -ne will start the py38 environment for Apache, run E2E tests, tear down the environment, and then do the same for Vault. Tip Since running tests implies code changes are being introduced, --new-env enables --dev by default. Execution \u00b6 Similar to the Agent's check command, you can perform manual check runs using ddev env check <INTEGRATION> <ENVIRONMENT> , for example: $ ddev env check envoy py38 --log-level debug ... ========= Collector ========= Running Checks ============== envoy (1.12.0) -------------- Instance ID: envoy:c705bd922a3c275c [OK] Configuration Source: file:/etc/datadog-agent/conf.d/envoy.d/envoy.yaml Total Runs: 1 Metric Samples: Last Run: 546, Total: 546 Events: Last Run: 0, Total: 0 Service Checks: Last Run: 1, Total: 1 Average Execution Time : 25ms Last Execution Date : 2020-02-17 00:58:05.000000 UTC Last Successful Execution Date : 2020-02-17 00:58:05.000000 UTC Debugging \u00b6 You may start an interactive debugging session using the --breakpoint / -b option. The option accepts an integer representing the line number at which to break. For convenience, 0 and -1 are shortcuts to the first and last line of the integration's check method, respectively. $ ddev env check envoy py38 -b 0 > /opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/envoy/envoy.py(34)check() -> custom_tags = instance.get('tags', []) (Pdb) list 29 self.blacklisted_metrics = set() 30 31 self.caching_metrics = None 32 33 def check(self, instance): 34 B-> custom_tags = instance.get('tags', []) 35 36 try: 37 stats_url = instance['stats_url'] 38 except KeyError: 39 msg = 'Envoy configuration setting `stats_url` is required' (Pdb) print(instance) {'stats_url': 'http://localhost:8001/stats'} Caveat The line number must be within the integration's check method. Refreshing state \u00b6 Testing and manual check runs always reflect the current state of code and configuration however, if you want to see the result of changes in-app , you will need to refresh the environment by running ddev env reload <INTEGRATION> <ENVIRONMENT> . Removal \u00b6 To stop an environment run ddev env stop <INTEGRATION> <ENVIRONMENT> . Any environments that haven't been explicitly stopped will show as active in the output of ddev env ls , even persisting through system restarts. If you are confident that environments are no longer active, you can run ddev env prune to remove all accumulated environment state.","title":"E2E"},{"location":"e2e/#discovery","text":"Use the ls command to see what environments are available, for example: $ ddev env ls envoy envoy: py27 py38 You'll notice that only environments that actually run tests are available. Running simply ddev env ls with no arguments will display the active environments.","title":"Discovery"},{"location":"e2e/#creation","text":"To start an environment run ddev env start <INTEGRATION> <ENVIRONMENT> , for example: $ ddev env start envoy py38 Setting up environment `py38`... success! Updating `datadog/agent-dev:master`... success! Detecting the major version... Agent 7 detected Writing configuration for `py38`... success! Starting the Agent... success! Config file (copied to your clipboard): C:\\Users\\ofek\\AppData\\Local\\dd-checks-dev\\envs\\envoy\\py38\\config\\envoy.yaml To run this check, do: ddev env check envoy py38 To stop this check, do: ddev env stop envoy py38 This sets up the selected environment and an instance of the Agent running in a Docker container. The default configuration is defined by each environment's test suite and is saved to a file, which is then mounted to the Agent container so you may freely modify it. Let's see what we have running: $ docker ps --format \"table {{.Image}}\\t{{.Status}}\\t{{.Ports}}\\t{{.Names}}\" IMAGE STATUS PORTS NAMES datadog/agent-dev:master-py3 Up 4 seconds (health: starting) dd_envoy_py38 default_service2 Up 5 seconds 80/tcp, 10000/tcp default_service2_1 envoyproxy/envoy:latest Up 5 seconds 0.0.0.0:8001->8001/tcp, 10000/tcp, 0.0.0.0:8000->80/tcp default_front-envoy_1 default_xds Up 5 seconds 8080/tcp default_xds_1 default_service1 Up 5 seconds 80/tcp, 10000/tcp default_service1_1","title":"Creation"},{"location":"e2e/#agent-version","text":"You can select a particular build of the Agent to use with the --agent / -a option. Any Docker image is valid e.g. datadog/agent:7.17.0 . A custom nightly build will be used by default, which is re-built on every commit to the Datadog Agent repository .","title":"Agent version"},{"location":"e2e/#integration-version","text":"By default the version of the integration used will be the one shipped with the chosen Agent version, as if you had passed in the --prod flag. If you wish to modify an integration and test changes in real time, use the --dev flag. Doing so will mount and install the integration in the Agent container. All modifications to the integration's directory will be propagated to the Agent, whether it be a code change or switching to a different Git branch. If you modify the base package then you will need to mount that with the --base flag, which implicitly activates --dev .","title":"Integration version"},{"location":"e2e/#testing","text":"To run tests against the live Agent, use the ddev env test command. It is similar to the test command except it is capable of running tests marked as E2E , and only runs such tests.","title":"Testing"},{"location":"e2e/#automation","text":"You can use the --new-env / -ne flag to automate environment management. For example running: ddev env test apache:py38 vault:py38 -ne will start the py38 environment for Apache, run E2E tests, tear down the environment, and then do the same for Vault. Tip Since running tests implies code changes are being introduced, --new-env enables --dev by default.","title":"Automation"},{"location":"e2e/#execution","text":"Similar to the Agent's check command, you can perform manual check runs using ddev env check <INTEGRATION> <ENVIRONMENT> , for example: $ ddev env check envoy py38 --log-level debug ... ========= Collector ========= Running Checks ============== envoy (1.12.0) -------------- Instance ID: envoy:c705bd922a3c275c [OK] Configuration Source: file:/etc/datadog-agent/conf.d/envoy.d/envoy.yaml Total Runs: 1 Metric Samples: Last Run: 546, Total: 546 Events: Last Run: 0, Total: 0 Service Checks: Last Run: 1, Total: 1 Average Execution Time : 25ms Last Execution Date : 2020-02-17 00:58:05.000000 UTC Last Successful Execution Date : 2020-02-17 00:58:05.000000 UTC","title":"Execution"},{"location":"e2e/#debugging","text":"You may start an interactive debugging session using the --breakpoint / -b option. The option accepts an integer representing the line number at which to break. For convenience, 0 and -1 are shortcuts to the first and last line of the integration's check method, respectively. $ ddev env check envoy py38 -b 0 > /opt/datadog-agent/embedded/lib/python3.8/site-packages/datadog_checks/envoy/envoy.py(34)check() -> custom_tags = instance.get('tags', []) (Pdb) list 29 self.blacklisted_metrics = set() 30 31 self.caching_metrics = None 32 33 def check(self, instance): 34 B-> custom_tags = instance.get('tags', []) 35 36 try: 37 stats_url = instance['stats_url'] 38 except KeyError: 39 msg = 'Envoy configuration setting `stats_url` is required' (Pdb) print(instance) {'stats_url': 'http://localhost:8001/stats'} Caveat The line number must be within the integration's check method.","title":"Debugging"},{"location":"e2e/#refreshing-state","text":"Testing and manual check runs always reflect the current state of code and configuration however, if you want to see the result of changes in-app , you will need to refresh the environment by running ddev env reload <INTEGRATION> <ENVIRONMENT> .","title":"Refreshing state"},{"location":"e2e/#removal","text":"To stop an environment run ddev env stop <INTEGRATION> <ENVIRONMENT> . Any environments that haven't been explicitly stopped will show as active in the output of ddev env ls , even persisting through system restarts. If you are confident that environments are no longer active, you can run ddev env prune to remove all accumulated environment state.","title":"Removal"},{"location":"setup/","text":"Setup This will be relatively painless, we promise! Integrations \u00b6 You will need to clone integrations-core and/or integrations-extras depending on which integrations you intend to work on. Python \u00b6 To work on any integration you must install Python 3.8+. After installation, restart your terminal and ensure that your newly installed Python comes first in your PATH . macOS We recommend using Homebrew . First update the formulae and Homebrew itself: brew update then either install Python: brew install python or upgrade it: brew upgrade python After it completes, check the output to see if it asked you to run any extra commands and if so, execute them. Verify successful PATH modification: which -a python Windows Windows users have it the easiest. Simply download the latest x86-64 executable installer from https://www.python.org/downloads/windows and run it. When prompted, be sure to select the option to add to your PATH . Also, it is recommended that you choose the per-user installation method. Verify successful PATH modification: where python Linux Ah, you enjoy difficult things. Are you using Gentoo? We recommend using either Miniconda or pyenv . Whatever you do, never modify the system Python. Verify successful PATH modification: which -a python ddev \u00b6 Installation \u00b6 You have 2 options to install the CLI provided by the package datadog-checks-dev . Warning For either option, if you are on macOS/Linux do not use sudo ! Doing so will result in a broken installation. Development \u00b6 If you cloned integrations-core and want to always use the version based on the current branch, run: python -m pip install -e \"path/to/datadog_checks_dev[cli]\" Note Be aware that this method does not keep track of dependencies so you will need to re-run the command if/when the required dependencies are changed. Stable \u00b6 The latest released version may be installed from PyPI : python -m pip install --upgrade \"datadog-checks-dev[cli]\" Configuration \u00b6 Upon the first invocation, ddev will create its config file if it does not yet exist. You will need to set the location of each cloned repository: ddev config set <REPO> /path/to/integrations-<REPO> The <REPO> may be either core or extras . By default, the repo core will be the target of all commands. If you want to switch to integrations-extras , run: ddev config set repo extras Docker \u00b6 Docker is used in nearly every integration's test suite therefore we simply require it to avoid confusion. macOS Install Docker Desktop for Mac . Right-click the Docker taskbar item and update Preferences > File Sharing with any locations you need to open. Windows Install Docker Desktop for Windows . Right-click the Docker taskbar item and update Settings > Shared Drives with any locations you need to open e.g. C:\\ . Linux Install Docker Engine for your distribution: Ubuntu Docker CE for Ubuntu Debian Docker CE for Debian Fedora Docker CE for Fedora CentOS Docker CE for CentOS Add your user to the docker group: sudo usermod -aG docker $USER Sign out and then back in again so your changes take effect. After installation, restart your terminal one last time.","title":"Setup"},{"location":"setup/#integrations","text":"You will need to clone integrations-core and/or integrations-extras depending on which integrations you intend to work on.","title":"Integrations"},{"location":"setup/#python","text":"To work on any integration you must install Python 3.8+. After installation, restart your terminal and ensure that your newly installed Python comes first in your PATH . macOS We recommend using Homebrew . First update the formulae and Homebrew itself: brew update then either install Python: brew install python or upgrade it: brew upgrade python After it completes, check the output to see if it asked you to run any extra commands and if so, execute them. Verify successful PATH modification: which -a python Windows Windows users have it the easiest. Simply download the latest x86-64 executable installer from https://www.python.org/downloads/windows and run it. When prompted, be sure to select the option to add to your PATH . Also, it is recommended that you choose the per-user installation method. Verify successful PATH modification: where python Linux Ah, you enjoy difficult things. Are you using Gentoo? We recommend using either Miniconda or pyenv . Whatever you do, never modify the system Python. Verify successful PATH modification: which -a python","title":"Python"},{"location":"setup/#ddev","text":"","title":"ddev"},{"location":"setup/#installation","text":"You have 2 options to install the CLI provided by the package datadog-checks-dev . Warning For either option, if you are on macOS/Linux do not use sudo ! Doing so will result in a broken installation.","title":"Installation"},{"location":"setup/#development","text":"If you cloned integrations-core and want to always use the version based on the current branch, run: python -m pip install -e \"path/to/datadog_checks_dev[cli]\" Note Be aware that this method does not keep track of dependencies so you will need to re-run the command if/when the required dependencies are changed.","title":"Development"},{"location":"setup/#stable","text":"The latest released version may be installed from PyPI : python -m pip install --upgrade \"datadog-checks-dev[cli]\"","title":"Stable"},{"location":"setup/#configuration","text":"Upon the first invocation, ddev will create its config file if it does not yet exist. You will need to set the location of each cloned repository: ddev config set <REPO> /path/to/integrations-<REPO> The <REPO> may be either core or extras . By default, the repo core will be the target of all commands. If you want to switch to integrations-extras , run: ddev config set repo extras","title":"Configuration"},{"location":"setup/#docker","text":"Docker is used in nearly every integration's test suite therefore we simply require it to avoid confusion. macOS Install Docker Desktop for Mac . Right-click the Docker taskbar item and update Preferences > File Sharing with any locations you need to open. Windows Install Docker Desktop for Windows . Right-click the Docker taskbar item and update Settings > Shared Drives with any locations you need to open e.g. C:\\ . Linux Install Docker Engine for your distribution: Ubuntu Docker CE for Ubuntu Debian Docker CE for Debian Fedora Docker CE for Fedora CentOS Docker CE for CentOS Add your user to the docker group: sudo usermod -aG docker $USER Sign out and then back in again so your changes take effect. After installation, restart your terminal one last time.","title":"Docker"},{"location":"testing/","text":"Testing The entrypoint for testing any integration is the command ddev test , which accepts an arbitrary number of integrations as arguments. Under the hood, we use tox for environment management and pytest as our test framework. Discovery \u00b6 Use the --list / -l flag to see what environments are available, for example: $ ddev test postgres envoy -l postgres: py27-10 py27-11 py27-93 py27-94 py27-95 py27-96 py38-10 py38-11 py38-93 py38-94 py38-95 py38-96 format_style style envoy: py27 py38 bench format_style style You'll notice that all environments for running tests are prefixed with pyXY , indicating the Python version to use. If you don't have a particular version installed (for example Python 2.7), such environments will be skipped. The second part of a test environment's name corresponds to the version of the product. For example, the 11 in py38-11 implies tests will run against version 11.x of PostgreSQL. If there is no version suffix, it means that either: the version is pinned, usually set to pull the latest release, or there is no concept of a product, such as the disk check Usage \u00b6 Explicit \u00b6 Passing just the integration name will run every test environment e.g. executing ddev test envoy will run the environments py27 , py38 , and style . You may select a subset of environments to run by appending a : followed by a comma-separated list of environments. For example, executing: ddev test postgres:py38-11,style envoy:py38 will run, in order, the environments py38-11 and style for the PostgreSQL check and the environment py38 for the Envoy check. Detection \u00b6 If no integrations are specified then only integrations that were changed will be tested, based on a diff between the latest commit to the current and master branches. The criteria for an integration to be considered changed is based on the file extension of paths in the diff. So for example if only Markdown files were modified then nothing will be tested. The integrations will be tested in lexicographical order. Coverage \u00b6 To measure code coverage, use the --cov / -c flag. Doing so will display a summary of coverage statistics after successful execution of integrations' tests. $ ddev test tls -c ... ---------- Coverage report ---------- Name Stmts Miss Branch BrPart Cover ------------------------------------------------------------------- datadog_checks\\tls\\__about__.py 1 0 0 0 100% datadog_checks\\tls\\__init__.py 3 0 0 0 100% datadog_checks\\tls\\tls.py 185 4 50 2 97% datadog_checks\\tls\\utils.py 43 0 16 0 100% tests\\__init__.py 0 0 0 0 100% tests\\conftest.py 105 0 0 0 100% tests\\test_config.py 47 0 0 0 100% tests\\test_local.py 113 0 0 0 100% tests\\test_remote.py 189 0 2 0 100% tests\\test_utils.py 15 0 0 0 100% tests\\utils.py 36 0 2 0 100% ------------------------------------------------------------------- TOTAL 737 4 70 2 99% To also show any line numbers that were not hit, use the --cov-missing / -cm flag instead. $ ddev test tls -cm ... ---------- Coverage report ---------- Name Stmts Miss Branch BrPart Cover Missing ----------------------------------------------------------------------------- datadog_checks\\tls\\__about__.py 1 0 0 0 100% datadog_checks\\tls\\__init__.py 3 0 0 0 100% datadog_checks\\tls\\tls.py 185 4 50 2 97% 160-167, 288->275, 297->300, 300 datadog_checks\\tls\\utils.py 43 0 16 0 100% tests\\__init__.py 0 0 0 0 100% tests\\conftest.py 105 0 0 0 100% tests\\test_config.py 47 0 0 0 100% tests\\test_local.py 113 0 0 0 100% tests\\test_remote.py 189 0 2 0 100% tests\\test_utils.py 15 0 0 0 100% tests\\utils.py 36 0 2 0 100% ----------------------------------------------------------------------------- TOTAL 737 4 70 2 99% Style \u00b6 To run only the style checking environments, use the --style / -s shortcut flag. You may also only run the formatter environment using the --format-style / -fs shortcut flag. The formatter will automatically resolve the most common errors caught by the style checker. Advanced \u00b6 There are a number of shortcut options available that correspond to pytest options . --marker / -m ( pytest : -m ) - Only run tests matching a given marker expression e.g. ddev test elastic:py38-7.2 -m unit --filter / -k ( pytest : -k ) - Only run tests matching a given substring expression e.g. ddev test redisdb -k replication --debug / -d ( pytest : --log-level=debug -s ) - Set the log level to debug --pdb ( pytest : --pdb -x ) - Drop to PDB on first failure, then end test session --verbose / -v ( pytest : -v --tb=auto ) - Increase verbosity (can be used additively) and disables shortened tracebacks You may also pass arguments directly to pytest using the --pytest-args / -pa option. For example, you could re-write -d as -pa \"--log-level=debug -s\" .","title":"Testing"},{"location":"testing/#discovery","text":"Use the --list / -l flag to see what environments are available, for example: $ ddev test postgres envoy -l postgres: py27-10 py27-11 py27-93 py27-94 py27-95 py27-96 py38-10 py38-11 py38-93 py38-94 py38-95 py38-96 format_style style envoy: py27 py38 bench format_style style You'll notice that all environments for running tests are prefixed with pyXY , indicating the Python version to use. If you don't have a particular version installed (for example Python 2.7), such environments will be skipped. The second part of a test environment's name corresponds to the version of the product. For example, the 11 in py38-11 implies tests will run against version 11.x of PostgreSQL. If there is no version suffix, it means that either: the version is pinned, usually set to pull the latest release, or there is no concept of a product, such as the disk check","title":"Discovery"},{"location":"testing/#usage","text":"","title":"Usage"},{"location":"testing/#explicit","text":"Passing just the integration name will run every test environment e.g. executing ddev test envoy will run the environments py27 , py38 , and style . You may select a subset of environments to run by appending a : followed by a comma-separated list of environments. For example, executing: ddev test postgres:py38-11,style envoy:py38 will run, in order, the environments py38-11 and style for the PostgreSQL check and the environment py38 for the Envoy check.","title":"Explicit"},{"location":"testing/#detection","text":"If no integrations are specified then only integrations that were changed will be tested, based on a diff between the latest commit to the current and master branches. The criteria for an integration to be considered changed is based on the file extension of paths in the diff. So for example if only Markdown files were modified then nothing will be tested. The integrations will be tested in lexicographical order.","title":"Detection"},{"location":"testing/#coverage","text":"To measure code coverage, use the --cov / -c flag. Doing so will display a summary of coverage statistics after successful execution of integrations' tests. $ ddev test tls -c ... ---------- Coverage report ---------- Name Stmts Miss Branch BrPart Cover ------------------------------------------------------------------- datadog_checks\\tls\\__about__.py 1 0 0 0 100% datadog_checks\\tls\\__init__.py 3 0 0 0 100% datadog_checks\\tls\\tls.py 185 4 50 2 97% datadog_checks\\tls\\utils.py 43 0 16 0 100% tests\\__init__.py 0 0 0 0 100% tests\\conftest.py 105 0 0 0 100% tests\\test_config.py 47 0 0 0 100% tests\\test_local.py 113 0 0 0 100% tests\\test_remote.py 189 0 2 0 100% tests\\test_utils.py 15 0 0 0 100% tests\\utils.py 36 0 2 0 100% ------------------------------------------------------------------- TOTAL 737 4 70 2 99% To also show any line numbers that were not hit, use the --cov-missing / -cm flag instead. $ ddev test tls -cm ... ---------- Coverage report ---------- Name Stmts Miss Branch BrPart Cover Missing ----------------------------------------------------------------------------- datadog_checks\\tls\\__about__.py 1 0 0 0 100% datadog_checks\\tls\\__init__.py 3 0 0 0 100% datadog_checks\\tls\\tls.py 185 4 50 2 97% 160-167, 288->275, 297->300, 300 datadog_checks\\tls\\utils.py 43 0 16 0 100% tests\\__init__.py 0 0 0 0 100% tests\\conftest.py 105 0 0 0 100% tests\\test_config.py 47 0 0 0 100% tests\\test_local.py 113 0 0 0 100% tests\\test_remote.py 189 0 2 0 100% tests\\test_utils.py 15 0 0 0 100% tests\\utils.py 36 0 2 0 100% ----------------------------------------------------------------------------- TOTAL 737 4 70 2 99%","title":"Coverage"},{"location":"testing/#style","text":"To run only the style checking environments, use the --style / -s shortcut flag. You may also only run the formatter environment using the --format-style / -fs shortcut flag. The formatter will automatically resolve the most common errors caught by the style checker.","title":"Style"},{"location":"testing/#advanced","text":"There are a number of shortcut options available that correspond to pytest options . --marker / -m ( pytest : -m ) - Only run tests matching a given marker expression e.g. ddev test elastic:py38-7.2 -m unit --filter / -k ( pytest : -k ) - Only run tests matching a given substring expression e.g. ddev test redisdb -k replication --debug / -d ( pytest : --log-level=debug -s ) - Set the log level to debug --pdb ( pytest : --pdb -x ) - Drop to PDB on first failure, then end test session --verbose / -v ( pytest : -v --tb=auto ) - Increase verbosity (can be used additively) and disables shortened tracebacks You may also pass arguments directly to pytest using the --pytest-args / -pa option. For example, you could re-write -d as -pa \"--log-level=debug -s\" .","title":"Advanced"},{"location":"base/about/","text":"About The package datadog-checks-base provides all the functionality and utilities necessary for writing Agent Integrations. Most importantly it provides the AgentCheck base class from which every Check must be inherited. You would use it like so: from datadog_checks.base import AgentCheck class AwesomeCheck ( AgentCheck ): __NAMESPACE__ = 'awesome' def check ( self , instance ): self . gauge ( 'test' , 1.23 , tags = [ 'foo:bar' ]) The check method is what the Datadog Agent will execute. In this example we created a Check and gave it a namespace of awesome . This means that by default, every submission's name will be prefixed with awesome. . We submitted a gauge metric named awesome.test with a value of 1.23 tagged by foo:bar . The magic hidden by the usability of the API is that this actually calls a C binding which communicates with the Agent (written in Go).","title":"About"},{"location":"base/api/","text":"API datadog_checks.base.checks.base.AgentCheck \u00b6 The base class for any Agent based integration. In general, you don't need to and you should not override anything from the base class except the check method but sometimes it might be useful for a Check to have its own constructor. When overriding __init__ you have to remember that, depending on the configuration, the Agent might create several different Check instances and the method would be called as many times. Agent 6,7 signature: AgentCheck(name, init_config, instances) # instances contain only 1 instance AgentCheck.check(instance) Agent 8 signature: AgentCheck(name, init_config, instance) # one instance AgentCheck.check() # no more instance argument for check method Note when loading a Custom check, the Agent will inspect the module searching for a subclass of AgentCheck . If such a class exists but has been derived in turn, it'll be ignored - you should never derive from an existing Check . __init__ ( self , * args , ** kwargs ) special \u00b6 name ( str ) - the name of the check init_config ( dict ) - the init_config section of the configuration. instance ( List[dict] ) - a one-element list containing the instance options from the configuration file (a list is used to keep backward compatibility with older versions of the Agent). Source code in datadog_checks\\base\\checks\\base.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def __init__ ( self , * args , ** kwargs ): # type: (*Any, **Any) -> None \"\"\" - **name** (_str_) - the name of the check - **init_config** (_dict_) - the `init_config` section of the configuration. - **instance** (_List[dict]_) - a one-element list containing the instance options from the configuration file (a list is used to keep backward compatibility with older versions of the Agent). \"\"\" # NOTE: these variable assignments exist to ease type checking when eventually assigned as attributes. name = kwargs . get ( 'name' , '' ) init_config = kwargs . get ( 'init_config' , {}) agentConfig = kwargs . get ( 'agentConfig' , {}) instances = kwargs . get ( 'instances' , []) if len ( args ) > 0 : name = args [ 0 ] if len ( args ) > 1 : init_config = args [ 1 ] if len ( args ) > 2 : # agent pass instances as tuple but in test we are usually using list, so we are testing for both if len ( args ) > 3 or not isinstance ( args [ 2 ], ( list , tuple )) or 'instances' in kwargs : # old-style init: the 3rd argument is `agentConfig` agentConfig = args [ 2 ] if len ( args ) > 3 : instances = args [ 3 ] else : # new-style init: the 3rd argument is `instances` instances = args [ 2 ] # NOTE: Agent 6+ should pass exactly one instance... But we are not abiding by that rule on our side # everywhere just yet. It's complicated... See: https://github.com/DataDog/integrations-core/pull/5573 instance = instances [ 0 ] if instances else None self . check_id = '' self . name = name # type: str self . init_config = init_config # type: InitConfigType self . agentConfig = agentConfig # type: AgentConfigType self . instance = instance # type: Optional[InstanceType] self . instances = instances # type: List[InstanceType] self . warnings = [] # type: List[str] self . metrics = defaultdict ( list ) # type: DefaultDict[str, List[str]] # `self.hostname` is deprecated, use `datadog_agent.get_hostname()` instead self . hostname = datadog_agent . get_hostname () # type: str logger = logging . getLogger ( ' {} . {} ' . format ( __name__ , self . name )) self . log = CheckLoggingAdapter ( logger , self ) # TODO: Remove with Agent 5 # Set proxy settings self . proxies = self . _get_requests_proxy () if not self . init_config : self . _use_agent_proxy = True else : self . _use_agent_proxy = is_affirmative ( self . init_config . get ( 'use_agent_proxy' , True )) # TODO: Remove with Agent 5 self . default_integration_http_timeout = float ( self . agentConfig . get ( 'default_integration_http_timeout' , 9 )) self . _deprecations = { 'increment' : ( False , ( 'DEPRECATION NOTICE: `AgentCheck.increment`/`AgentCheck.decrement` are deprecated, please ' 'use `AgentCheck.gauge` or `AgentCheck.count` instead, with a different metric name' ), ), 'device_name' : ( False , ( 'DEPRECATION NOTICE: `device_name` is deprecated, please use a `device:` ' 'tag in the `tags` list instead' ), ), 'in_developer_mode' : ( False , 'DEPRECATION NOTICE: `in_developer_mode` is deprecated, please stop using it.' , ), 'no_proxy' : ( False , ( 'DEPRECATION NOTICE: The `no_proxy` config option has been renamed ' 'to `skip_proxy` and will be removed in Agent version 6.13.' ), ), 'service_tag' : ( False , ( 'DEPRECATION NOTICE: The `service` tag is deprecated and has been renamed to ` %s `. ' 'Set `disable_legacy_service_tag` to `true` to disable this warning. ' 'The default will become `true` and cannot be changed in Agent version 8.' ), ), } # type: Dict[str, Tuple[bool, str]] # Setup metric limits self . metric_limiter = self . _get_metric_limiter ( self . name , instance = self . instance ) # Functions that will be called exactly once (if successful) before the first check run self . check_initializations = deque ([ self . send_config_metadata ]) # type: Deque[Callable[[], None]] count ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ) \u00b6 Sample a raw count metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def count ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a raw count metric. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . COUNT , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw ) event ( self , event ) \u00b6 Send an event. An event is a dictionary with the following keys and data types: { \"timestamp\" : int , # the epoch timestamp for the event \"event_type\" : str , # the event name \"api_key\" : str , # the api key for your account \"msg_title\" : str , # the title of the event \"msg_text\" : str , # the text body of the event \"aggregation_key\" : str , # a key to use for aggregating events \"alert_type\" : str , # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info' \"source_type_name\" : str , # (optional) the source type name \"host\" : str , # (optional) the name of the host \"tags\" : list , # (optional) a list of tags to associate with this event \"priority\" : str , # (optional) specifies the priority of the event (\"normal\" or \"low\") } event ( dict ) - the event to be sent Source code in datadog_checks\\base\\checks\\base.py 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 def event ( self , event ): # type: (Event) -> None \"\"\"Send an event. An event is a dictionary with the following keys and data types: ```python { \"timestamp\": int, # the epoch timestamp for the event \"event_type\": str, # the event name \"api_key\": str, # the api key for your account \"msg_title\": str, # the title of the event \"msg_text\": str, # the text body of the event \"aggregation_key\": str, # a key to use for aggregating events \"alert_type\": str, # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info' \"source_type_name\": str, # (optional) the source type name \"host\": str, # (optional) the name of the host \"tags\": list, # (optional) a list of tags to associate with this event \"priority\": str, # (optional) specifies the priority of the event (\"normal\" or \"low\") } ``` - **event** (_dict_) - the event to be sent \"\"\" # Enforce types of some fields, considerably facilitates handling in go bindings downstream for key , value in iteritems ( event ): if not isinstance ( value , ( text_type , binary_type )): continue try : event [ key ] = to_native_string ( value ) # type: ignore # ^ Mypy complains about dynamic key assignment -- arguably for good reason. # Ideally we should convert this to a dict literal so that submitted events only include known keys. except UnicodeError : self . log . warning ( 'Encoding error with field ` %s `, cannot submit event' , key ) return if event . get ( 'tags' ): event [ 'tags' ] = self . _normalize_tags_type ( event [ 'tags' ]) if event . get ( 'timestamp' ): event [ 'timestamp' ] = int ( event [ 'timestamp' ]) if event . get ( 'aggregation_key' ): event [ 'aggregation_key' ] = to_native_string ( event [ 'aggregation_key' ]) if self . __NAMESPACE__ : event . setdefault ( 'source_type_name' , self . __NAMESPACE__ ) aggregator . submit_event ( self , self . check_id , event ) gauge ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ) \u00b6 Sample a gauge metric. Parameters: name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def gauge ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a gauge metric. **Parameters:** - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . GAUGE , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw ) histogram ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ) \u00b6 Sample a histogram metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 def histogram ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a histogram metric. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . HISTOGRAM , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw ) historate ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ) \u00b6 Sample a histogram based on rate metrics. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 def historate ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a histogram based on rate metrics. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . HISTORATE , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw ) monotonic_count ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ) \u00b6 Sample an increasing counter metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def monotonic_count ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample an increasing counter metric. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . MONOTONIC_COUNT , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw ) rate ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ) \u00b6 Sample a point, with the rate calculated at the end of the check. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def rate ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a point, with the rate calculated at the end of the check. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . RATE , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw ) service_check ( self , name , status , tags = None , hostname = None , message = None , raw = False ) \u00b6 Send the status of a service. name ( str ) - the name of the service check status ( int ) - a constant describing the service status. tags ( List[str]) ) - a list of tags to associate with this service check message ( str ) - additional information or a description of why this status occurred. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 def service_check ( self , name , status , tags = None , hostname = None , message = None , raw = False ): # type: (str, ServiceCheckStatus, Sequence[str], str, str, bool) -> None \"\"\"Send the status of a service. - **name** (_str_) - the name of the service check - **status** (_int_) - a constant describing the service status. - **tags** (_List[str])_) - a list of tags to associate with this service check - **message** (_str_) - additional information or a description of why this status occurred. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" tags = self . _normalize_tags_type ( tags or []) if hostname is None : hostname = '' if message is None : message = '' else : message = to_native_string ( message ) message = self . sanitize ( message ) aggregator . submit_service_check ( self , self . check_id , self . _format_namespace ( name , raw ), status , tags , hostname , message ) Stubs \u00b6 datadog_checks.base.stubs.aggregator.AggregatorStub \u00b6 Mainly used for unit testing checks, this stub makes possible to execute a check without a running Agent. assert_all_metrics_covered ( self ) \u00b6 Source code in datadog_checks\\base\\stubs\\aggregator.py 293 294 295 296 297 298 299 300 301 302 def assert_all_metrics_covered ( self ): # use `condition` to avoid building the `msg` if not needed condition = self . metrics_asserted_pct >= 100.0 msg = '' if not condition : prefix = ' \\n\\t - ' msg = 'Some metrics are missing:' msg += ' \\n Asserted Metrics: {}{} ' . format ( prefix , prefix . join ( sorted ( self . _asserted ))) msg += ' \\n Missing Metrics: {}{} ' . format ( prefix , prefix . join ( sorted ( self . not_asserted ()))) assert condition , msg assert_event ( self , msg_text , count = None , at_least = 1 , exact_match = True , tags = None , ** kwargs ) \u00b6 Source code in datadog_checks\\base\\stubs\\aggregator.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def assert_event ( self , msg_text , count = None , at_least = 1 , exact_match = True , tags = None , ** kwargs ): candidates = [] for e in self . events : if exact_match and msg_text != e [ 'msg_text' ] or msg_text not in e [ 'msg_text' ]: continue if tags and set ( tags ) != set ( e [ 'tags' ]): continue for name , value in iteritems ( kwargs ): if e [ name ] != value : break else : candidates . append ( e ) msg = \"Candidates size assertion for ` {} `, count: {} , at_least: {} ) failed\" . format ( msg_text , count , at_least ) if count is not None : assert len ( candidates ) == count , msg else : assert len ( candidates ) >= at_least , msg assert_metric ( self , name , value = None , tags = None , count = None , at_least = 1 , hostname = None , metric_type = None , device = None ) \u00b6 Assert a metric was processed by this stub Source code in datadog_checks\\base\\stubs\\aggregator.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def assert_metric ( self , name , value = None , tags = None , count = None , at_least = 1 , hostname = None , metric_type = None , device = None ): \"\"\" Assert a metric was processed by this stub \"\"\" self . _asserted . add ( name ) expected_tags = normalize_tags ( tags , sort = True ) candidates = [] for metric in self . metrics ( name ): if value is not None and not self . is_aggregate ( metric . type ) and value != metric . value : continue if expected_tags and expected_tags != sorted ( metric . tags ): continue if hostname and hostname != metric . hostname : continue if metric_type is not None and metric_type != metric . type : continue if device is not None and device != metric . device : continue candidates . append ( metric ) expected_metric = MetricStub ( name , metric_type , value , tags , hostname , device ) if value is not None and candidates and all ( self . is_aggregate ( m . type ) for m in candidates ): got = sum ( m . value for m in candidates ) msg = \"Expected count value for ' {} ': {} , got {} \" . format ( name , value , got ) condition = value == got elif count is not None : msg = \"Needed exactly {} candidates for ' {} ', got {} \" . format ( count , name , len ( candidates )) condition = len ( candidates ) == count else : msg = \"Needed at least {} candidates for ' {} ', got {} \" . format ( at_least , name , len ( candidates )) condition = len ( candidates ) >= at_least self . _assert ( condition , msg = msg , expected_stub = expected_metric , submitted_elements = self . _metrics ) assert_metric_has_tag ( self , metric_name , tag , count = None , at_least = 1 ) \u00b6 Assert a metric is tagged with tag Source code in datadog_checks\\base\\stubs\\aggregator.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def assert_metric_has_tag ( self , metric_name , tag , count = None , at_least = 1 ): \"\"\" Assert a metric is tagged with tag \"\"\" self . _asserted . add ( metric_name ) candidates = [] for metric in self . metrics ( metric_name ): if tag in metric . tags : candidates . append ( metric ) msg = \"Candidates size assertion for ` {} `, count: {} , at_least: {} ) failed\" . format ( metric_name , count , at_least ) if count is not None : assert len ( candidates ) == count , msg else : assert len ( candidates ) >= at_least , msg assert_metric_has_tag_prefix ( self , metric_name , tag_prefix , count = None , at_least = 1 ) \u00b6 Source code in datadog_checks\\base\\stubs\\aggregator.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def assert_metric_has_tag_prefix ( self , metric_name , tag_prefix , count = None , at_least = 1 ): candidates = [] self . _asserted . add ( metric_name ) for metric in self . metrics ( metric_name ): tags = metric . tags gtags = [ t for t in tags if t . startswith ( tag_prefix )] if len ( gtags ) > 0 : candidates . append ( metric ) msg = \"Candidates size assertion for ` {} `, count: {} , at_least: {} ) failed\" . format ( metric_name , count , at_least ) if count is not None : assert len ( candidates ) == count , msg else : assert len ( candidates ) >= at_least , msg assert_no_duplicate_metrics ( self ) \u00b6 Assert no duplicate metrics have been submitted. Metrics are considered duplicate when all following fields match: metric name type (gauge, rate, etc) tags hostname Source code in datadog_checks\\base\\stubs\\aggregator.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def assert_no_duplicate_metrics ( self ): \"\"\" Assert no duplicate metrics have been submitted. Metrics are considered duplicate when all following fields match: - metric name - type (gauge, rate, etc) - tags - hostname \"\"\" # metric types that intended to be called multiple times are ignored ignored_types = [ self . COUNT , self . MONOTONIC_COUNT , self . COUNTER ] metric_stubs = [ m for metrics in self . _metrics . values () for m in metrics if m . type not in ignored_types ] def stub_to_key_fn ( stub ): return stub . name , stub . type , str ( sorted ( stub . tags )), stub . hostname self . _assert_no_duplicate_stub ( 'metric' , metric_stubs , stub_to_key_fn ) assert_no_duplicate_service_checks ( self ) \u00b6 Assert no duplicate service checks have been submitted. Service checks are considered duplicate when all following fields match: - metric name - status - tags - hostname Source code in datadog_checks\\base\\stubs\\aggregator.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def assert_no_duplicate_service_checks ( self ): \"\"\" Assert no duplicate service checks have been submitted. Service checks are considered duplicate when all following fields match: - metric name - status - tags - hostname \"\"\" service_check_stubs = [ m for metrics in self . _service_checks . values () for m in metrics ] def stub_to_key_fn ( stub ): return stub . name , stub . status , str ( sorted ( stub . tags )), stub . hostname self . _assert_no_duplicate_stub ( 'service_check' , service_check_stubs , stub_to_key_fn ) assert_service_check ( self , name , status = None , tags = None , count = None , at_least = 1 , hostname = None , message = None ) \u00b6 Assert a service check was processed by this stub Source code in datadog_checks\\base\\stubs\\aggregator.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def assert_service_check ( self , name , status = None , tags = None , count = None , at_least = 1 , hostname = None , message = None ): \"\"\" Assert a service check was processed by this stub \"\"\" tags = normalize_tags ( tags , sort = True ) candidates = [] for sc in self . service_checks ( name ): if status is not None and status != sc . status : continue if tags and tags != sorted ( sc . tags ): continue if hostname is not None and hostname != sc . hostname : continue if message is not None and message != sc . message : continue candidates . append ( sc ) expected_service_check = ServiceCheckStub ( None , name = name , status = status , tags = tags , hostname = hostname , message = message ) if count is not None : msg = \"Needed exactly {} candidates for ' {} ', got {} \" . format ( count , name , len ( candidates )) condition = len ( candidates ) == count else : msg = \"Needed at least {} candidates for ' {} ', got {} \" . format ( at_least , name , len ( candidates )) condition = len ( candidates ) >= at_least self . _assert ( condition = condition , msg = msg , expected_stub = expected_service_check , submitted_elements = self . _service_checks ) datadog_checks.base.stubs.datadog_agent.DatadogAgentStub \u00b6 __init__ ( self ) special \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 7 8 9 def __init__ ( self ): self . _metadata = {} self . _config = self . get_default_config () assert_metadata ( self , check_id , data ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 18 19 20 21 22 23 24 def assert_metadata ( self , check_id , data ): actual = {} for name in data : key = ( check_id , name ) if key in self . _metadata : actual [ name ] = self . _metadata [ key ] assert data == actual assert_metadata_count ( self , count ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 26 27 def assert_metadata_count ( self , count ): assert len ( self . _metadata ) == count get_config ( self , config_option ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 32 33 def get_config ( self , config_option ): return self . _config . get ( config_option , '' ) get_default_config ( self ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 11 12 def get_default_config ( self ): return { 'enable_metadata_collection' : True } get_hostname ( self ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 29 30 def get_hostname ( self ): return 'stubbed.hostname' get_version ( self ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 35 36 def get_version ( self ): return '0.0.0' log ( self , * args , ** kwargs ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 38 39 def log ( self , * args , ** kwargs ): pass reset ( self ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 14 15 16 def reset ( self ): self . _metadata . clear () self . _config = self . get_default_config () set_check_metadata ( self , check_id , name , value ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 41 42 def set_check_metadata ( self , check_id , name , value ): self . _metadata [( check_id , name )] = value set_external_tags ( self , * args , ** kwargs ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 44 45 def set_external_tags ( self , * args , ** kwargs ): pass tracemalloc_enabled ( self , * args , ** kwargs ) \u00b6 Source code in datadog_checks\\base\\stubs\\datadog_agent.py 47 48 def tracemalloc_enabled ( self , * args , ** kwargs ): return False","title":"API"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck","text":"The base class for any Agent based integration. In general, you don't need to and you should not override anything from the base class except the check method but sometimes it might be useful for a Check to have its own constructor. When overriding __init__ you have to remember that, depending on the configuration, the Agent might create several different Check instances and the method would be called as many times. Agent 6,7 signature: AgentCheck(name, init_config, instances) # instances contain only 1 instance AgentCheck.check(instance) Agent 8 signature: AgentCheck(name, init_config, instance) # one instance AgentCheck.check() # no more instance argument for check method Note when loading a Custom check, the Agent will inspect the module searching for a subclass of AgentCheck . If such a class exists but has been derived in turn, it'll be ignored - you should never derive from an existing Check .","title":"AgentCheck"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.__init__","text":"name ( str ) - the name of the check init_config ( dict ) - the init_config section of the configuration. instance ( List[dict] ) - a one-element list containing the instance options from the configuration file (a list is used to keep backward compatibility with older versions of the Agent). Source code in datadog_checks\\base\\checks\\base.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def __init__ ( self , * args , ** kwargs ): # type: (*Any, **Any) -> None \"\"\" - **name** (_str_) - the name of the check - **init_config** (_dict_) - the `init_config` section of the configuration. - **instance** (_List[dict]_) - a one-element list containing the instance options from the configuration file (a list is used to keep backward compatibility with older versions of the Agent). \"\"\" # NOTE: these variable assignments exist to ease type checking when eventually assigned as attributes. name = kwargs . get ( 'name' , '' ) init_config = kwargs . get ( 'init_config' , {}) agentConfig = kwargs . get ( 'agentConfig' , {}) instances = kwargs . get ( 'instances' , []) if len ( args ) > 0 : name = args [ 0 ] if len ( args ) > 1 : init_config = args [ 1 ] if len ( args ) > 2 : # agent pass instances as tuple but in test we are usually using list, so we are testing for both if len ( args ) > 3 or not isinstance ( args [ 2 ], ( list , tuple )) or 'instances' in kwargs : # old-style init: the 3rd argument is `agentConfig` agentConfig = args [ 2 ] if len ( args ) > 3 : instances = args [ 3 ] else : # new-style init: the 3rd argument is `instances` instances = args [ 2 ] # NOTE: Agent 6+ should pass exactly one instance... But we are not abiding by that rule on our side # everywhere just yet. It's complicated... See: https://github.com/DataDog/integrations-core/pull/5573 instance = instances [ 0 ] if instances else None self . check_id = '' self . name = name # type: str self . init_config = init_config # type: InitConfigType self . agentConfig = agentConfig # type: AgentConfigType self . instance = instance # type: Optional[InstanceType] self . instances = instances # type: List[InstanceType] self . warnings = [] # type: List[str] self . metrics = defaultdict ( list ) # type: DefaultDict[str, List[str]] # `self.hostname` is deprecated, use `datadog_agent.get_hostname()` instead self . hostname = datadog_agent . get_hostname () # type: str logger = logging . getLogger ( ' {} . {} ' . format ( __name__ , self . name )) self . log = CheckLoggingAdapter ( logger , self ) # TODO: Remove with Agent 5 # Set proxy settings self . proxies = self . _get_requests_proxy () if not self . init_config : self . _use_agent_proxy = True else : self . _use_agent_proxy = is_affirmative ( self . init_config . get ( 'use_agent_proxy' , True )) # TODO: Remove with Agent 5 self . default_integration_http_timeout = float ( self . agentConfig . get ( 'default_integration_http_timeout' , 9 )) self . _deprecations = { 'increment' : ( False , ( 'DEPRECATION NOTICE: `AgentCheck.increment`/`AgentCheck.decrement` are deprecated, please ' 'use `AgentCheck.gauge` or `AgentCheck.count` instead, with a different metric name' ), ), 'device_name' : ( False , ( 'DEPRECATION NOTICE: `device_name` is deprecated, please use a `device:` ' 'tag in the `tags` list instead' ), ), 'in_developer_mode' : ( False , 'DEPRECATION NOTICE: `in_developer_mode` is deprecated, please stop using it.' , ), 'no_proxy' : ( False , ( 'DEPRECATION NOTICE: The `no_proxy` config option has been renamed ' 'to `skip_proxy` and will be removed in Agent version 6.13.' ), ), 'service_tag' : ( False , ( 'DEPRECATION NOTICE: The `service` tag is deprecated and has been renamed to ` %s `. ' 'Set `disable_legacy_service_tag` to `true` to disable this warning. ' 'The default will become `true` and cannot be changed in Agent version 8.' ), ), } # type: Dict[str, Tuple[bool, str]] # Setup metric limits self . metric_limiter = self . _get_metric_limiter ( self . name , instance = self . instance ) # Functions that will be called exactly once (if successful) before the first check run self . check_initializations = deque ([ self . send_config_metadata ]) # type: Deque[Callable[[], None]]","title":"__init__()"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.count","text":"Sample a raw count metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def count ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a raw count metric. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . COUNT , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw )","title":"count()"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.event","text":"Send an event. An event is a dictionary with the following keys and data types: { \"timestamp\" : int , # the epoch timestamp for the event \"event_type\" : str , # the event name \"api_key\" : str , # the api key for your account \"msg_title\" : str , # the title of the event \"msg_text\" : str , # the text body of the event \"aggregation_key\" : str , # a key to use for aggregating events \"alert_type\" : str , # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info' \"source_type_name\" : str , # (optional) the source type name \"host\" : str , # (optional) the name of the host \"tags\" : list , # (optional) a list of tags to associate with this event \"priority\" : str , # (optional) specifies the priority of the event (\"normal\" or \"low\") } event ( dict ) - the event to be sent Source code in datadog_checks\\base\\checks\\base.py 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 def event ( self , event ): # type: (Event) -> None \"\"\"Send an event. An event is a dictionary with the following keys and data types: ```python { \"timestamp\": int, # the epoch timestamp for the event \"event_type\": str, # the event name \"api_key\": str, # the api key for your account \"msg_title\": str, # the title of the event \"msg_text\": str, # the text body of the event \"aggregation_key\": str, # a key to use for aggregating events \"alert_type\": str, # (optional) one of ('error', 'warning', 'success', 'info'), defaults to 'info' \"source_type_name\": str, # (optional) the source type name \"host\": str, # (optional) the name of the host \"tags\": list, # (optional) a list of tags to associate with this event \"priority\": str, # (optional) specifies the priority of the event (\"normal\" or \"low\") } ``` - **event** (_dict_) - the event to be sent \"\"\" # Enforce types of some fields, considerably facilitates handling in go bindings downstream for key , value in iteritems ( event ): if not isinstance ( value , ( text_type , binary_type )): continue try : event [ key ] = to_native_string ( value ) # type: ignore # ^ Mypy complains about dynamic key assignment -- arguably for good reason. # Ideally we should convert this to a dict literal so that submitted events only include known keys. except UnicodeError : self . log . warning ( 'Encoding error with field ` %s `, cannot submit event' , key ) return if event . get ( 'tags' ): event [ 'tags' ] = self . _normalize_tags_type ( event [ 'tags' ]) if event . get ( 'timestamp' ): event [ 'timestamp' ] = int ( event [ 'timestamp' ]) if event . get ( 'aggregation_key' ): event [ 'aggregation_key' ] = to_native_string ( event [ 'aggregation_key' ]) if self . __NAMESPACE__ : event . setdefault ( 'source_type_name' , self . __NAMESPACE__ ) aggregator . submit_event ( self , self . check_id , event )","title":"event()"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.gauge","text":"Sample a gauge metric. Parameters: name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def gauge ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a gauge metric. **Parameters:** - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . GAUGE , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw )","title":"gauge()"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.histogram","text":"Sample a histogram metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 def histogram ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a histogram metric. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . HISTOGRAM , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw )","title":"histogram()"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.historate","text":"Sample a histogram based on rate metrics. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 def historate ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a histogram based on rate metrics. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . HISTORATE , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw )","title":"historate()"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.monotonic_count","text":"Sample an increasing counter metric. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def monotonic_count ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample an increasing counter metric. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . MONOTONIC_COUNT , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw )","title":"monotonic_count()"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.rate","text":"Sample a point, with the rate calculated at the end of the check. name ( str ) - the name of the metric value ( float ) - the value for the metric tags ( List[str]) ) - a list of tags to associate with this metric hostname ( str ) - a hostname to associate with this metric. Defaults to the current host. device_name ( str ) - deprecated add a tag in the form device:<device_name> to the tags list instead. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def rate ( self , name , value , tags = None , hostname = None , device_name = None , raw = False ): # type: (str, float, Sequence[str], str, str, bool) -> None \"\"\"Sample a point, with the rate calculated at the end of the check. - **name** (_str_) - the name of the metric - **value** (_float_) - the value for the metric - **tags** (_List[str])_) - a list of tags to associate with this metric - **hostname** (_str_) - a hostname to associate with this metric. Defaults to the current host. - **device_name** (_str_) - **deprecated** add a tag in the form `device:<device_name>` to the `tags` list instead. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" self . _submit_metric ( aggregator . RATE , name , value , tags = tags , hostname = hostname , device_name = device_name , raw = raw )","title":"rate()"},{"location":"base/api/#datadog_checks.base.checks.base.AgentCheck.service_check","text":"Send the status of a service. name ( str ) - the name of the service check status ( int ) - a constant describing the service status. tags ( List[str]) ) - a list of tags to associate with this service check message ( str ) - additional information or a description of why this status occurred. raw ( bool ) - whether to ignore any defined namespace prefix Source code in datadog_checks\\base\\checks\\base.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 def service_check ( self , name , status , tags = None , hostname = None , message = None , raw = False ): # type: (str, ServiceCheckStatus, Sequence[str], str, str, bool) -> None \"\"\"Send the status of a service. - **name** (_str_) - the name of the service check - **status** (_int_) - a constant describing the service status. - **tags** (_List[str])_) - a list of tags to associate with this service check - **message** (_str_) - additional information or a description of why this status occurred. - **raw** (_bool_) - whether to ignore any defined namespace prefix \"\"\" tags = self . _normalize_tags_type ( tags or []) if hostname is None : hostname = '' if message is None : message = '' else : message = to_native_string ( message ) message = self . sanitize ( message ) aggregator . submit_service_check ( self , self . check_id , self . _format_namespace ( name , raw ), status , tags , hostname , message )","title":"service_check()"},{"location":"base/api/#stubs","text":"","title":"Stubs"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub","text":"Mainly used for unit testing checks, this stub makes possible to execute a check without a running Agent.","title":"AggregatorStub"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_all_metrics_covered","text":"Source code in datadog_checks\\base\\stubs\\aggregator.py 293 294 295 296 297 298 299 300 301 302 def assert_all_metrics_covered ( self ): # use `condition` to avoid building the `msg` if not needed condition = self . metrics_asserted_pct >= 100.0 msg = '' if not condition : prefix = ' \\n\\t - ' msg = 'Some metrics are missing:' msg += ' \\n Asserted Metrics: {}{} ' . format ( prefix , prefix . join ( sorted ( self . _asserted ))) msg += ' \\n Missing Metrics: {}{} ' . format ( prefix , prefix . join ( sorted ( self . not_asserted ()))) assert condition , msg","title":"assert_all_metrics_covered()"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_event","text":"Source code in datadog_checks\\base\\stubs\\aggregator.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def assert_event ( self , msg_text , count = None , at_least = 1 , exact_match = True , tags = None , ** kwargs ): candidates = [] for e in self . events : if exact_match and msg_text != e [ 'msg_text' ] or msg_text not in e [ 'msg_text' ]: continue if tags and set ( tags ) != set ( e [ 'tags' ]): continue for name , value in iteritems ( kwargs ): if e [ name ] != value : break else : candidates . append ( e ) msg = \"Candidates size assertion for ` {} `, count: {} , at_least: {} ) failed\" . format ( msg_text , count , at_least ) if count is not None : assert len ( candidates ) == count , msg else : assert len ( candidates ) >= at_least , msg","title":"assert_event()"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_metric","text":"Assert a metric was processed by this stub Source code in datadog_checks\\base\\stubs\\aggregator.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def assert_metric ( self , name , value = None , tags = None , count = None , at_least = 1 , hostname = None , metric_type = None , device = None ): \"\"\" Assert a metric was processed by this stub \"\"\" self . _asserted . add ( name ) expected_tags = normalize_tags ( tags , sort = True ) candidates = [] for metric in self . metrics ( name ): if value is not None and not self . is_aggregate ( metric . type ) and value != metric . value : continue if expected_tags and expected_tags != sorted ( metric . tags ): continue if hostname and hostname != metric . hostname : continue if metric_type is not None and metric_type != metric . type : continue if device is not None and device != metric . device : continue candidates . append ( metric ) expected_metric = MetricStub ( name , metric_type , value , tags , hostname , device ) if value is not None and candidates and all ( self . is_aggregate ( m . type ) for m in candidates ): got = sum ( m . value for m in candidates ) msg = \"Expected count value for ' {} ': {} , got {} \" . format ( name , value , got ) condition = value == got elif count is not None : msg = \"Needed exactly {} candidates for ' {} ', got {} \" . format ( count , name , len ( candidates )) condition = len ( candidates ) == count else : msg = \"Needed at least {} candidates for ' {} ', got {} \" . format ( at_least , name , len ( candidates )) condition = len ( candidates ) >= at_least self . _assert ( condition , msg = msg , expected_stub = expected_metric , submitted_elements = self . _metrics )","title":"assert_metric()"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_metric_has_tag","text":"Assert a metric is tagged with tag Source code in datadog_checks\\base\\stubs\\aggregator.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def assert_metric_has_tag ( self , metric_name , tag , count = None , at_least = 1 ): \"\"\" Assert a metric is tagged with tag \"\"\" self . _asserted . add ( metric_name ) candidates = [] for metric in self . metrics ( metric_name ): if tag in metric . tags : candidates . append ( metric ) msg = \"Candidates size assertion for ` {} `, count: {} , at_least: {} ) failed\" . format ( metric_name , count , at_least ) if count is not None : assert len ( candidates ) == count , msg else : assert len ( candidates ) >= at_least , msg","title":"assert_metric_has_tag()"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_metric_has_tag_prefix","text":"Source code in datadog_checks\\base\\stubs\\aggregator.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 def assert_metric_has_tag_prefix ( self , metric_name , tag_prefix , count = None , at_least = 1 ): candidates = [] self . _asserted . add ( metric_name ) for metric in self . metrics ( metric_name ): tags = metric . tags gtags = [ t for t in tags if t . startswith ( tag_prefix )] if len ( gtags ) > 0 : candidates . append ( metric ) msg = \"Candidates size assertion for ` {} `, count: {} , at_least: {} ) failed\" . format ( metric_name , count , at_least ) if count is not None : assert len ( candidates ) == count , msg else : assert len ( candidates ) >= at_least , msg","title":"assert_metric_has_tag_prefix()"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_no_duplicate_metrics","text":"Assert no duplicate metrics have been submitted. Metrics are considered duplicate when all following fields match: metric name type (gauge, rate, etc) tags hostname Source code in datadog_checks\\base\\stubs\\aggregator.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def assert_no_duplicate_metrics ( self ): \"\"\" Assert no duplicate metrics have been submitted. Metrics are considered duplicate when all following fields match: - metric name - type (gauge, rate, etc) - tags - hostname \"\"\" # metric types that intended to be called multiple times are ignored ignored_types = [ self . COUNT , self . MONOTONIC_COUNT , self . COUNTER ] metric_stubs = [ m for metrics in self . _metrics . values () for m in metrics if m . type not in ignored_types ] def stub_to_key_fn ( stub ): return stub . name , stub . type , str ( sorted ( stub . tags )), stub . hostname self . _assert_no_duplicate_stub ( 'metric' , metric_stubs , stub_to_key_fn )","title":"assert_no_duplicate_metrics()"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_no_duplicate_service_checks","text":"Assert no duplicate service checks have been submitted. Service checks are considered duplicate when all following fields match: - metric name - status - tags - hostname Source code in datadog_checks\\base\\stubs\\aggregator.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def assert_no_duplicate_service_checks ( self ): \"\"\" Assert no duplicate service checks have been submitted. Service checks are considered duplicate when all following fields match: - metric name - status - tags - hostname \"\"\" service_check_stubs = [ m for metrics in self . _service_checks . values () for m in metrics ] def stub_to_key_fn ( stub ): return stub . name , stub . status , str ( sorted ( stub . tags )), stub . hostname self . _assert_no_duplicate_stub ( 'service_check' , service_check_stubs , stub_to_key_fn )","title":"assert_no_duplicate_service_checks()"},{"location":"base/api/#datadog_checks.base.stubs.aggregator.AggregatorStub.assert_service_check","text":"Assert a service check was processed by this stub Source code in datadog_checks\\base\\stubs\\aggregator.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def assert_service_check ( self , name , status = None , tags = None , count = None , at_least = 1 , hostname = None , message = None ): \"\"\" Assert a service check was processed by this stub \"\"\" tags = normalize_tags ( tags , sort = True ) candidates = [] for sc in self . service_checks ( name ): if status is not None and status != sc . status : continue if tags and tags != sorted ( sc . tags ): continue if hostname is not None and hostname != sc . hostname : continue if message is not None and message != sc . message : continue candidates . append ( sc ) expected_service_check = ServiceCheckStub ( None , name = name , status = status , tags = tags , hostname = hostname , message = message ) if count is not None : msg = \"Needed exactly {} candidates for ' {} ', got {} \" . format ( count , name , len ( candidates )) condition = len ( candidates ) == count else : msg = \"Needed at least {} candidates for ' {} ', got {} \" . format ( at_least , name , len ( candidates )) condition = len ( candidates ) >= at_least self . _assert ( condition = condition , msg = msg , expected_stub = expected_service_check , submitted_elements = self . _service_checks )","title":"assert_service_check()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub","text":"","title":"DatadogAgentStub"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.__init__","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 7 8 9 def __init__ ( self ): self . _metadata = {} self . _config = self . get_default_config ()","title":"__init__()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.assert_metadata","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 18 19 20 21 22 23 24 def assert_metadata ( self , check_id , data ): actual = {} for name in data : key = ( check_id , name ) if key in self . _metadata : actual [ name ] = self . _metadata [ key ] assert data == actual","title":"assert_metadata()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.assert_metadata_count","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 26 27 def assert_metadata_count ( self , count ): assert len ( self . _metadata ) == count","title":"assert_metadata_count()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.get_config","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 32 33 def get_config ( self , config_option ): return self . _config . get ( config_option , '' )","title":"get_config()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.get_default_config","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 11 12 def get_default_config ( self ): return { 'enable_metadata_collection' : True }","title":"get_default_config()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.get_hostname","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 29 30 def get_hostname ( self ): return 'stubbed.hostname'","title":"get_hostname()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.get_version","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 35 36 def get_version ( self ): return '0.0.0'","title":"get_version()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.log","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 38 39 def log ( self , * args , ** kwargs ): pass","title":"log()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.reset","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 14 15 16 def reset ( self ): self . _metadata . clear () self . _config = self . get_default_config ()","title":"reset()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.set_check_metadata","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 41 42 def set_check_metadata ( self , check_id , name , value ): self . _metadata [( check_id , name )] = value","title":"set_check_metadata()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.set_external_tags","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 44 45 def set_external_tags ( self , * args , ** kwargs ): pass","title":"set_external_tags()"},{"location":"base/api/#datadog_checks.base.stubs.datadog_agent.DatadogAgentStub.tracemalloc_enabled","text":"Source code in datadog_checks\\base\\stubs\\datadog_agent.py 47 48 def tracemalloc_enabled ( self , * args , ** kwargs ): return False","title":"tracemalloc_enabled()"},{"location":"base/databases/","text":"Databases What is a database, you may wonder. Well, the answer to that question is fascinating ! No matter the database you wish to monitor, the base package provides a standard way to define and collect data from arbitrary queries. The core premise is that you define a function that accepts a query (usually a str ) and it returns a sequence of equal length results. Interface \u00b6 All the functionality is exposed by the Query and QueryManager classes. datadog_checks.base.utils.db.query.Query \u00b6 This class accepts a single dict argument which is the necessary data to run the query. The representation is based on our custom_queries format originally designed and implemented in !1528 . It is now part of all our database integrations and other products have since adopted this format. __init__ ( self , query_data ) special \u00b6 Source code in datadog_checks\\base\\utils\\db\\query.py 21 22 23 24 25 26 27 def __init__ ( self , query_data ): self . query_data = deepcopy ( query_data or {}) self . name = None self . query = None self . columns = None self . extras = None self . tags = None compile ( self , column_transformers , extra_transformers ) \u00b6 This idempotent method will be called by QueryManager.compile_queries so you should never need to call it directly. Source code in datadog_checks\\base\\utils\\db\\query.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def compile ( self , column_transformers , extra_transformers ): \"\"\" This idempotent method will be called by `QueryManager.compile_queries` so you should never need to call it directly. \"\"\" # Check for previous compilation if self . name is not None : return query_name = self . query_data . get ( 'name' ) if not query_name : raise ValueError ( 'query field `name` is required' ) elif not isinstance ( query_name , str ): raise ValueError ( 'query field `name` must be a string' ) query = self . query_data . get ( 'query' ) if not query : raise ValueError ( 'field `query` for {} is required' . format ( query_name )) elif not isinstance ( query , str ): raise ValueError ( 'field `query` for {} must be a string' . format ( query_name )) columns = self . query_data . get ( 'columns' ) if not columns : raise ValueError ( 'field `columns` for {} is required' . format ( query_name )) elif not isinstance ( columns , list ): raise ValueError ( 'field `columns` for {} must be a list' . format ( query_name )) tags = self . query_data . get ( 'tags' , []) if tags is not None and not isinstance ( tags , list ): raise ValueError ( 'field `tags` for {} must be a list' . format ( query_name )) # Keep track of all defined names sources = {} column_data = [] for i , column in enumerate ( columns , 1 ): # Columns can be ignored via configuration. if not column : column_data . append (( None , None )) continue elif not isinstance ( column , dict ): raise ValueError ( 'column # {} of {} is not a mapping' . format ( i , query_name )) column_name = column . get ( 'name' ) if not column_name : raise ValueError ( 'field `name` for column # {} of {} is required' . format ( i , query_name )) elif not isinstance ( column_name , str ): raise ValueError ( 'field `name` for column # {} of {} must be a string' . format ( i , query_name )) elif column_name in sources : raise ValueError ( 'the name {} of {} was already defined in {} # {} ' . format ( column_name , query_name , sources [ column_name ][ 'type' ], sources [ column_name ][ 'index' ] ) ) sources [ column_name ] = { 'type' : 'column' , 'index' : i } column_type = column . get ( 'type' ) if not column_type : raise ValueError ( 'field `type` for column {} of {} is required' . format ( column_name , query_name )) elif not isinstance ( column_type , str ): raise ValueError ( 'field `type` for column {} of {} must be a string' . format ( column_name , query_name )) elif column_type == 'source' : column_data . append (( column_name , ( None , None ))) continue elif column_type not in column_transformers : raise ValueError ( 'unknown type ` {} ` for column {} of {} ' . format ( column_type , column_name , query_name )) modifiers = { key : value for key , value in column . items () if key not in ( 'name' , 'type' )} try : transformer = column_transformers [ column_type ]( column_transformers , column_name , ** modifiers ) except Exception as e : error = 'error compiling type ` {} ` for column {} of {} : {} ' . format ( column_type , column_name , query_name , e ) # Prepend helpful error text. # # When an exception is raised in the context of another one, both will be printed. To avoid # this we set the context to None. https://www.python.org/dev/peps/pep-0409/ raise_from ( type ( e )( error ), None ) else : if column_type == 'tag' : column_data . append (( column_name , ( column_type , transformer ))) else : # All these would actually submit data. As that is the default case, we represent it as # a reference to None since if we use e.g. `value` it would never be checked anyway. column_data . append (( column_name , ( None , transformer ))) submission_transformers = column_transformers . copy () submission_transformers . pop ( 'tag' ) extras = self . query_data . get ( 'extras' , []) if not isinstance ( extras , list ): raise ValueError ( 'field `extras` for {} must be a list' . format ( query_name )) extra_data = [] for i , extra in enumerate ( extras , 1 ): if not isinstance ( extra , dict ): raise ValueError ( 'extra # {} of {} is not a mapping' . format ( i , query_name )) extra_name = extra . get ( 'name' ) if not extra_name : raise ValueError ( 'field `name` for extra # {} of {} is required' . format ( i , query_name )) elif not isinstance ( extra_name , str ): raise ValueError ( 'field `name` for extra # {} of {} must be a string' . format ( i , query_name )) elif extra_name in sources : raise ValueError ( 'the name {} of {} was already defined in {} # {} ' . format ( extra_name , query_name , sources [ extra_name ][ 'type' ], sources [ extra_name ][ 'index' ] ) ) sources [ extra_name ] = { 'type' : 'extra' , 'index' : i } extra_type = extra . get ( 'type' ) if not extra_type : if 'expression' in extra : extra_type = 'expression' else : raise ValueError ( 'field `type` for extra {} of {} is required' . format ( extra_name , query_name )) elif not isinstance ( extra_type , str ): raise ValueError ( 'field `type` for extra {} of {} must be a string' . format ( extra_name , query_name )) elif extra_type not in extra_transformers and extra_type not in submission_transformers : raise ValueError ( 'unknown type ` {} ` for extra {} of {} ' . format ( extra_type , extra_name , query_name )) transformer_factory = extra_transformers . get ( extra_type , submission_transformers . get ( extra_type )) extra_source = extra . get ( 'source' ) if extra_type in submission_transformers : if not extra_source : raise ValueError ( 'field `source` for extra {} of {} is required' . format ( extra_name , query_name )) modifiers = { key : value for key , value in extra . items () if key not in ( 'name' , 'type' , 'source' )} else : modifiers = { key : value for key , value in extra . items () if key not in ( 'name' , 'type' )} modifiers [ 'sources' ] = sources try : transformer = transformer_factory ( submission_transformers , extra_name , ** modifiers ) except Exception as e : error = 'error compiling type ` {} ` for extra {} of {} : {} ' . format ( extra_type , extra_name , query_name , e ) raise_from ( type ( e )( error ), None ) else : if extra_type in submission_transformers : transformer = create_extra_transformer ( transformer , extra_source ) extra_data . append (( extra_name , transformer )) self . name = query_name self . query = query self . columns = tuple ( column_data ) self . extras = tuple ( extra_data ) self . tags = tags del self . query_data datadog_checks.base.utils.db.core.QueryManager \u00b6 This class is in charge of running any number of Query instances for a single Check instance. You will most often see it created during Check initialization like this: self . _query_manager = QueryManager ( self , self . execute_query , queries = [ queries . SomeQuery1 , queries . SomeQuery2 , queries . SomeQuery3 , queries . SomeQuery4 , queries . SomeQuery5 , ], tags = self . instance . get ( 'tags' , []), error_handler = self . _error_sanitizer , ) self . check_initializations . append ( self . _query_manager . compile_queries ) __init__ ( self , check , executor , queries = None , tags = None , error_handler = None ) special \u00b6 check ( AgentCheck ) - an instance of a Check executor ( callable ) - a callable accepting a str query as its sole argument and returning a sequence representing either the full result set or an iterator over the result set queries ( List[Query]) ) - a list of Query instances tags ( List[str]) ) - a list of tags to associate with every submission error_handler ( callable ) - a callable accepting a str error as its sole argument and returning a sanitized string, useful for scrubbing potentially sensitive information libraries emit Source code in datadog_checks\\base\\utils\\db\\core.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , check , executor , queries = None , tags = None , error_handler = None ): \"\"\" - **check** (_AgentCheck_) - an instance of a Check - **executor** (_callable_) - a callable accepting a `str` query as its sole argument and returning a sequence representing either the full result set or an iterator over the result set - **queries** (_List[Query])_) - a list of `Query` instances - **tags** (_List[str])_) - a list of tags to associate with every submission - **error_handler** (_callable_) - a callable accepting a `str` error as its sole argument and returning a sanitized string, useful for scrubbing potentially sensitive information libraries emit \"\"\" self . check = check self . executor = executor self . queries = queries or [] self . tags = tags or [] self . error_handler = error_handler custom_queries = list ( self . check . instance . get ( 'custom_queries' , [])) use_global_custom_queries = self . check . instance . get ( 'use_global_custom_queries' , True ) # Handle overrides if use_global_custom_queries == 'extend' : custom_queries . extend ( self . check . init_config . get ( 'global_custom_queries' , [])) elif ( not custom_queries and 'global_custom_queries' in self . check . init_config and is_affirmative ( use_global_custom_queries ) ): custom_queries = self . check . init_config . get ( 'global_custom_queries' , []) # Deduplicate for i , custom_query in enumerate ( iter_unique ( custom_queries ), 1 ): query = Query ( custom_query ) query . query_data . setdefault ( 'name' , 'custom query # {} ' . format ( i )) self . queries . append ( query ) compile_queries ( self ) \u00b6 This method compiles every Query object. Source code in datadog_checks\\base\\utils\\db\\core.py 72 73 74 75 76 77 78 79 80 81 82 def compile_queries ( self ): \"\"\"This method compiles every `Query` object.\"\"\" column_transformers = COLUMN_TRANSFORMERS . copy () for submission_method , transformer_name in SUBMISSION_METHODS . items (): method = getattr ( self . check , submission_method ) # Save each method in the initializer -> callable format column_transformers [ transformer_name ] = create_submission_transformer ( method ) for query in self . queries : query . compile ( column_transformers , EXTRA_TRANSFORMERS . copy ()) execute ( self ) \u00b6 This method is what you call every check run. Source code in datadog_checks\\base\\utils\\db\\core.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def execute ( self ): \"\"\"This method is what you call every check run.\"\"\" logger = self . check . log global_tags = self . tags for query in self . queries : query_name = query . name query_columns = query . columns query_extras = query . extras query_tags = query . tags num_columns = len ( query_columns ) try : rows = self . execute_query ( query . query ) except Exception as e : if self . error_handler : logger . error ( 'Error querying %s : %s ' , query_name , self . error_handler ( str ( e ))) else : logger . error ( 'Error querying %s : %s ' , query_name , e ) continue for row in rows : if not row : logger . debug ( 'Query %s returned an empty result' , query_name ) continue if num_columns != len ( row ): logger . error ( 'Query %s expected %d column %s , got %d ' , query_name , num_columns , 's' if num_columns > 1 else '' , len ( row ), ) continue sources = {} submission_queue = [] tags = list ( global_tags ) tags . extend ( query_tags ) for ( column_name , transformer ), value in zip ( query_columns , row ): # Columns can be ignored via configuration if not column_name : continue sources [ column_name ] = value column_type , transformer = transformer # The transformer can be None for `source` types. Those such columns do not submit # anything but are collected into the row values for other columns to reference. if transformer is None : continue elif column_type == 'tag' : tags . append ( transformer ( None , value )) else : submission_queue . append (( transformer , value )) for transformer , value in submission_queue : transformer ( sources , value , tags = tags ) for name , transformer in query_extras : try : result = transformer ( sources , tags = tags ) except Exception as e : logger . error ( 'Error transforming %s : %s ' , name , e ) continue else : if result is not None : sources [ name ] = result execute_query ( self , query ) \u00b6 Called by execute , this triggers query execution to check for errors immediately in a way that is compatible with any library. If there are no errors, this is guaranteed to return an iterator over the result set. Source code in datadog_checks\\base\\utils\\db\\core.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def execute_query ( self , query ): \"\"\" Called by `execute`, this triggers query execution to check for errors immediately in a way that is compatible with any library. If there are no errors, this is guaranteed to return an iterator over the result set. \"\"\" rows = self . executor ( query ) if rows is None : return iter ([]) else : rows = iter ( rows ) # Ensure we trigger query execution try : first_row = next ( rows ) except StopIteration : return iter ([]) return chain (( first_row ,), rows ) Transformers \u00b6 datadog_checks.base.utils.db.transform.ColumnTransformers \u00b6 match ( transformers , column_name , ** modifiers ) \u00b6 This is used for querying unstructured data. For example, say you want to collect the fields named foo and bar . Typically, they would be stored like: foo bar 4 2 and would be queried like: SELECT foo , bar FROM ... Often, you will instead find data stored in the following format: metric value foo 4 bar 2 and would be queried like: SELECT metric , value FROM ... In this case, the metric column stores the name with which to match on and its value is stored in a separate column. The required items modifier is a mapping of matched names to column data values. Consider the values to be exactly the same as the entries in the columns top level field. You must also define a source modifier either for this transformer itself or in the values of items (which will take precedence). The source will be treated as the value of the match. Say this is your configuration: query : SELECT source1, source2, metric FROM TABLE columns : - name : value1 type : source - name : value2 type : source - name : metric_name type : match source : value1 items : foo : name : test.foo type : gauge source : value2 bar : name : test.bar type : monotonic_gauge and the result set is: source1 source2 metric 1 2 foo 3 4 baz 5 6 bar Here's what would be submitted: foo - test.foo as a gauge with a value of 2 bar - test.bar.total as a gauge and test.bar.count as a monotonic_count , both with a value of 5 baz - nothing since it was not defined as a match item Source code in datadog_checks\\base\\utils\\db\\transform.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def get_match ( transformers , column_name , ** modifiers ): \"\"\" This is used for querying unstructured data. For example, say you want to collect the fields named `foo` and `bar`. Typically, they would be stored like: | foo | bar | | --- | --- | | 4 | 2 | and would be queried like: ```sql SELECT foo, bar FROM ... ``` Often, you will instead find data stored in the following format: | metric | value | | ------ | ----- | | foo | 4 | | bar | 2 | and would be queried like: ```sql SELECT metric, value FROM ... ``` In this case, the `metric` column stores the name with which to match on and its `value` is stored in a separate column. The required `items` modifier is a mapping of matched names to column data values. Consider the values to be exactly the same as the entries in the `columns` top level field. You must also define a `source` modifier either for this transformer itself or in the values of `items` (which will take precedence). The source will be treated as the value of the match. Say this is your configuration: ```yaml query: SELECT source1, source2, metric FROM TABLE columns: - name: value1 type: source - name: value2 type: source - name: metric_name type: match source: value1 items: foo: name: test.foo type: gauge source: value2 bar: name: test.bar type: monotonic_gauge ``` and the result set is: | source1 | source2 | metric | | ------- | ------- | ------ | | 1 | 2 | foo | | 3 | 4 | baz | | 5 | 6 | bar | Here's what would be submitted: - `foo` - `test.foo` as a `gauge` with a value of `2` - `bar` - `test.bar.total` as a `gauge` and `test.bar.count` as a `monotonic_count`, both with a value of `5` - `baz` - nothing since it was not defined as a match item \"\"\" # Do work in a separate function to avoid having to `del` a bunch of variables compiled_items = _compile_match_items ( transformers , modifiers ) def match ( sources , value , ** kwargs ): if value in compiled_items : source , transformer = compiled_items [ value ] transformer ( sources , sources [ source ], ** kwargs ) return match monotonic_gauge ( transformers , column_name , ** modifiers ) \u00b6 Send the result as both a gauge suffixed by .total and a monotonic_count suffixed by .count . Source code in datadog_checks\\base\\utils\\db\\transform.py 51 52 53 54 55 56 57 58 59 60 61 62 def get_monotonic_gauge ( transformers , column_name , ** modifiers ): \"\"\" Send the result as both a `gauge` suffixed by `.total` and a `monotonic_count` suffixed by `.count`. \"\"\" gauge = transformers [ 'gauge' ]( transformers , ' {} .total' . format ( column_name ), ** modifiers ) monotonic_count = transformers [ 'monotonic_count' ]( transformers , ' {} .count' . format ( column_name ), ** modifiers ) def monotonic_gauge ( _ , value , ** kwargs ): gauge ( _ , value , ** kwargs ) monotonic_count ( _ , value , ** kwargs ) return monotonic_gauge service_check ( transformers , column_name , ** modifiers ) \u00b6 Submit a service check. The required modifier status_map is a mapping of values to statuses. Valid statuses include: OK WARNING CRITICAL UNKNOWN Any encountered values that are not defined will be sent as UNKNOWN . Source code in datadog_checks\\base\\utils\\db\\transform.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def get_service_check ( transformers , column_name , ** modifiers ): \"\"\" Submit a service check. The required modifier `status_map` is a mapping of values to statuses. Valid statuses include: - `OK` - `WARNING` - `CRITICAL` - `UNKNOWN` Any encountered values that are not defined will be sent as `UNKNOWN`. \"\"\" # Do work in a separate function to avoid having to `del` a bunch of variables status_map = _compile_service_check_statuses ( modifiers ) service_check_method = transformers [ '__service_check' ]( transformers , column_name , ** modifiers ) def service_check ( _ , value , ** kwargs ): service_check_method ( _ , status_map . get ( value , ServiceCheck . UNKNOWN ), ** kwargs ) return service_check tag ( transformers , column_name , ** modifiers ) \u00b6 Convert a column to a tag that will be used in every subsequent submission. For example, if you named the column env and the column returned the value prod1 , all submissions from that row will be tagged by env:prod1 . This also accepts an optional modifier called boolean that when set to true will transform the result to the string true or false . So for example if you named the column alive and the result was the number 0 the tag will be alive:false . Source code in datadog_checks\\base\\utils\\db\\transform.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def get_tag ( transformers , column_name , ** modifiers ): \"\"\" Convert a column to a tag that will be used in every subsequent submission. For example, if you named the column `env` and the column returned the value `prod1`, all submissions from that row will be tagged by `env:prod1`. This also accepts an optional modifier called `boolean` that when set to `true` will transform the result to the string `true` or `false`. So for example if you named the column `alive` and the result was the number `0` the tag will be `alive:false`. \"\"\" template = ' {} :{{}}' . format ( column_name ) boolean = is_affirmative ( modifiers . pop ( 'boolean' , None )) def tag ( _ , value , ** kwargs ): if boolean : value = str ( is_affirmative ( value )) . lower () return template . format ( value ) return tag temporal_percent ( transformers , column_name , ** modifiers ) \u00b6 Send the result as percentage of time since the last check run as a rate . For example, say the result is a forever increasing counter representing the total time spent pausing for garbage collection since start up. That number by itself is quite useless, but as a percentage of time spent pausing since the previous collection interval it becomes a useful metric. There is one required parameter called scale that indicates what unit of time the result should be considered. Valid values are: second millisecond microsecond nanosecond You may also define the unit as an integer number of parts compared to seconds e.g. millisecond is equivalent to 1000 . Source code in datadog_checks\\base\\utils\\db\\transform.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get_temporal_percent ( transformers , column_name , ** modifiers ): \"\"\" Send the result as percentage of time since the last check run as a `rate`. For example, say the result is a forever increasing counter representing the total time spent pausing for garbage collection since start up. That number by itself is quite useless, but as a percentage of time spent pausing since the previous collection interval it becomes a useful metric. There is one required parameter called `scale` that indicates what unit of time the result should be considered. Valid values are: - `second` - `millisecond` - `microsecond` - `nanosecond` You may also define the unit as an integer number of parts compared to seconds e.g. `millisecond` is equivalent to `1000`. \"\"\" scale = modifiers . pop ( 'scale' , None ) if scale is None : raise ValueError ( 'the `scale` parameter is required' ) if isinstance ( scale , str ): scale = constants . TIME_UNITS . get ( scale . lower ()) if scale is None : raise ValueError ( 'the `scale` parameter must be one of: {} ' . format ( ' | ' . join ( sorted ( constants . TIME_UNITS ))) ) elif not isinstance ( scale , int ): raise ValueError ( 'the `scale` parameter must be an integer representing parts of a second e.g. 1000 for millisecond' ) rate = transformers [ 'rate' ]( transformers , column_name , ** modifiers ) def temporal_percent ( _ , value , ** kwargs ): rate ( _ , total_time_to_temporal_percent ( float ( value ), scale = scale ), ** kwargs ) return temporal_percent time_elapsed ( transformers , column_name , ** modifiers ) \u00b6 Send the number of seconds elapsed from a time in the past as a gauge . For example, if the result is an instance of datetime.datetime representing 5 seconds ago, then this would submit with a value of 5 . The optional modifier format indicates what format the result is in. By default it is native , assuming the underlying library provides timestamps as datetime objects. If it does not and passes them through directly as strings, you must provide the expected timestamp format using the supported codes . Note The code %z (lower case) is not supported on Windows. Source code in datadog_checks\\base\\utils\\db\\transform.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def get_time_elapsed ( transformers , column_name , ** modifiers ): \"\"\" Send the number of seconds elapsed from a time in the past as a `gauge`. For example, if the result is an instance of [datetime.datetime](https://docs.python.org/3/library/datetime.html#datetime.datetime) representing 5 seconds ago, then this would submit with a value of `5`. The optional modifier `format` indicates what format the result is in. By default it is `native`, assuming the underlying library provides timestamps as `datetime` objects. If it does not and passes them through directly as strings, you must provide the expected timestamp format using the [supported codes](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes). !!! note The code `%z` (lower case) is not supported on Windows. \"\"\" time_format = modifiers . pop ( 'format' , 'native' ) if not isinstance ( time_format , str ): raise ValueError ( 'the `format` parameter must be a string' ) gauge = transformers [ 'gauge' ]( transformers , column_name , ** modifiers ) if time_format == 'native' : def time_elapsed ( _ , value , ** kwargs ): value = normalize_datetime ( value ) gauge ( _ , ( datetime . now ( value . tzinfo ) - value ) . total_seconds (), ** kwargs ) else : def time_elapsed ( _ , value , ** kwargs ): value = normalize_datetime ( datetime . strptime ( value , time_format )) gauge ( _ , ( datetime . now ( value . tzinfo ) - value ) . total_seconds (), ** kwargs ) return time_elapsed datadog_checks.base.utils.db.transform.ExtraTransformers \u00b6 Every column transformer (except tag ) is supported at this level, the only difference being one must set a source to retrieve the desired value. So for example here: columns : - name : foo.bar type : rate extras : - name : foo.current type : gauge source : foo.bar the metric foo.current will be sent as a gauge will the value of foo.bar . expression ( transformers , name , ** modifiers ) \u00b6 This allows the evaluation of a limited subset of Python syntax and built-in functions. columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.free expression : disk.total - disk.used submit_type : gauge For brevity, if the expression attribute exists and type does not then it is assumed the type is expression . The submit_type can be any transformer and any extra options are passed down to it. The result of every expression is stored, so in lieu of a submit_type the above example could also be written as: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : free expression : disk.total - disk.used - name : disk.free type : gauge source : free The order matters though, so for example the following will fail: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.free type : gauge source : free - name : free expression : disk.total - disk.used since the source free does not yet exist. Source code in datadog_checks\\base\\utils\\db\\transform.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def get_expression ( transformers , name , ** modifiers ): \"\"\" This allows the evaluation of a limited subset of Python syntax and built-in functions. ```yaml columns: - name: disk.total type: gauge - name: disk.used type: gauge extras: - name: disk.free expression: disk.total - disk.used submit_type: gauge ``` For brevity, if the `expression` attribute exists and `type` does not then it is assumed the type is `expression`. The `submit_type` can be any transformer and any extra options are passed down to it. The result of every expression is stored, so in lieu of a `submit_type` the above example could also be written as: ```yaml columns: - name: disk.total type: gauge - name: disk.used type: gauge extras: - name: free expression: disk.total - disk.used - name: disk.free type: gauge source: free ``` The order matters though, so for example the following will fail: ```yaml columns: - name: disk.total type: gauge - name: disk.used type: gauge extras: - name: disk.free type: gauge source: free - name: free expression: disk.total - disk.used ``` since the source `free` does not yet exist. \"\"\" available_sources = modifiers . pop ( 'sources' ) expression = modifiers . pop ( 'expression' , None ) if expression is None : raise ValueError ( 'the `expression` parameter is required' ) elif not isinstance ( expression , str ): raise ValueError ( 'the `expression` parameter must be a string' ) elif not expression : raise ValueError ( 'the `expression` parameter must not be empty' ) if not modifiers . pop ( 'verbose' , False ): # Sort the sources in reverse order of length to prevent greedy matching available_sources = sorted ( available_sources , key = lambda s : - len ( s )) # Escape special characters, mostly for the possible dots in metric names available_sources = list ( map ( re . escape , available_sources )) # Finally, utilize the order by relying on the guarantees provided by the alternation operator available_sources = '|' . join ( available_sources ) expression = re . sub ( SOURCE_PATTERN . format ( available_sources ), # Replace by the particular source that matched lambda match_obj : 'SOURCES[\" {} \"]' . format ( match_obj . group ( 1 )), expression , ) expression = compile ( expression , filename = name , mode = 'eval' ) del available_sources if 'submit_type' in modifiers : if modifiers [ 'submit_type' ] not in transformers : raise ValueError ( 'unknown submit_type ` {} `' . format ( modifiers [ 'submit_type' ])) submit_method = transformers [ modifiers . pop ( 'submit_type' )]( transformers , name , ** modifiers ) submit_method = create_extra_transformer ( submit_method ) def execute_expression ( sources , ** kwargs ): result = eval ( expression , ALLOWED_GLOBALS , { 'SOURCES' : sources }) submit_method ( sources , result , ** kwargs ) return result else : def execute_expression ( sources , ** kwargs ): return eval ( expression , ALLOWED_GLOBALS , { 'SOURCES' : sources }) return execute_expression percent ( transformers , name , ** modifiers ) \u00b6 Send a percentage based on 2 sources as a gauge . The required modifiers are part and total . For example, if you have this configuration: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.utilized type : percent part : disk.used total : disk.total then the extra metric disk.utilized would be sent as a gauge calculated as disk.used / disk.total * 100 . If the source of total is 0 , then the submitted value will always be sent as 0 too. Source code in datadog_checks\\base\\utils\\db\\transform.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def get_percent ( transformers , name , ** modifiers ): \"\"\" Send a percentage based on 2 sources as a `gauge`. The required modifiers are `part` and `total`. For example, if you have this configuration: ```yaml columns: - name: disk.total type: gauge - name: disk.used type: gauge extras: - name: disk.utilized type: percent part: disk.used total: disk.total ``` then the extra metric `disk.utilized` would be sent as a `gauge` calculated as `disk.used / disk.total * 100`. If the source of `total` is `0`, then the submitted value will always be sent as `0` too. \"\"\" available_sources = modifiers . pop ( 'sources' ) part = modifiers . pop ( 'part' , None ) if part is None : raise ValueError ( 'the `part` parameter is required' ) elif not isinstance ( part , str ): raise ValueError ( 'the `part` parameter must be a string' ) elif part not in available_sources : raise ValueError ( 'the `part` parameter ` {} ` is not an available source' . format ( part )) total = modifiers . pop ( 'total' , None ) if total is None : raise ValueError ( 'the `total` parameter is required' ) elif not isinstance ( total , str ): raise ValueError ( 'the `total` parameter must be a string' ) elif total not in available_sources : raise ValueError ( 'the `total` parameter ` {} ` is not an available source' . format ( total )) del available_sources gauge = transformers [ 'gauge' ]( transformers , name , ** modifiers ) gauge = create_extra_transformer ( gauge ) def percent ( sources , ** kwargs ): gauge ( sources , compute_percent ( sources [ part ], sources [ total ]), ** kwargs ) return percent","title":"Databases"},{"location":"base/databases/#interface","text":"All the functionality is exposed by the Query and QueryManager classes.","title":"Interface"},{"location":"base/databases/#datadog_checks.base.utils.db.query.Query","text":"This class accepts a single dict argument which is the necessary data to run the query. The representation is based on our custom_queries format originally designed and implemented in !1528 . It is now part of all our database integrations and other products have since adopted this format.","title":"Query"},{"location":"base/databases/#datadog_checks.base.utils.db.query.Query.__init__","text":"Source code in datadog_checks\\base\\utils\\db\\query.py 21 22 23 24 25 26 27 def __init__ ( self , query_data ): self . query_data = deepcopy ( query_data or {}) self . name = None self . query = None self . columns = None self . extras = None self . tags = None","title":"__init__()"},{"location":"base/databases/#datadog_checks.base.utils.db.query.Query.compile","text":"This idempotent method will be called by QueryManager.compile_queries so you should never need to call it directly. Source code in datadog_checks\\base\\utils\\db\\query.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def compile ( self , column_transformers , extra_transformers ): \"\"\" This idempotent method will be called by `QueryManager.compile_queries` so you should never need to call it directly. \"\"\" # Check for previous compilation if self . name is not None : return query_name = self . query_data . get ( 'name' ) if not query_name : raise ValueError ( 'query field `name` is required' ) elif not isinstance ( query_name , str ): raise ValueError ( 'query field `name` must be a string' ) query = self . query_data . get ( 'query' ) if not query : raise ValueError ( 'field `query` for {} is required' . format ( query_name )) elif not isinstance ( query , str ): raise ValueError ( 'field `query` for {} must be a string' . format ( query_name )) columns = self . query_data . get ( 'columns' ) if not columns : raise ValueError ( 'field `columns` for {} is required' . format ( query_name )) elif not isinstance ( columns , list ): raise ValueError ( 'field `columns` for {} must be a list' . format ( query_name )) tags = self . query_data . get ( 'tags' , []) if tags is not None and not isinstance ( tags , list ): raise ValueError ( 'field `tags` for {} must be a list' . format ( query_name )) # Keep track of all defined names sources = {} column_data = [] for i , column in enumerate ( columns , 1 ): # Columns can be ignored via configuration. if not column : column_data . append (( None , None )) continue elif not isinstance ( column , dict ): raise ValueError ( 'column # {} of {} is not a mapping' . format ( i , query_name )) column_name = column . get ( 'name' ) if not column_name : raise ValueError ( 'field `name` for column # {} of {} is required' . format ( i , query_name )) elif not isinstance ( column_name , str ): raise ValueError ( 'field `name` for column # {} of {} must be a string' . format ( i , query_name )) elif column_name in sources : raise ValueError ( 'the name {} of {} was already defined in {} # {} ' . format ( column_name , query_name , sources [ column_name ][ 'type' ], sources [ column_name ][ 'index' ] ) ) sources [ column_name ] = { 'type' : 'column' , 'index' : i } column_type = column . get ( 'type' ) if not column_type : raise ValueError ( 'field `type` for column {} of {} is required' . format ( column_name , query_name )) elif not isinstance ( column_type , str ): raise ValueError ( 'field `type` for column {} of {} must be a string' . format ( column_name , query_name )) elif column_type == 'source' : column_data . append (( column_name , ( None , None ))) continue elif column_type not in column_transformers : raise ValueError ( 'unknown type ` {} ` for column {} of {} ' . format ( column_type , column_name , query_name )) modifiers = { key : value for key , value in column . items () if key not in ( 'name' , 'type' )} try : transformer = column_transformers [ column_type ]( column_transformers , column_name , ** modifiers ) except Exception as e : error = 'error compiling type ` {} ` for column {} of {} : {} ' . format ( column_type , column_name , query_name , e ) # Prepend helpful error text. # # When an exception is raised in the context of another one, both will be printed. To avoid # this we set the context to None. https://www.python.org/dev/peps/pep-0409/ raise_from ( type ( e )( error ), None ) else : if column_type == 'tag' : column_data . append (( column_name , ( column_type , transformer ))) else : # All these would actually submit data. As that is the default case, we represent it as # a reference to None since if we use e.g. `value` it would never be checked anyway. column_data . append (( column_name , ( None , transformer ))) submission_transformers = column_transformers . copy () submission_transformers . pop ( 'tag' ) extras = self . query_data . get ( 'extras' , []) if not isinstance ( extras , list ): raise ValueError ( 'field `extras` for {} must be a list' . format ( query_name )) extra_data = [] for i , extra in enumerate ( extras , 1 ): if not isinstance ( extra , dict ): raise ValueError ( 'extra # {} of {} is not a mapping' . format ( i , query_name )) extra_name = extra . get ( 'name' ) if not extra_name : raise ValueError ( 'field `name` for extra # {} of {} is required' . format ( i , query_name )) elif not isinstance ( extra_name , str ): raise ValueError ( 'field `name` for extra # {} of {} must be a string' . format ( i , query_name )) elif extra_name in sources : raise ValueError ( 'the name {} of {} was already defined in {} # {} ' . format ( extra_name , query_name , sources [ extra_name ][ 'type' ], sources [ extra_name ][ 'index' ] ) ) sources [ extra_name ] = { 'type' : 'extra' , 'index' : i } extra_type = extra . get ( 'type' ) if not extra_type : if 'expression' in extra : extra_type = 'expression' else : raise ValueError ( 'field `type` for extra {} of {} is required' . format ( extra_name , query_name )) elif not isinstance ( extra_type , str ): raise ValueError ( 'field `type` for extra {} of {} must be a string' . format ( extra_name , query_name )) elif extra_type not in extra_transformers and extra_type not in submission_transformers : raise ValueError ( 'unknown type ` {} ` for extra {} of {} ' . format ( extra_type , extra_name , query_name )) transformer_factory = extra_transformers . get ( extra_type , submission_transformers . get ( extra_type )) extra_source = extra . get ( 'source' ) if extra_type in submission_transformers : if not extra_source : raise ValueError ( 'field `source` for extra {} of {} is required' . format ( extra_name , query_name )) modifiers = { key : value for key , value in extra . items () if key not in ( 'name' , 'type' , 'source' )} else : modifiers = { key : value for key , value in extra . items () if key not in ( 'name' , 'type' )} modifiers [ 'sources' ] = sources try : transformer = transformer_factory ( submission_transformers , extra_name , ** modifiers ) except Exception as e : error = 'error compiling type ` {} ` for extra {} of {} : {} ' . format ( extra_type , extra_name , query_name , e ) raise_from ( type ( e )( error ), None ) else : if extra_type in submission_transformers : transformer = create_extra_transformer ( transformer , extra_source ) extra_data . append (( extra_name , transformer )) self . name = query_name self . query = query self . columns = tuple ( column_data ) self . extras = tuple ( extra_data ) self . tags = tags del self . query_data","title":"compile()"},{"location":"base/databases/#datadog_checks.base.utils.db.core.QueryManager","text":"This class is in charge of running any number of Query instances for a single Check instance. You will most often see it created during Check initialization like this: self . _query_manager = QueryManager ( self , self . execute_query , queries = [ queries . SomeQuery1 , queries . SomeQuery2 , queries . SomeQuery3 , queries . SomeQuery4 , queries . SomeQuery5 , ], tags = self . instance . get ( 'tags' , []), error_handler = self . _error_sanitizer , ) self . check_initializations . append ( self . _query_manager . compile_queries )","title":"QueryManager"},{"location":"base/databases/#datadog_checks.base.utils.db.core.QueryManager.__init__","text":"check ( AgentCheck ) - an instance of a Check executor ( callable ) - a callable accepting a str query as its sole argument and returning a sequence representing either the full result set or an iterator over the result set queries ( List[Query]) ) - a list of Query instances tags ( List[str]) ) - a list of tags to associate with every submission error_handler ( callable ) - a callable accepting a str error as its sole argument and returning a sanitized string, useful for scrubbing potentially sensitive information libraries emit Source code in datadog_checks\\base\\utils\\db\\core.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , check , executor , queries = None , tags = None , error_handler = None ): \"\"\" - **check** (_AgentCheck_) - an instance of a Check - **executor** (_callable_) - a callable accepting a `str` query as its sole argument and returning a sequence representing either the full result set or an iterator over the result set - **queries** (_List[Query])_) - a list of `Query` instances - **tags** (_List[str])_) - a list of tags to associate with every submission - **error_handler** (_callable_) - a callable accepting a `str` error as its sole argument and returning a sanitized string, useful for scrubbing potentially sensitive information libraries emit \"\"\" self . check = check self . executor = executor self . queries = queries or [] self . tags = tags or [] self . error_handler = error_handler custom_queries = list ( self . check . instance . get ( 'custom_queries' , [])) use_global_custom_queries = self . check . instance . get ( 'use_global_custom_queries' , True ) # Handle overrides if use_global_custom_queries == 'extend' : custom_queries . extend ( self . check . init_config . get ( 'global_custom_queries' , [])) elif ( not custom_queries and 'global_custom_queries' in self . check . init_config and is_affirmative ( use_global_custom_queries ) ): custom_queries = self . check . init_config . get ( 'global_custom_queries' , []) # Deduplicate for i , custom_query in enumerate ( iter_unique ( custom_queries ), 1 ): query = Query ( custom_query ) query . query_data . setdefault ( 'name' , 'custom query # {} ' . format ( i )) self . queries . append ( query )","title":"__init__()"},{"location":"base/databases/#datadog_checks.base.utils.db.core.QueryManager.compile_queries","text":"This method compiles every Query object. Source code in datadog_checks\\base\\utils\\db\\core.py 72 73 74 75 76 77 78 79 80 81 82 def compile_queries ( self ): \"\"\"This method compiles every `Query` object.\"\"\" column_transformers = COLUMN_TRANSFORMERS . copy () for submission_method , transformer_name in SUBMISSION_METHODS . items (): method = getattr ( self . check , submission_method ) # Save each method in the initializer -> callable format column_transformers [ transformer_name ] = create_submission_transformer ( method ) for query in self . queries : query . compile ( column_transformers , EXTRA_TRANSFORMERS . copy ())","title":"compile_queries()"},{"location":"base/databases/#datadog_checks.base.utils.db.core.QueryManager.execute","text":"This method is what you call every check run. Source code in datadog_checks\\base\\utils\\db\\core.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def execute ( self ): \"\"\"This method is what you call every check run.\"\"\" logger = self . check . log global_tags = self . tags for query in self . queries : query_name = query . name query_columns = query . columns query_extras = query . extras query_tags = query . tags num_columns = len ( query_columns ) try : rows = self . execute_query ( query . query ) except Exception as e : if self . error_handler : logger . error ( 'Error querying %s : %s ' , query_name , self . error_handler ( str ( e ))) else : logger . error ( 'Error querying %s : %s ' , query_name , e ) continue for row in rows : if not row : logger . debug ( 'Query %s returned an empty result' , query_name ) continue if num_columns != len ( row ): logger . error ( 'Query %s expected %d column %s , got %d ' , query_name , num_columns , 's' if num_columns > 1 else '' , len ( row ), ) continue sources = {} submission_queue = [] tags = list ( global_tags ) tags . extend ( query_tags ) for ( column_name , transformer ), value in zip ( query_columns , row ): # Columns can be ignored via configuration if not column_name : continue sources [ column_name ] = value column_type , transformer = transformer # The transformer can be None for `source` types. Those such columns do not submit # anything but are collected into the row values for other columns to reference. if transformer is None : continue elif column_type == 'tag' : tags . append ( transformer ( None , value )) else : submission_queue . append (( transformer , value )) for transformer , value in submission_queue : transformer ( sources , value , tags = tags ) for name , transformer in query_extras : try : result = transformer ( sources , tags = tags ) except Exception as e : logger . error ( 'Error transforming %s : %s ' , name , e ) continue else : if result is not None : sources [ name ] = result","title":"execute()"},{"location":"base/databases/#datadog_checks.base.utils.db.core.QueryManager.execute_query","text":"Called by execute , this triggers query execution to check for errors immediately in a way that is compatible with any library. If there are no errors, this is guaranteed to return an iterator over the result set. Source code in datadog_checks\\base\\utils\\db\\core.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def execute_query ( self , query ): \"\"\" Called by `execute`, this triggers query execution to check for errors immediately in a way that is compatible with any library. If there are no errors, this is guaranteed to return an iterator over the result set. \"\"\" rows = self . executor ( query ) if rows is None : return iter ([]) else : rows = iter ( rows ) # Ensure we trigger query execution try : first_row = next ( rows ) except StopIteration : return iter ([]) return chain (( first_row ,), rows )","title":"execute_query()"},{"location":"base/databases/#transformers","text":"","title":"Transformers"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ColumnTransformers","text":"","title":"ColumnTransformers"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ColumnTransformers.match","text":"This is used for querying unstructured data. For example, say you want to collect the fields named foo and bar . Typically, they would be stored like: foo bar 4 2 and would be queried like: SELECT foo , bar FROM ... Often, you will instead find data stored in the following format: metric value foo 4 bar 2 and would be queried like: SELECT metric , value FROM ... In this case, the metric column stores the name with which to match on and its value is stored in a separate column. The required items modifier is a mapping of matched names to column data values. Consider the values to be exactly the same as the entries in the columns top level field. You must also define a source modifier either for this transformer itself or in the values of items (which will take precedence). The source will be treated as the value of the match. Say this is your configuration: query : SELECT source1, source2, metric FROM TABLE columns : - name : value1 type : source - name : value2 type : source - name : metric_name type : match source : value1 items : foo : name : test.foo type : gauge source : value2 bar : name : test.bar type : monotonic_gauge and the result set is: source1 source2 metric 1 2 foo 3 4 baz 5 6 bar Here's what would be submitted: foo - test.foo as a gauge with a value of 2 bar - test.bar.total as a gauge and test.bar.count as a monotonic_count , both with a value of 5 baz - nothing since it was not defined as a match item Source code in datadog_checks\\base\\utils\\db\\transform.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def get_match ( transformers , column_name , ** modifiers ): \"\"\" This is used for querying unstructured data. For example, say you want to collect the fields named `foo` and `bar`. Typically, they would be stored like: | foo | bar | | --- | --- | | 4 | 2 | and would be queried like: ```sql SELECT foo, bar FROM ... ``` Often, you will instead find data stored in the following format: | metric | value | | ------ | ----- | | foo | 4 | | bar | 2 | and would be queried like: ```sql SELECT metric, value FROM ... ``` In this case, the `metric` column stores the name with which to match on and its `value` is stored in a separate column. The required `items` modifier is a mapping of matched names to column data values. Consider the values to be exactly the same as the entries in the `columns` top level field. You must also define a `source` modifier either for this transformer itself or in the values of `items` (which will take precedence). The source will be treated as the value of the match. Say this is your configuration: ```yaml query: SELECT source1, source2, metric FROM TABLE columns: - name: value1 type: source - name: value2 type: source - name: metric_name type: match source: value1 items: foo: name: test.foo type: gauge source: value2 bar: name: test.bar type: monotonic_gauge ``` and the result set is: | source1 | source2 | metric | | ------- | ------- | ------ | | 1 | 2 | foo | | 3 | 4 | baz | | 5 | 6 | bar | Here's what would be submitted: - `foo` - `test.foo` as a `gauge` with a value of `2` - `bar` - `test.bar.total` as a `gauge` and `test.bar.count` as a `monotonic_count`, both with a value of `5` - `baz` - nothing since it was not defined as a match item \"\"\" # Do work in a separate function to avoid having to `del` a bunch of variables compiled_items = _compile_match_items ( transformers , modifiers ) def match ( sources , value , ** kwargs ): if value in compiled_items : source , transformer = compiled_items [ value ] transformer ( sources , sources [ source ], ** kwargs ) return match","title":"match()"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ColumnTransformers.monotonic_gauge","text":"Send the result as both a gauge suffixed by .total and a monotonic_count suffixed by .count . Source code in datadog_checks\\base\\utils\\db\\transform.py 51 52 53 54 55 56 57 58 59 60 61 62 def get_monotonic_gauge ( transformers , column_name , ** modifiers ): \"\"\" Send the result as both a `gauge` suffixed by `.total` and a `monotonic_count` suffixed by `.count`. \"\"\" gauge = transformers [ 'gauge' ]( transformers , ' {} .total' . format ( column_name ), ** modifiers ) monotonic_count = transformers [ 'monotonic_count' ]( transformers , ' {} .count' . format ( column_name ), ** modifiers ) def monotonic_gauge ( _ , value , ** kwargs ): gauge ( _ , value , ** kwargs ) monotonic_count ( _ , value , ** kwargs ) return monotonic_gauge","title":"monotonic_gauge()"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ColumnTransformers.service_check","text":"Submit a service check. The required modifier status_map is a mapping of values to statuses. Valid statuses include: OK WARNING CRITICAL UNKNOWN Any encountered values that are not defined will be sent as UNKNOWN . Source code in datadog_checks\\base\\utils\\db\\transform.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def get_service_check ( transformers , column_name , ** modifiers ): \"\"\" Submit a service check. The required modifier `status_map` is a mapping of values to statuses. Valid statuses include: - `OK` - `WARNING` - `CRITICAL` - `UNKNOWN` Any encountered values that are not defined will be sent as `UNKNOWN`. \"\"\" # Do work in a separate function to avoid having to `del` a bunch of variables status_map = _compile_service_check_statuses ( modifiers ) service_check_method = transformers [ '__service_check' ]( transformers , column_name , ** modifiers ) def service_check ( _ , value , ** kwargs ): service_check_method ( _ , status_map . get ( value , ServiceCheck . UNKNOWN ), ** kwargs ) return service_check","title":"service_check()"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ColumnTransformers.tag","text":"Convert a column to a tag that will be used in every subsequent submission. For example, if you named the column env and the column returned the value prod1 , all submissions from that row will be tagged by env:prod1 . This also accepts an optional modifier called boolean that when set to true will transform the result to the string true or false . So for example if you named the column alive and the result was the number 0 the tag will be alive:false . Source code in datadog_checks\\base\\utils\\db\\transform.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def get_tag ( transformers , column_name , ** modifiers ): \"\"\" Convert a column to a tag that will be used in every subsequent submission. For example, if you named the column `env` and the column returned the value `prod1`, all submissions from that row will be tagged by `env:prod1`. This also accepts an optional modifier called `boolean` that when set to `true` will transform the result to the string `true` or `false`. So for example if you named the column `alive` and the result was the number `0` the tag will be `alive:false`. \"\"\" template = ' {} :{{}}' . format ( column_name ) boolean = is_affirmative ( modifiers . pop ( 'boolean' , None )) def tag ( _ , value , ** kwargs ): if boolean : value = str ( is_affirmative ( value )) . lower () return template . format ( value ) return tag","title":"tag()"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ColumnTransformers.temporal_percent","text":"Send the result as percentage of time since the last check run as a rate . For example, say the result is a forever increasing counter representing the total time spent pausing for garbage collection since start up. That number by itself is quite useless, but as a percentage of time spent pausing since the previous collection interval it becomes a useful metric. There is one required parameter called scale that indicates what unit of time the result should be considered. Valid values are: second millisecond microsecond nanosecond You may also define the unit as an integer number of parts compared to seconds e.g. millisecond is equivalent to 1000 . Source code in datadog_checks\\base\\utils\\db\\transform.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get_temporal_percent ( transformers , column_name , ** modifiers ): \"\"\" Send the result as percentage of time since the last check run as a `rate`. For example, say the result is a forever increasing counter representing the total time spent pausing for garbage collection since start up. That number by itself is quite useless, but as a percentage of time spent pausing since the previous collection interval it becomes a useful metric. There is one required parameter called `scale` that indicates what unit of time the result should be considered. Valid values are: - `second` - `millisecond` - `microsecond` - `nanosecond` You may also define the unit as an integer number of parts compared to seconds e.g. `millisecond` is equivalent to `1000`. \"\"\" scale = modifiers . pop ( 'scale' , None ) if scale is None : raise ValueError ( 'the `scale` parameter is required' ) if isinstance ( scale , str ): scale = constants . TIME_UNITS . get ( scale . lower ()) if scale is None : raise ValueError ( 'the `scale` parameter must be one of: {} ' . format ( ' | ' . join ( sorted ( constants . TIME_UNITS ))) ) elif not isinstance ( scale , int ): raise ValueError ( 'the `scale` parameter must be an integer representing parts of a second e.g. 1000 for millisecond' ) rate = transformers [ 'rate' ]( transformers , column_name , ** modifiers ) def temporal_percent ( _ , value , ** kwargs ): rate ( _ , total_time_to_temporal_percent ( float ( value ), scale = scale ), ** kwargs ) return temporal_percent","title":"temporal_percent()"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ColumnTransformers.time_elapsed","text":"Send the number of seconds elapsed from a time in the past as a gauge . For example, if the result is an instance of datetime.datetime representing 5 seconds ago, then this would submit with a value of 5 . The optional modifier format indicates what format the result is in. By default it is native , assuming the underlying library provides timestamps as datetime objects. If it does not and passes them through directly as strings, you must provide the expected timestamp format using the supported codes . Note The code %z (lower case) is not supported on Windows. Source code in datadog_checks\\base\\utils\\db\\transform.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def get_time_elapsed ( transformers , column_name , ** modifiers ): \"\"\" Send the number of seconds elapsed from a time in the past as a `gauge`. For example, if the result is an instance of [datetime.datetime](https://docs.python.org/3/library/datetime.html#datetime.datetime) representing 5 seconds ago, then this would submit with a value of `5`. The optional modifier `format` indicates what format the result is in. By default it is `native`, assuming the underlying library provides timestamps as `datetime` objects. If it does not and passes them through directly as strings, you must provide the expected timestamp format using the [supported codes](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes). !!! note The code `%z` (lower case) is not supported on Windows. \"\"\" time_format = modifiers . pop ( 'format' , 'native' ) if not isinstance ( time_format , str ): raise ValueError ( 'the `format` parameter must be a string' ) gauge = transformers [ 'gauge' ]( transformers , column_name , ** modifiers ) if time_format == 'native' : def time_elapsed ( _ , value , ** kwargs ): value = normalize_datetime ( value ) gauge ( _ , ( datetime . now ( value . tzinfo ) - value ) . total_seconds (), ** kwargs ) else : def time_elapsed ( _ , value , ** kwargs ): value = normalize_datetime ( datetime . strptime ( value , time_format )) gauge ( _ , ( datetime . now ( value . tzinfo ) - value ) . total_seconds (), ** kwargs ) return time_elapsed","title":"time_elapsed()"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ExtraTransformers","text":"Every column transformer (except tag ) is supported at this level, the only difference being one must set a source to retrieve the desired value. So for example here: columns : - name : foo.bar type : rate extras : - name : foo.current type : gauge source : foo.bar the metric foo.current will be sent as a gauge will the value of foo.bar .","title":"ExtraTransformers"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ExtraTransformers.expression","text":"This allows the evaluation of a limited subset of Python syntax and built-in functions. columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.free expression : disk.total - disk.used submit_type : gauge For brevity, if the expression attribute exists and type does not then it is assumed the type is expression . The submit_type can be any transformer and any extra options are passed down to it. The result of every expression is stored, so in lieu of a submit_type the above example could also be written as: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : free expression : disk.total - disk.used - name : disk.free type : gauge source : free The order matters though, so for example the following will fail: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.free type : gauge source : free - name : free expression : disk.total - disk.used since the source free does not yet exist. Source code in datadog_checks\\base\\utils\\db\\transform.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def get_expression ( transformers , name , ** modifiers ): \"\"\" This allows the evaluation of a limited subset of Python syntax and built-in functions. ```yaml columns: - name: disk.total type: gauge - name: disk.used type: gauge extras: - name: disk.free expression: disk.total - disk.used submit_type: gauge ``` For brevity, if the `expression` attribute exists and `type` does not then it is assumed the type is `expression`. The `submit_type` can be any transformer and any extra options are passed down to it. The result of every expression is stored, so in lieu of a `submit_type` the above example could also be written as: ```yaml columns: - name: disk.total type: gauge - name: disk.used type: gauge extras: - name: free expression: disk.total - disk.used - name: disk.free type: gauge source: free ``` The order matters though, so for example the following will fail: ```yaml columns: - name: disk.total type: gauge - name: disk.used type: gauge extras: - name: disk.free type: gauge source: free - name: free expression: disk.total - disk.used ``` since the source `free` does not yet exist. \"\"\" available_sources = modifiers . pop ( 'sources' ) expression = modifiers . pop ( 'expression' , None ) if expression is None : raise ValueError ( 'the `expression` parameter is required' ) elif not isinstance ( expression , str ): raise ValueError ( 'the `expression` parameter must be a string' ) elif not expression : raise ValueError ( 'the `expression` parameter must not be empty' ) if not modifiers . pop ( 'verbose' , False ): # Sort the sources in reverse order of length to prevent greedy matching available_sources = sorted ( available_sources , key = lambda s : - len ( s )) # Escape special characters, mostly for the possible dots in metric names available_sources = list ( map ( re . escape , available_sources )) # Finally, utilize the order by relying on the guarantees provided by the alternation operator available_sources = '|' . join ( available_sources ) expression = re . sub ( SOURCE_PATTERN . format ( available_sources ), # Replace by the particular source that matched lambda match_obj : 'SOURCES[\" {} \"]' . format ( match_obj . group ( 1 )), expression , ) expression = compile ( expression , filename = name , mode = 'eval' ) del available_sources if 'submit_type' in modifiers : if modifiers [ 'submit_type' ] not in transformers : raise ValueError ( 'unknown submit_type ` {} `' . format ( modifiers [ 'submit_type' ])) submit_method = transformers [ modifiers . pop ( 'submit_type' )]( transformers , name , ** modifiers ) submit_method = create_extra_transformer ( submit_method ) def execute_expression ( sources , ** kwargs ): result = eval ( expression , ALLOWED_GLOBALS , { 'SOURCES' : sources }) submit_method ( sources , result , ** kwargs ) return result else : def execute_expression ( sources , ** kwargs ): return eval ( expression , ALLOWED_GLOBALS , { 'SOURCES' : sources }) return execute_expression","title":"expression()"},{"location":"base/databases/#datadog_checks.base.utils.db.transform.ExtraTransformers.percent","text":"Send a percentage based on 2 sources as a gauge . The required modifiers are part and total . For example, if you have this configuration: columns : - name : disk.total type : gauge - name : disk.used type : gauge extras : - name : disk.utilized type : percent part : disk.used total : disk.total then the extra metric disk.utilized would be sent as a gauge calculated as disk.used / disk.total * 100 . If the source of total is 0 , then the submitted value will always be sent as 0 too. Source code in datadog_checks\\base\\utils\\db\\transform.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def get_percent ( transformers , name , ** modifiers ): \"\"\" Send a percentage based on 2 sources as a `gauge`. The required modifiers are `part` and `total`. For example, if you have this configuration: ```yaml columns: - name: disk.total type: gauge - name: disk.used type: gauge extras: - name: disk.utilized type: percent part: disk.used total: disk.total ``` then the extra metric `disk.utilized` would be sent as a `gauge` calculated as `disk.used / disk.total * 100`. If the source of `total` is `0`, then the submitted value will always be sent as `0` too. \"\"\" available_sources = modifiers . pop ( 'sources' ) part = modifiers . pop ( 'part' , None ) if part is None : raise ValueError ( 'the `part` parameter is required' ) elif not isinstance ( part , str ): raise ValueError ( 'the `part` parameter must be a string' ) elif part not in available_sources : raise ValueError ( 'the `part` parameter ` {} ` is not an available source' . format ( part )) total = modifiers . pop ( 'total' , None ) if total is None : raise ValueError ( 'the `total` parameter is required' ) elif not isinstance ( total , str ): raise ValueError ( 'the `total` parameter must be a string' ) elif total not in available_sources : raise ValueError ( 'the `total` parameter ` {} ` is not an available source' . format ( total )) del available_sources gauge = transformers [ 'gauge' ]( transformers , name , ** modifiers ) gauge = create_extra_transformer ( gauge ) def percent ( sources , ** kwargs ): gauge ( sources , compute_percent ( sources [ part ], sources [ total ]), ** kwargs ) return percent","title":"percent()"},{"location":"base/http/","text":"HTTP Whenever you need to make HTTP requests, the base class provides a convenience member that has the same interface as the popular requests library and ensures consistent behavior across all integrations. The wrapper automatically parses and uses configuration from the instance , init_config , and Agent config. Also, this is only done once during initialization and cached to reduce the overhead of every call. All you have to do is e.g.: response = self . http . get ( url ) and the wrapper will pass the right things to requests . All methods accept optional keyword arguments like stream , etc. Any method-level option will override configuration. So for example if tls_verify was set to false and you do self.http.get(url, verify=True) , then SSL certificates will be verified on that particular request. You can use the keyword argument persist to override persist_connections . There is also support for non-standard or legacy configurations with the HTTP_CONFIG_REMAPPER class attribute. For example: class MyCheck ( AgentCheck ): HTTP_CONFIG_REMAPPER = { 'disable_ssl_validation' : { 'name' : 'tls_verify' , 'default' : False , 'invert' : True , }, ... } ... Options \u00b6 Some options can be set globally in init_config (with instances taking precedence). For complete documentation of every option, see the associated configuration templates for the instances and init_config sections. auth_type aws_host aws_region aws_service connect_timeout extra_headers headers kerberos_auth kerberos_cache kerberos_delegate kerberos_force_initiate kerberos_hostname kerberos_keytab kerberos_principal log_requests ntlm_domain password persist_connections proxy read_timeout skip_proxy tls_ca_cert tls_cert tls_use_host_header tls_ignore_warning tls_private_key tls_verify timeout username Future \u00b6 Support for UNIX sockets Support for configuring cookies! Since they can be set globally, per-domain, and even per-path, the configuration may be complex if not thought out adequately. We'll discuss options for what that might look like. Only our spark and cisco_aci checks currently set cookies, and that is based on code logic, not configuration.","title":"HTTP"},{"location":"base/http/#options","text":"Some options can be set globally in init_config (with instances taking precedence). For complete documentation of every option, see the associated configuration templates for the instances and init_config sections. auth_type aws_host aws_region aws_service connect_timeout extra_headers headers kerberos_auth kerberos_cache kerberos_delegate kerberos_force_initiate kerberos_hostname kerberos_keytab kerberos_principal log_requests ntlm_domain password persist_connections proxy read_timeout skip_proxy tls_ca_cert tls_cert tls_use_host_header tls_ignore_warning tls_private_key tls_verify timeout username","title":"Options"},{"location":"base/http/#future","text":"Support for UNIX sockets Support for configuring cookies! Since they can be set globally, per-domain, and even per-path, the configuration may be complex if not thought out adequately. We'll discuss options for what that might look like. Only our spark and cisco_aci checks currently set cookies, and that is based on code logic, not configuration.","title":"Future"},{"location":"ddev/cli/","text":"ddev Usage: ddev [OPTIONS] COMMAND [ARGS]... Options: -c, --core Work on `integrations-core`. -e, --extras Work on `integrations-extras`. -a, --agent Work on `datadog-agent`. -x, --here Work on the current location. --color / --no-color Whether or not to display colored output (default true). -q, --quiet Silence output -d, --debug Include debug output --version Show the version and exit. agent \u00b6 A collection of tasks related to the Datadog Agent Usage: ddev agent [OPTIONS] COMMAND [ARGS]... changelog \u00b6 Generates a markdown file containing the list of checks that changed for a given Agent release. Agent version numbers are derived inspecting tags on integrations-core so running this tool might provide unexpected results if the repo is not up to date with the Agent release process. If neither --since or --to are passed (the most common use case), the tool will generate the whole changelog since Agent version 6.3.0 (before that point we don't have enough information to build the log). Usage: ddev agent changelog [OPTIONS] Options: --since TEXT Initial Agent version --to TEXT Final Agent version -w, --write Write to the changelog file, if omitted contents will be printed to stdout -f, --force Replace an existing file integrations \u00b6 Generates a markdown file containing the list of integrations shipped in a given Agent release. Agent version numbers are derived inspecting tags on integrations-core so running this tool might provide unexpected results if the repo is not up to date with the Agent release process. If neither --since or --to are passed (the most common use case), the tool will generate the list for every Agent since version 6.3.0 (before that point we don't have enough information to build the log). Usage: ddev agent integrations [OPTIONS] Options: --since TEXT Initial Agent version --to TEXT Final Agent version -w, --write Write to file, if omitted contents will be printed to stdout -f, --force Replace an existing file requirements \u00b6 Write the requirements-agent-release.txt file at the root of the repo listing all the Agent-based integrations pinned at the version they currently have in HEAD. Usage: ddev agent requirements [OPTIONS] ci \u00b6 CI related utils. Anything here should be considered experimental. Usage: ddev ci [OPTIONS] COMMAND [ARGS]... setup \u00b6 Run CI setup scripts Usage: ddev ci setup [OPTIONS] [CHECKS]... Options: --changed Only target changed checks clean \u00b6 Remove build and test artifacts for the given CHECK. If CHECK is not specified, the current working directory is used. Usage: ddev clean [OPTIONS] [CHECK] Options: -c, --compiled-only Remove compiled files only (__pycache__, *.pyc, *.pyo, *.pyd, *.whl). -a, --all Disable the detection of a project's dedicated virtual env and/or editable installation. By default, these will not be considered. -f, --force If set and the command is run from the root directory, allow removing build and test artifacts (.benchmarks, .pytest_cache, .cache, *.egg-info, .tox, .coverage, build, .eggs, dist). -v, --verbose Shows removed paths. config \u00b6 Manage the config file Usage: ddev config [OPTIONS] COMMAND [ARGS]... explore \u00b6 Open the config location in your file manager. Usage: ddev config explore [OPTIONS] find \u00b6 Show the location of the config file. Usage: ddev config find [OPTIONS] restore \u00b6 Restore the config file to default settings. Usage: ddev config restore [OPTIONS] set \u00b6 Assigns values to config file entries. If the value is omitted, you will be prompted, with the input hidden if it is sensitive. $ ddev config set github.user foo New setting: [github] user = \"foo\" You can also assign values on a per-org basis. $ ddev config set orgs.<ORG_NAME>.api_key New setting: [orgs.<ORG_NAME>] api_key = \"***********\" Usage: ddev config set [OPTIONS] KEY [VALUE] show \u00b6 Show the contents of the config file. Usage: ddev config show [OPTIONS] Options: -a, --all No not scrub secret fields update \u00b6 Update the config file with any new fields. Usage: ddev config update [OPTIONS] create \u00b6 Create scaffolding for a new integration. Usage: ddev create [OPTIONS] NAME Options: -t, --type [check|jmx|logs|tile] The type of integration to create -l, --location TEXT The directory where files will be written -ni, --non-interactive Disable prompting for fields -q, --quiet Show less output -n, --dry-run Only show what would be created dep \u00b6 Manage dependencies Usage: ddev dep [OPTIONS] COMMAND [ARGS]... freeze \u00b6 Combine all dependencies for the Agent's static environment. Usage: ddev dep freeze [OPTIONS] pin \u00b6 Pin a dependency for all checks that require it. This can also resolve transient dependencies. Setting the version to none will remove the package. You can specify an unlimited number of additional checks to apply the pin for via arguments. Usage: ddev dep pin [OPTIONS] PACKAGE VERSION [CHECKS]... Options: -m, --marker TEXT Environment marker to use -r, --resolve Resolve transient dependencies -l, --lazy Do not attempt to upgrade transient dependencies when resolving -q, --quiet resolve \u00b6 Resolve transient dependencies for any number of checks. If you want to do this en masse, put all . Usage: ddev dep resolve [OPTIONS] CHECKS... Options: -l, --lazy Do not attempt to upgrade transient dependencies -q, --quiet docs \u00b6 Manage documentation Usage: ddev docs [OPTIONS] COMMAND [ARGS]... build \u00b6 Build documentation. Usage: ddev docs build [OPTIONS] Options: -v, --verbose Increase verbosity (can be used additively) push \u00b6 Push built documentation. Usage: ddev docs push [OPTIONS] [BRANCH] serve \u00b6 Serve and view documentation in a web browser. Usage: ddev docs serve [OPTIONS] Options: -n, --no-open Do not open the documentation in a web browser -v, --verbose Increase verbosity (can be used additively) env \u00b6 Manage environments Usage: ddev env [OPTIONS] COMMAND [ARGS]... check \u00b6 Run an Agent check. Usage: ddev env check [OPTIONS] CHECK [ENV] Options: -r, --rate Compute rates by running the check twice with a pause between each run -t, --times INTEGER Number of times to run the check --pause INTEGER Number of milliseconds to pause between multiple check runs -d, --delay INTEGER Delay in milliseconds between running the check and grabbing what was collected -l, --log-level TEXT Set the log level (default `off`) --json Format the aggregator and check runner output as JSON -b, --breakpoint INTEGER Line number to start a PDB session (0: first line, -1: last line) --config TEXT Path to a JSON check configuration to use --jmx-list TEXT JMX metrics listing method ls \u00b6 List active or available environments. Usage: ddev env ls [OPTIONS] [CHECKS]... prune \u00b6 Remove all configuration for environments. Usage: ddev env prune [OPTIONS] Options: -f, --force reload \u00b6 Restart an Agent to detect environment changes. Usage: ddev env reload [OPTIONS] CHECK [ENV] start \u00b6 Start an environment. Usage: ddev env start [OPTIONS] CHECK ENV Options: -a, --agent TEXT The agent build to use e.g. a Docker image like `datadog/agent:6.5.2`. For Docker environments you can use an integer corresponding to fields in the config (agent5, agent6, etc.) -py, --python INTEGER The version of Python to use. Defaults to 2 if no tox Python is specified. --dev / --prod Whether to use the latest version of a check or what is shipped --base Whether to use the latest version of the base check or what is shipped -e, --env-vars TEXT ENV Variable that should be passed to the Agent container. Ex: -e DD_URL=app.datadoghq.com -e DD_API_KEY=123456 -o, --org-name TEXT The org to use for data submission. -pm, --profile-memory Whether to collect metrics about memory usage stop \u00b6 Stop environments, use \"all\" as check argument to stop everything. Usage: ddev env stop [OPTIONS] CHECK [ENV] test \u00b6 Test an environment. Usage: ddev env test [OPTIONS] [CHECKS]... Options: -a, --agent TEXT The agent build to use e.g. a Docker image like `datadog/agent:6.5.2`. For Docker environments you can use an integer corresponding to fields in the config (agent5, agent6, etc.) -py, --python INTEGER The version of Python to use. Defaults to 2 if no tox Python is specified. --dev / --prod Whether to use the latest version of a check or what is shipped --base Whether to use the latest version of the base check or what is shipped -e, --env-vars TEXT ENV Variable that should be passed to the Agent container. Ex: -e DD_URL=app.datadoghq.com -e DD_API_KEY=123456 -ne, --new-env Execute setup and tear down actions -pm, --profile-memory Whether to collect metrics about memory usage -j, --junit Generate junit reports meta \u00b6 Anything here should be considered experimental. This meta namespace can be used for an arbitrary number of niche or beta features without bloating the root namespace. Usage: ddev meta [OPTIONS] COMMAND [ARGS]... catalog \u00b6 Create a catalog with information about integrations Usage: ddev meta catalog [OPTIONS] CHECKS... Options: -f, --file TEXT Output to file (it will be overwritten), you can pass \"tmp\" to generate a temporary file -m, --markdown Output to markdown instead of CSV changes \u00b6 Show changes since a specific date. Usage: ddev meta changes [OPTIONS] SINCE Options: -o, --out Output to file --eager Skip validation of commit subjects dash \u00b6 Dashboard utilities Usage: ddev meta dash [OPTIONS] COMMAND [ARGS]... export \u00b6 Export a Dashboard as JSON Usage: ddev meta dash export [OPTIONS] URL [INTEGRATION] jmx \u00b6 JMX utilities Usage: ddev meta jmx [OPTIONS] COMMAND [ARGS]... query-endpoint \u00b6 Query endpoint for JMX info Usage: ddev meta jmx query-endpoint [OPTIONS] HOST PORT [DOMAIN] prom \u00b6 Prometheus utilities Usage: ddev meta prom [OPTIONS] COMMAND [ARGS]... info \u00b6 Show metric info from a Prometheus endpoint. Example: $ ddev meta prom info :8080/_status/vars Usage: ddev meta prom info [OPTIONS] ENDPOINT parse \u00b6 Interactively parse metric info from a Prometheus endpoint and write it to metadata.csv. Usage: ddev meta prom parse [OPTIONS] ENDPOINT CHECK Options: -x, --here Output to the current location scripts \u00b6 Miscellaneous scripts that may be useful Usage: ddev meta scripts [OPTIONS] COMMAND [ARGS]... email2ghuser \u00b6 Given an email, attempt to find a Github username associated with the email. $ ddev meta scripts email2ghuser example@datadoghq.com Usage: ddev meta scripts email2ghuser [OPTIONS] EMAIL metrics2md \u00b6 Convert a check's metadata.csv file to a Markdown table, which will be copied to your clipboard. By default it will be compact and only contain the most useful fields. If you wish to use arbitrary metric data, you may set the check to cb to target the current contents of your clipboard. Usage: ddev meta scripts metrics2md [OPTIONS] CHECK [FIELDS]... remove-labels \u00b6 Remove all labels from an issue or pull request. This is useful when there are too many labels and its state cannot be modified (known GitHub issue). $ ddev meta scripts remove-labels 5626 Usage: ddev meta scripts remove-labels [OPTIONS] ISSUE_NUMBER upgrade-python \u00b6 Upgrade the Python version of all test environments. $ ddev meta scripts upgrade-python 3.8 Usage: ddev meta scripts upgrade-python [OPTIONS] NEW_VERSION [OLD_VERSION] snmp \u00b6 SNMP utilities Usage: ddev meta snmp [OPTIONS] COMMAND [ARGS]... translate-profile \u00b6 Do OID translation in a SNMP profile. This isn't a plain replacement, as it doesn't preserve comments and indent, but it should automate most of the work. You'll need to install pysnmp and pysnmp-mibs manually beforehand. Usage: ddev meta snmp translate-profile [OPTIONS] PROFILE_PATH release \u00b6 Manage the release of checks Usage: ddev release [OPTIONS] COMMAND [ARGS]... build \u00b6 Build a wheel for a check as it is on the repo HEAD Usage: ddev release build [OPTIONS] CHECK Options: -s, --sdist changelog \u00b6 Perform the operations needed to update the changelog. This method is supposed to be used by other tasks and not directly. Usage: ddev release changelog [OPTIONS] CHECK VERSION [OLD_VERSION] Options: --initial -q, --quiet -n, --dry-run -o, --output-file TEXT [default: CHANGELOG.md] -tp, --tag-prefix TEXT [default: v] make \u00b6 Perform a set of operations needed to release checks: update the version in __about__.py update the changelog update the requirements-agent-release.txt file update in-toto metadata commit the above changes You can release everything at once by setting the check to all . If you run into issues signing: Ensure you did gpg --import <YOUR_KEY_ID>.gpg.pub Usage: ddev release make [OPTIONS] CHECKS... Options: --version TEXT --new Ensure versions are at 1.0.0 --skip-sign Skip the signing of release metadata --sign-only Only sign release metadata --exclude TEXT Comma-separated list of checks to skip show \u00b6 To avoid GitHub's public API rate limits, you need to set github.user / github.token in your config file or use the DD_GITHUB_USER / DD_GITHUB_TOKEN environment variables. Usage: ddev release show [OPTIONS] COMMAND [ARGS]... changes \u00b6 Show all the pending PRs for a given check. Usage: ddev release show changes [OPTIONS] CHECK Options: -n, --dry-run ready \u00b6 Show all the checks that can be released. Usage: ddev release show ready [OPTIONS] Options: -q, --quiet tag \u00b6 Tag the HEAD of the git repo with the current release number for a specific check. The tag is pushed to origin by default. You can tag everything at once by setting the check to all . Notice: specifying a different version than the one in __about__.py is a maintenance task that should be run under very specific circumstances (e.g. re-align an old release performed on the wrong commit). Usage: ddev release tag [OPTIONS] CHECK [VERSION] Options: --push / --no-push -n, --dry-run testable \u00b6 Create a Trello card for changes since a previous release (referenced by BASE_REF ) that need to be tested for the next release (referenced by TARGET_REF ). BASE_REF and TARGET_REF can be any valid git references. It practice, you should use either: A tag: 7.16.1 , 7.17.0-rc.4 , ... A release branch: 6.16.x , 7.17.x , ... The master branch. NOTE: using a minor version shorthand (e.g. 7.16 ) is not supported, as it is ambiguous. Example: assuming we are working on the release of 7.17.0, we can... Create cards for changes between a previous Agent release and master (useful when preparing an initial RC): $ ddev release testable 7.16.1 origin/master Create cards for changes between a previous RC and master (useful when preparing a new RC, and a separate release branch was not created yet): $ ddev release testable 7.17.0-rc.2 origin/master Create cards for changes between a previous RC and a release branch (useful to only review changes in a release branch that has diverged from master ): $ ddev release testable 7.17.0-rc.4 7.17.x Create cards for changes between two arbitrary tags, e.g. between RCs: $ ddev release testable 7.17.0-rc.4 7.17.0-rc.5 TIP: run with ddev -x release testable to force the use of the current directory. To avoid GitHub's public API rate limits, you need to set github.user / github.token in your config file or use the DD_GITHUB_USER / DD_GITHUB_TOKEN environment variables. To use Trello: 1. Go to https://trello.com/app-key and copy your API key. 2. Run ddev config set trello.key and paste your API key. 3. Go to https://trello.com/1/authorize?key=key&name=name&scope=read,write&expiration=never&response_type=token , where key is your API key and name is the name to give your token, e.g. ReleaseTestingYourName. Authorize access and copy your token. 4. Run ddev config set trello.token and paste your token. Usage: ddev release testable [OPTIONS] BASE_REF TARGET_REF Options: --milestone TEXT The PR milestone to filter by -n, --dry-run Only show the changes upload \u00b6 Release a specific check to PyPI as it is on the repo HEAD. Usage: ddev release upload [OPTIONS] CHECK Options: -s, --sdist -n, --dry-run run \u00b6 Run commands in the proper repo. Usage: ddev run [OPTIONS] [ARGS]... test \u00b6 Run tests for Agent-based checks. If no checks are specified, this will only test checks that were changed compared to the master branch. You can also select specific comma-separated environments to test like so: $ ddev test mysql:mysql57,maria10130 Usage: ddev test [OPTIONS] [CHECKS]... Options: -fs, --format-style Run only the code style formatter -s, --style Run only style checks -b, --bench Run only benchmarks --latest-metrics Only verify support of new metrics --e2e Run only end-to-end tests -c, --cov Measure code coverage -cm, --cov-missing Show line numbers of statements that were not executed -j, --junit Generate junit reports -m, --marker TEXT Only run tests matching given marker expression -k, --filter TEXT Only run tests matching given substring expression --pdb Drop to PDB on first failure, then end test session -d, --debug Set the log level to debug -v, --verbose Increase verbosity (can be used additively) -l, --list List available test environments --passenv TEXT Additional environment variables to pass down --changed Only test changed checks --cov-keep Keep coverage reports --skip-env Skip environment creation and assume it is already running -pa, --pytest-args TEXT Additional arguments to pytest validate \u00b6 Verify certain aspects of the repo Usage: ddev validate [OPTIONS] COMMAND [ARGS]... agent-reqs \u00b6 Verify that the checks versions are in sync with the requirements-agent-release.txt file Usage: ddev validate agent-reqs [OPTIONS] ci \u00b6 Validate CI infrastructure configuration. Usage: ddev validate ci [OPTIONS] Options: --fix Attempt to fix errors config \u00b6 Validate default configuration files. Usage: ddev validate config [OPTIONS] [CHECK] Options: --sync Generate example configuration files based on specifications dashboards \u00b6 Validate all Dashboard definition files. Usage: ddev validate dashboards [OPTIONS] dep \u00b6 This command will: Verify the uniqueness of dependency versions across all checks. Verify all the dependencies are pinned. Verify the embedded Python environment defined in the base check and requirements listed in every integration are compatible. Usage: ddev validate dep [OPTIONS] imports \u00b6 Validate proper imports in checks. Usage: ddev validate imports [OPTIONS] [CHECKS]... Options: --autofix Apply suggested fix legacy-signature \u00b6 Validate that no integration uses the legacy signature. Usage: ddev validate legacy-signature [OPTIONS] [CHECK] manifest \u00b6 Validate manifest.json files. Usage: ddev validate manifest [OPTIONS] Options: --fix Attempt to fix errors -i, --include-extras Include optional fields metadata \u00b6 Validates metadata.csv files If check is specified, only the check will be validated, otherwise all metadata files in the repo will be. Usage: ddev validate metadata [OPTIONS] [CHECK] Options: --check-duplicates Output warnings if there are duplicate short names and descriptions service-checks \u00b6 Validate all service_checks.json files. Usage: ddev validate service-checks [OPTIONS] Options: --sync Generate example configuration files based on specifications","title":"CLI"},{"location":"ddev/cli/#agent","text":"A collection of tasks related to the Datadog Agent Usage: ddev agent [OPTIONS] COMMAND [ARGS]...","title":"agent"},{"location":"ddev/cli/#changelog","text":"Generates a markdown file containing the list of checks that changed for a given Agent release. Agent version numbers are derived inspecting tags on integrations-core so running this tool might provide unexpected results if the repo is not up to date with the Agent release process. If neither --since or --to are passed (the most common use case), the tool will generate the whole changelog since Agent version 6.3.0 (before that point we don't have enough information to build the log). Usage: ddev agent changelog [OPTIONS] Options: --since TEXT Initial Agent version --to TEXT Final Agent version -w, --write Write to the changelog file, if omitted contents will be printed to stdout -f, --force Replace an existing file","title":"changelog"},{"location":"ddev/cli/#integrations","text":"Generates a markdown file containing the list of integrations shipped in a given Agent release. Agent version numbers are derived inspecting tags on integrations-core so running this tool might provide unexpected results if the repo is not up to date with the Agent release process. If neither --since or --to are passed (the most common use case), the tool will generate the list for every Agent since version 6.3.0 (before that point we don't have enough information to build the log). Usage: ddev agent integrations [OPTIONS] Options: --since TEXT Initial Agent version --to TEXT Final Agent version -w, --write Write to file, if omitted contents will be printed to stdout -f, --force Replace an existing file","title":"integrations"},{"location":"ddev/cli/#requirements","text":"Write the requirements-agent-release.txt file at the root of the repo listing all the Agent-based integrations pinned at the version they currently have in HEAD. Usage: ddev agent requirements [OPTIONS]","title":"requirements"},{"location":"ddev/cli/#ci","text":"CI related utils. Anything here should be considered experimental. Usage: ddev ci [OPTIONS] COMMAND [ARGS]...","title":"ci"},{"location":"ddev/cli/#setup","text":"Run CI setup scripts Usage: ddev ci setup [OPTIONS] [CHECKS]... Options: --changed Only target changed checks","title":"setup"},{"location":"ddev/cli/#clean","text":"Remove build and test artifacts for the given CHECK. If CHECK is not specified, the current working directory is used. Usage: ddev clean [OPTIONS] [CHECK] Options: -c, --compiled-only Remove compiled files only (__pycache__, *.pyc, *.pyo, *.pyd, *.whl). -a, --all Disable the detection of a project's dedicated virtual env and/or editable installation. By default, these will not be considered. -f, --force If set and the command is run from the root directory, allow removing build and test artifacts (.benchmarks, .pytest_cache, .cache, *.egg-info, .tox, .coverage, build, .eggs, dist). -v, --verbose Shows removed paths.","title":"clean"},{"location":"ddev/cli/#config","text":"Manage the config file Usage: ddev config [OPTIONS] COMMAND [ARGS]...","title":"config"},{"location":"ddev/cli/#explore","text":"Open the config location in your file manager. Usage: ddev config explore [OPTIONS]","title":"explore"},{"location":"ddev/cli/#find","text":"Show the location of the config file. Usage: ddev config find [OPTIONS]","title":"find"},{"location":"ddev/cli/#restore","text":"Restore the config file to default settings. Usage: ddev config restore [OPTIONS]","title":"restore"},{"location":"ddev/cli/#set","text":"Assigns values to config file entries. If the value is omitted, you will be prompted, with the input hidden if it is sensitive. $ ddev config set github.user foo New setting: [github] user = \"foo\" You can also assign values on a per-org basis. $ ddev config set orgs.<ORG_NAME>.api_key New setting: [orgs.<ORG_NAME>] api_key = \"***********\" Usage: ddev config set [OPTIONS] KEY [VALUE]","title":"set"},{"location":"ddev/cli/#show","text":"Show the contents of the config file. Usage: ddev config show [OPTIONS] Options: -a, --all No not scrub secret fields","title":"show"},{"location":"ddev/cli/#update","text":"Update the config file with any new fields. Usage: ddev config update [OPTIONS]","title":"update"},{"location":"ddev/cli/#create","text":"Create scaffolding for a new integration. Usage: ddev create [OPTIONS] NAME Options: -t, --type [check|jmx|logs|tile] The type of integration to create -l, --location TEXT The directory where files will be written -ni, --non-interactive Disable prompting for fields -q, --quiet Show less output -n, --dry-run Only show what would be created","title":"create"},{"location":"ddev/cli/#dep","text":"Manage dependencies Usage: ddev dep [OPTIONS] COMMAND [ARGS]...","title":"dep"},{"location":"ddev/cli/#freeze","text":"Combine all dependencies for the Agent's static environment. Usage: ddev dep freeze [OPTIONS]","title":"freeze"},{"location":"ddev/cli/#pin","text":"Pin a dependency for all checks that require it. This can also resolve transient dependencies. Setting the version to none will remove the package. You can specify an unlimited number of additional checks to apply the pin for via arguments. Usage: ddev dep pin [OPTIONS] PACKAGE VERSION [CHECKS]... Options: -m, --marker TEXT Environment marker to use -r, --resolve Resolve transient dependencies -l, --lazy Do not attempt to upgrade transient dependencies when resolving -q, --quiet","title":"pin"},{"location":"ddev/cli/#resolve","text":"Resolve transient dependencies for any number of checks. If you want to do this en masse, put all . Usage: ddev dep resolve [OPTIONS] CHECKS... Options: -l, --lazy Do not attempt to upgrade transient dependencies -q, --quiet","title":"resolve"},{"location":"ddev/cli/#docs","text":"Manage documentation Usage: ddev docs [OPTIONS] COMMAND [ARGS]...","title":"docs"},{"location":"ddev/cli/#build","text":"Build documentation. Usage: ddev docs build [OPTIONS] Options: -v, --verbose Increase verbosity (can be used additively)","title":"build"},{"location":"ddev/cli/#push","text":"Push built documentation. Usage: ddev docs push [OPTIONS] [BRANCH]","title":"push"},{"location":"ddev/cli/#serve","text":"Serve and view documentation in a web browser. Usage: ddev docs serve [OPTIONS] Options: -n, --no-open Do not open the documentation in a web browser -v, --verbose Increase verbosity (can be used additively)","title":"serve"},{"location":"ddev/cli/#env","text":"Manage environments Usage: ddev env [OPTIONS] COMMAND [ARGS]...","title":"env"},{"location":"ddev/cli/#check","text":"Run an Agent check. Usage: ddev env check [OPTIONS] CHECK [ENV] Options: -r, --rate Compute rates by running the check twice with a pause between each run -t, --times INTEGER Number of times to run the check --pause INTEGER Number of milliseconds to pause between multiple check runs -d, --delay INTEGER Delay in milliseconds between running the check and grabbing what was collected -l, --log-level TEXT Set the log level (default `off`) --json Format the aggregator and check runner output as JSON -b, --breakpoint INTEGER Line number to start a PDB session (0: first line, -1: last line) --config TEXT Path to a JSON check configuration to use --jmx-list TEXT JMX metrics listing method","title":"check"},{"location":"ddev/cli/#ls","text":"List active or available environments. Usage: ddev env ls [OPTIONS] [CHECKS]...","title":"ls"},{"location":"ddev/cli/#prune","text":"Remove all configuration for environments. Usage: ddev env prune [OPTIONS] Options: -f, --force","title":"prune"},{"location":"ddev/cli/#reload","text":"Restart an Agent to detect environment changes. Usage: ddev env reload [OPTIONS] CHECK [ENV]","title":"reload"},{"location":"ddev/cli/#start","text":"Start an environment. Usage: ddev env start [OPTIONS] CHECK ENV Options: -a, --agent TEXT The agent build to use e.g. a Docker image like `datadog/agent:6.5.2`. For Docker environments you can use an integer corresponding to fields in the config (agent5, agent6, etc.) -py, --python INTEGER The version of Python to use. Defaults to 2 if no tox Python is specified. --dev / --prod Whether to use the latest version of a check or what is shipped --base Whether to use the latest version of the base check or what is shipped -e, --env-vars TEXT ENV Variable that should be passed to the Agent container. Ex: -e DD_URL=app.datadoghq.com -e DD_API_KEY=123456 -o, --org-name TEXT The org to use for data submission. -pm, --profile-memory Whether to collect metrics about memory usage","title":"start"},{"location":"ddev/cli/#stop","text":"Stop environments, use \"all\" as check argument to stop everything. Usage: ddev env stop [OPTIONS] CHECK [ENV]","title":"stop"},{"location":"ddev/cli/#test","text":"Test an environment. Usage: ddev env test [OPTIONS] [CHECKS]... Options: -a, --agent TEXT The agent build to use e.g. a Docker image like `datadog/agent:6.5.2`. For Docker environments you can use an integer corresponding to fields in the config (agent5, agent6, etc.) -py, --python INTEGER The version of Python to use. Defaults to 2 if no tox Python is specified. --dev / --prod Whether to use the latest version of a check or what is shipped --base Whether to use the latest version of the base check or what is shipped -e, --env-vars TEXT ENV Variable that should be passed to the Agent container. Ex: -e DD_URL=app.datadoghq.com -e DD_API_KEY=123456 -ne, --new-env Execute setup and tear down actions -pm, --profile-memory Whether to collect metrics about memory usage -j, --junit Generate junit reports","title":"test"},{"location":"ddev/cli/#meta","text":"Anything here should be considered experimental. This meta namespace can be used for an arbitrary number of niche or beta features without bloating the root namespace. Usage: ddev meta [OPTIONS] COMMAND [ARGS]...","title":"meta"},{"location":"ddev/cli/#catalog","text":"Create a catalog with information about integrations Usage: ddev meta catalog [OPTIONS] CHECKS... Options: -f, --file TEXT Output to file (it will be overwritten), you can pass \"tmp\" to generate a temporary file -m, --markdown Output to markdown instead of CSV","title":"catalog"},{"location":"ddev/cli/#changes","text":"Show changes since a specific date. Usage: ddev meta changes [OPTIONS] SINCE Options: -o, --out Output to file --eager Skip validation of commit subjects","title":"changes"},{"location":"ddev/cli/#dash","text":"Dashboard utilities Usage: ddev meta dash [OPTIONS] COMMAND [ARGS]...","title":"dash"},{"location":"ddev/cli/#export","text":"Export a Dashboard as JSON Usage: ddev meta dash export [OPTIONS] URL [INTEGRATION]","title":"export"},{"location":"ddev/cli/#jmx","text":"JMX utilities Usage: ddev meta jmx [OPTIONS] COMMAND [ARGS]...","title":"jmx"},{"location":"ddev/cli/#query-endpoint","text":"Query endpoint for JMX info Usage: ddev meta jmx query-endpoint [OPTIONS] HOST PORT [DOMAIN]","title":"query-endpoint"},{"location":"ddev/cli/#prom","text":"Prometheus utilities Usage: ddev meta prom [OPTIONS] COMMAND [ARGS]...","title":"prom"},{"location":"ddev/cli/#info","text":"Show metric info from a Prometheus endpoint. Example: $ ddev meta prom info :8080/_status/vars Usage: ddev meta prom info [OPTIONS] ENDPOINT","title":"info"},{"location":"ddev/cli/#parse","text":"Interactively parse metric info from a Prometheus endpoint and write it to metadata.csv. Usage: ddev meta prom parse [OPTIONS] ENDPOINT CHECK Options: -x, --here Output to the current location","title":"parse"},{"location":"ddev/cli/#scripts","text":"Miscellaneous scripts that may be useful Usage: ddev meta scripts [OPTIONS] COMMAND [ARGS]...","title":"scripts"},{"location":"ddev/cli/#email2ghuser","text":"Given an email, attempt to find a Github username associated with the email. $ ddev meta scripts email2ghuser example@datadoghq.com Usage: ddev meta scripts email2ghuser [OPTIONS] EMAIL","title":"email2ghuser"},{"location":"ddev/cli/#metrics2md","text":"Convert a check's metadata.csv file to a Markdown table, which will be copied to your clipboard. By default it will be compact and only contain the most useful fields. If you wish to use arbitrary metric data, you may set the check to cb to target the current contents of your clipboard. Usage: ddev meta scripts metrics2md [OPTIONS] CHECK [FIELDS]...","title":"metrics2md"},{"location":"ddev/cli/#remove-labels","text":"Remove all labels from an issue or pull request. This is useful when there are too many labels and its state cannot be modified (known GitHub issue). $ ddev meta scripts remove-labels 5626 Usage: ddev meta scripts remove-labels [OPTIONS] ISSUE_NUMBER","title":"remove-labels"},{"location":"ddev/cli/#upgrade-python","text":"Upgrade the Python version of all test environments. $ ddev meta scripts upgrade-python 3.8 Usage: ddev meta scripts upgrade-python [OPTIONS] NEW_VERSION [OLD_VERSION]","title":"upgrade-python"},{"location":"ddev/cli/#snmp","text":"SNMP utilities Usage: ddev meta snmp [OPTIONS] COMMAND [ARGS]...","title":"snmp"},{"location":"ddev/cli/#translate-profile","text":"Do OID translation in a SNMP profile. This isn't a plain replacement, as it doesn't preserve comments and indent, but it should automate most of the work. You'll need to install pysnmp and pysnmp-mibs manually beforehand. Usage: ddev meta snmp translate-profile [OPTIONS] PROFILE_PATH","title":"translate-profile"},{"location":"ddev/cli/#release","text":"Manage the release of checks Usage: ddev release [OPTIONS] COMMAND [ARGS]...","title":"release"},{"location":"ddev/cli/#build_1","text":"Build a wheel for a check as it is on the repo HEAD Usage: ddev release build [OPTIONS] CHECK Options: -s, --sdist","title":"build"},{"location":"ddev/cli/#changelog_1","text":"Perform the operations needed to update the changelog. This method is supposed to be used by other tasks and not directly. Usage: ddev release changelog [OPTIONS] CHECK VERSION [OLD_VERSION] Options: --initial -q, --quiet -n, --dry-run -o, --output-file TEXT [default: CHANGELOG.md] -tp, --tag-prefix TEXT [default: v]","title":"changelog"},{"location":"ddev/cli/#make","text":"Perform a set of operations needed to release checks: update the version in __about__.py update the changelog update the requirements-agent-release.txt file update in-toto metadata commit the above changes You can release everything at once by setting the check to all . If you run into issues signing: Ensure you did gpg --import <YOUR_KEY_ID>.gpg.pub Usage: ddev release make [OPTIONS] CHECKS... Options: --version TEXT --new Ensure versions are at 1.0.0 --skip-sign Skip the signing of release metadata --sign-only Only sign release metadata --exclude TEXT Comma-separated list of checks to skip","title":"make"},{"location":"ddev/cli/#show_1","text":"To avoid GitHub's public API rate limits, you need to set github.user / github.token in your config file or use the DD_GITHUB_USER / DD_GITHUB_TOKEN environment variables. Usage: ddev release show [OPTIONS] COMMAND [ARGS]...","title":"show"},{"location":"ddev/cli/#changes_1","text":"Show all the pending PRs for a given check. Usage: ddev release show changes [OPTIONS] CHECK Options: -n, --dry-run","title":"changes"},{"location":"ddev/cli/#ready","text":"Show all the checks that can be released. Usage: ddev release show ready [OPTIONS] Options: -q, --quiet","title":"ready"},{"location":"ddev/cli/#tag","text":"Tag the HEAD of the git repo with the current release number for a specific check. The tag is pushed to origin by default. You can tag everything at once by setting the check to all . Notice: specifying a different version than the one in __about__.py is a maintenance task that should be run under very specific circumstances (e.g. re-align an old release performed on the wrong commit). Usage: ddev release tag [OPTIONS] CHECK [VERSION] Options: --push / --no-push -n, --dry-run","title":"tag"},{"location":"ddev/cli/#testable","text":"Create a Trello card for changes since a previous release (referenced by BASE_REF ) that need to be tested for the next release (referenced by TARGET_REF ). BASE_REF and TARGET_REF can be any valid git references. It practice, you should use either: A tag: 7.16.1 , 7.17.0-rc.4 , ... A release branch: 6.16.x , 7.17.x , ... The master branch. NOTE: using a minor version shorthand (e.g. 7.16 ) is not supported, as it is ambiguous. Example: assuming we are working on the release of 7.17.0, we can... Create cards for changes between a previous Agent release and master (useful when preparing an initial RC): $ ddev release testable 7.16.1 origin/master Create cards for changes between a previous RC and master (useful when preparing a new RC, and a separate release branch was not created yet): $ ddev release testable 7.17.0-rc.2 origin/master Create cards for changes between a previous RC and a release branch (useful to only review changes in a release branch that has diverged from master ): $ ddev release testable 7.17.0-rc.4 7.17.x Create cards for changes between two arbitrary tags, e.g. between RCs: $ ddev release testable 7.17.0-rc.4 7.17.0-rc.5 TIP: run with ddev -x release testable to force the use of the current directory. To avoid GitHub's public API rate limits, you need to set github.user / github.token in your config file or use the DD_GITHUB_USER / DD_GITHUB_TOKEN environment variables. To use Trello: 1. Go to https://trello.com/app-key and copy your API key. 2. Run ddev config set trello.key and paste your API key. 3. Go to https://trello.com/1/authorize?key=key&name=name&scope=read,write&expiration=never&response_type=token , where key is your API key and name is the name to give your token, e.g. ReleaseTestingYourName. Authorize access and copy your token. 4. Run ddev config set trello.token and paste your token. Usage: ddev release testable [OPTIONS] BASE_REF TARGET_REF Options: --milestone TEXT The PR milestone to filter by -n, --dry-run Only show the changes","title":"testable"},{"location":"ddev/cli/#upload","text":"Release a specific check to PyPI as it is on the repo HEAD. Usage: ddev release upload [OPTIONS] CHECK Options: -s, --sdist -n, --dry-run","title":"upload"},{"location":"ddev/cli/#run","text":"Run commands in the proper repo. Usage: ddev run [OPTIONS] [ARGS]...","title":"run"},{"location":"ddev/cli/#test_1","text":"Run tests for Agent-based checks. If no checks are specified, this will only test checks that were changed compared to the master branch. You can also select specific comma-separated environments to test like so: $ ddev test mysql:mysql57,maria10130 Usage: ddev test [OPTIONS] [CHECKS]... Options: -fs, --format-style Run only the code style formatter -s, --style Run only style checks -b, --bench Run only benchmarks --latest-metrics Only verify support of new metrics --e2e Run only end-to-end tests -c, --cov Measure code coverage -cm, --cov-missing Show line numbers of statements that were not executed -j, --junit Generate junit reports -m, --marker TEXT Only run tests matching given marker expression -k, --filter TEXT Only run tests matching given substring expression --pdb Drop to PDB on first failure, then end test session -d, --debug Set the log level to debug -v, --verbose Increase verbosity (can be used additively) -l, --list List available test environments --passenv TEXT Additional environment variables to pass down --changed Only test changed checks --cov-keep Keep coverage reports --skip-env Skip environment creation and assume it is already running -pa, --pytest-args TEXT Additional arguments to pytest","title":"test"},{"location":"ddev/cli/#validate","text":"Verify certain aspects of the repo Usage: ddev validate [OPTIONS] COMMAND [ARGS]...","title":"validate"},{"location":"ddev/cli/#agent-reqs","text":"Verify that the checks versions are in sync with the requirements-agent-release.txt file Usage: ddev validate agent-reqs [OPTIONS]","title":"agent-reqs"},{"location":"ddev/cli/#ci_1","text":"Validate CI infrastructure configuration. Usage: ddev validate ci [OPTIONS] Options: --fix Attempt to fix errors","title":"ci"},{"location":"ddev/cli/#config_1","text":"Validate default configuration files. Usage: ddev validate config [OPTIONS] [CHECK] Options: --sync Generate example configuration files based on specifications","title":"config"},{"location":"ddev/cli/#dashboards","text":"Validate all Dashboard definition files. Usage: ddev validate dashboards [OPTIONS]","title":"dashboards"},{"location":"ddev/cli/#dep_1","text":"This command will: Verify the uniqueness of dependency versions across all checks. Verify all the dependencies are pinned. Verify the embedded Python environment defined in the base check and requirements listed in every integration are compatible. Usage: ddev validate dep [OPTIONS]","title":"dep"},{"location":"ddev/cli/#imports","text":"Validate proper imports in checks. Usage: ddev validate imports [OPTIONS] [CHECKS]... Options: --autofix Apply suggested fix","title":"imports"},{"location":"ddev/cli/#legacy-signature","text":"Validate that no integration uses the legacy signature. Usage: ddev validate legacy-signature [OPTIONS] [CHECK]","title":"legacy-signature"},{"location":"ddev/cli/#manifest","text":"Validate manifest.json files. Usage: ddev validate manifest [OPTIONS] Options: --fix Attempt to fix errors -i, --include-extras Include optional fields","title":"manifest"},{"location":"ddev/cli/#metadata","text":"Validates metadata.csv files If check is specified, only the check will be validated, otherwise all metadata files in the repo will be. Usage: ddev validate metadata [OPTIONS] [CHECK] Options: --check-duplicates Output warnings if there are duplicate short names and descriptions","title":"metadata"},{"location":"ddev/cli/#service-checks","text":"Validate all service_checks.json files. Usage: ddev validate service-checks [OPTIONS] Options: --sync Generate example configuration files based on specifications","title":"service-checks"},{"location":"ddev/configuration/","text":"Configuration All configuration can be managed entirely by the ddev config command group. To locate the TOML config file, run: ddev config find Repository \u00b6 All CLI commands are aware of the current repository context, defined by the option repo . This option should be a reference to a key in repos which is set to the path of a supported repository. For example, this configuration: repo = \"core\" [repos] core = \"/path/to/integrations-core\" extras = \"/path/to/integrations-extras\" agent = \"/path/to/datadog-agent\" would make it so running e.g. ddev test nginx will look for an integration named nginx in /path/to/integrations-core no matter what directory you are in. If the selected path does not exist, then the current directory will be used. By default, repo is set to core . Agent \u00b6 Organization \u00b6 GitHub \u00b6 To avoid GitHub's public API rate limits, you need to set github.user / github.token in your config file or use the DD_GITHUB_USER / DD_GITHUB_TOKEN environment variables. Run ddev config show to see if your GitHub user and token is set. If not: Run ddev config set github.user <YOUR_GITHUB_USERNAME> Create a personal access token with public_repo permissions Run ddev config set github.token then paste the token Enable single sign-on for the token Trello \u00b6 To participate as an Agent release manager , you need to set trello.key / trello.token in your config file. Run ddev config show to see if your Trello key and token is set. If not: Go to https://trello.com/app-key and copy your API key Run ddev config set trello.key then paste your API key Go to https://trello.com/1/authorize?key=<KEY>&name=<NAME>&scope=read,write&expiration=never&response_type=token , where <KEY> is your API key and <NAME> is the name to give your token, e.g. ReleaseTestingYourName . Authorize access and copy your token. Run ddev config set trello.token and paste your token","title":"Configuration"},{"location":"ddev/configuration/#repository","text":"All CLI commands are aware of the current repository context, defined by the option repo . This option should be a reference to a key in repos which is set to the path of a supported repository. For example, this configuration: repo = \"core\" [repos] core = \"/path/to/integrations-core\" extras = \"/path/to/integrations-extras\" agent = \"/path/to/datadog-agent\" would make it so running e.g. ddev test nginx will look for an integration named nginx in /path/to/integrations-core no matter what directory you are in. If the selected path does not exist, then the current directory will be used. By default, repo is set to core .","title":"Repository"},{"location":"ddev/configuration/#agent","text":"","title":"Agent"},{"location":"ddev/configuration/#organization","text":"","title":"Organization"},{"location":"ddev/configuration/#github","text":"To avoid GitHub's public API rate limits, you need to set github.user / github.token in your config file or use the DD_GITHUB_USER / DD_GITHUB_TOKEN environment variables. Run ddev config show to see if your GitHub user and token is set. If not: Run ddev config set github.user <YOUR_GITHUB_USERNAME> Create a personal access token with public_repo permissions Run ddev config set github.token then paste the token Enable single sign-on for the token","title":"GitHub"},{"location":"ddev/configuration/#trello","text":"To participate as an Agent release manager , you need to set trello.key / trello.token in your config file. Run ddev config show to see if your Trello key and token is set. If not: Go to https://trello.com/app-key and copy your API key Run ddev config set trello.key then paste your API key Go to https://trello.com/1/authorize?key=<KEY>&name=<NAME>&scope=read,write&expiration=never&response_type=token , where <KEY> is your API key and <NAME> is the name to give your token, e.g. ReleaseTestingYourName . Authorize access and copy your token. Run ddev config set trello.token and paste your token","title":"Trello"},{"location":"ddev/layers/","text":"What's in the box? The Dev package, often referred to as its CLI entrypoint ddev , is fundamentally split into 2 parts. Test framework \u00b6 The test framework provides everything necessary to test integrations, such as: Dependencies like pytest , mock , requests , etc. Utilities for consistently handling complex logic or common operations An orchestrator for arbitrary E2E environments CLI \u00b6 The CLI provides the interface through which tests are invoked, E2E environments are managed, and general repository maintenance (such as dependency management) occurs. Separation \u00b6 As the dependencies of the test framework are a subset of what is required for the CLI, the CLI tooling may import from the test framework, but not vice versa. The diagram below shows the import hierarchy between each component. Clicking a node will open that component's location in the source code. graph BT A([Plugins]) click A \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev/plugin\" \"Test framework plugins location\" B([Test framework]) click B \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev\" \"Test framework location\" C([CLI]) click C \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev/tooling\" \"CLI tooling location\" A-->B C-->B var config = { securityLevel: \"loose\", startOnLoad: false, theme: \"default\", flowchart: { htmlLabels: false }, sequence: { useMaxWidth: false }, class: { textHeight: 16, dividerMargin: 16 } }; mermaid.initialize(config);","title":"What's in the box?"},{"location":"ddev/layers/#test-framework","text":"The test framework provides everything necessary to test integrations, such as: Dependencies like pytest , mock , requests , etc. Utilities for consistently handling complex logic or common operations An orchestrator for arbitrary E2E environments","title":"Test framework"},{"location":"ddev/layers/#cli","text":"The CLI provides the interface through which tests are invoked, E2E environments are managed, and general repository maintenance (such as dependency management) occurs.","title":"CLI"},{"location":"ddev/layers/#separation","text":"As the dependencies of the test framework are a subset of what is required for the CLI, the CLI tooling may import from the test framework, but not vice versa. The diagram below shows the import hierarchy between each component. Clicking a node will open that component's location in the source code. graph BT A([Plugins]) click A \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev/plugin\" \"Test framework plugins location\" B([Test framework]) click B \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev\" \"Test framework location\" C([CLI]) click C \"https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/datadog_checks/dev/tooling\" \"CLI tooling location\" A-->B C-->B var config = { securityLevel: \"loose\", startOnLoad: false, theme: \"default\", flowchart: { htmlLabels: false }, sequence: { useMaxWidth: false }, class: { textHeight: 16, dividerMargin: 16 } }; mermaid.initialize(config);","title":"Separation"},{"location":"ddev/plugins/","text":"Plugins tox \u00b6 Our tox plugin dynamically adds environments based on the presence of options defined in the [testenv] section of each integration's tox.ini file. Style \u00b6 Setting dd_check_style to true will enable 2 environments for enforcing our style conventions : style - This will check the formatting and will error if any issues are found. You may use the -s/--style flag of ddev test to execute only this environment. format_style - This will format the code for you, resolving the most common issues caught by style environment. You can run the formatter by using the -fs/--format-style flag of ddev test . pytest \u00b6 Our pytest plugin makes a few fixtures available globally for use during tests. Also, it's responsible for managing the control flow of E2E environments. Fixtures \u00b6 Agent stubs \u00b6 The stubs provided by each fixture will automatically have their state reset before each test. aggregator datadog_agent Check execution \u00b6 Most tests will execute checks via the run method of the AgentCheck interface (if the check is stateful ). A consequence of this is that, unlike the check method, exceptions are not propagated to the caller meaning not only can an exception not be asserted, but also errors are silently ignored. The dd_run_check fixture takes a check instance and executes it while also propagating any exceptions like normal. def test_metrics ( aggregator , dd_run_check ): check = AwesomeCheck ( 'awesome' , {}, [{ 'port' : 8080 }]) dd_run_check ( check ) ... You can use the extract_message option to condense any exception message to just the original message rather than the full traceback. def test_config ( dd_run_check ): check = AwesomeCheck ( 'awesome' , {}, [{ 'port' : 'foo' }]) with pytest . raises ( Exception , match = '^Option `port` must be an integer$' ): dd_run_check ( check , extract_message = True ) E2E \u00b6 Agent check runner \u00b6 The dd_agent_check fixture will run the integration with a given configuration on a live Agent and return a populated aggregator . It accepts a single dict configuration representing either: a single instance a full configuration with top level keys instances , init_config , etc. Internally, this is a wrapper around ddev env check and you can pass through any supported options or flags. This fixture can only be used from tests marked as e2e . For example: @pytest . mark . e2e def test_e2e_metrics ( dd_agent_check , instance ): aggregator = dd_agent_check ( instance , rate = True ) ... State \u00b6 Occasionally, you will need to persist some data only known at the time of environment creation (like a generated token) through the test and environment tear down phases. To do so, use the following fixtures: dd_save_state - When executing the necessary steps to spin up an environment you may use this to save any object that can be serialized to JSON. For example: dd_save_state ( 'my_data' , { 'foo' : 'bar' }) dd_get_state - This may be used to retrieve the data: my_data = dd_get_state ( 'my_data' , default = {}) Environment manager \u00b6 The fixture dd_environment_runner manages communication between environments and the ddev env command group. You will never use it directly as it runs automatically. It acts upon a fixture named dd_environment that every integration's test suite will define if E2E testing on a live Agent is desired. This fixture is responsible for starting and stopping environments and must adhere to the following requirements: It yield s a single dict representing the default configuration the Agent will use. It must be either: a single instance a full configuration with top level keys instances , init_config , etc. Additionally, you can pass a second dict containing metadata . The setup logic must occur before the yield and the tear down logic must occur after it. Also, both steps must only execute based on the value of environment variables. Setup - only if DDEV_E2E_UP is not set to false Tear down - only if DDEV_E2E_DOWN is not set to false Note The provided Docker and Terraform environment runner utilities will do this automatically for you. Metadata \u00b6 env_type - This is the type of interface that will be used to interact with the Agent. Currently, we support docker (default) and local . env_vars - A dict of environment variables and their values that will be present when starting the Agent. docker_volumes - A list of str representing Docker volume mounts if env_type is docker e.g. /local/path:/agent/container/path:ro . logs_config - A list of configs that will be used by the Logs Agent. You will never need to use this directly, but rather via higher level abstractions .","title":"Plugins"},{"location":"ddev/plugins/#tox","text":"Our tox plugin dynamically adds environments based on the presence of options defined in the [testenv] section of each integration's tox.ini file.","title":"tox"},{"location":"ddev/plugins/#style","text":"Setting dd_check_style to true will enable 2 environments for enforcing our style conventions : style - This will check the formatting and will error if any issues are found. You may use the -s/--style flag of ddev test to execute only this environment. format_style - This will format the code for you, resolving the most common issues caught by style environment. You can run the formatter by using the -fs/--format-style flag of ddev test .","title":"Style"},{"location":"ddev/plugins/#pytest","text":"Our pytest plugin makes a few fixtures available globally for use during tests. Also, it's responsible for managing the control flow of E2E environments.","title":"pytest"},{"location":"ddev/plugins/#fixtures","text":"","title":"Fixtures"},{"location":"ddev/plugins/#agent-stubs","text":"The stubs provided by each fixture will automatically have their state reset before each test. aggregator datadog_agent","title":"Agent stubs"},{"location":"ddev/plugins/#check-execution","text":"Most tests will execute checks via the run method of the AgentCheck interface (if the check is stateful ). A consequence of this is that, unlike the check method, exceptions are not propagated to the caller meaning not only can an exception not be asserted, but also errors are silently ignored. The dd_run_check fixture takes a check instance and executes it while also propagating any exceptions like normal. def test_metrics ( aggregator , dd_run_check ): check = AwesomeCheck ( 'awesome' , {}, [{ 'port' : 8080 }]) dd_run_check ( check ) ... You can use the extract_message option to condense any exception message to just the original message rather than the full traceback. def test_config ( dd_run_check ): check = AwesomeCheck ( 'awesome' , {}, [{ 'port' : 'foo' }]) with pytest . raises ( Exception , match = '^Option `port` must be an integer$' ): dd_run_check ( check , extract_message = True )","title":"Check execution"},{"location":"ddev/plugins/#e2e","text":"","title":"E2E"},{"location":"ddev/plugins/#agent-check-runner","text":"The dd_agent_check fixture will run the integration with a given configuration on a live Agent and return a populated aggregator . It accepts a single dict configuration representing either: a single instance a full configuration with top level keys instances , init_config , etc. Internally, this is a wrapper around ddev env check and you can pass through any supported options or flags. This fixture can only be used from tests marked as e2e . For example: @pytest . mark . e2e def test_e2e_metrics ( dd_agent_check , instance ): aggregator = dd_agent_check ( instance , rate = True ) ...","title":"Agent check runner"},{"location":"ddev/plugins/#state","text":"Occasionally, you will need to persist some data only known at the time of environment creation (like a generated token) through the test and environment tear down phases. To do so, use the following fixtures: dd_save_state - When executing the necessary steps to spin up an environment you may use this to save any object that can be serialized to JSON. For example: dd_save_state ( 'my_data' , { 'foo' : 'bar' }) dd_get_state - This may be used to retrieve the data: my_data = dd_get_state ( 'my_data' , default = {})","title":"State"},{"location":"ddev/plugins/#environment-manager","text":"The fixture dd_environment_runner manages communication between environments and the ddev env command group. You will never use it directly as it runs automatically. It acts upon a fixture named dd_environment that every integration's test suite will define if E2E testing on a live Agent is desired. This fixture is responsible for starting and stopping environments and must adhere to the following requirements: It yield s a single dict representing the default configuration the Agent will use. It must be either: a single instance a full configuration with top level keys instances , init_config , etc. Additionally, you can pass a second dict containing metadata . The setup logic must occur before the yield and the tear down logic must occur after it. Also, both steps must only execute based on the value of environment variables. Setup - only if DDEV_E2E_UP is not set to false Tear down - only if DDEV_E2E_DOWN is not set to false Note The provided Docker and Terraform environment runner utilities will do this automatically for you.","title":"Environment manager"},{"location":"ddev/plugins/#metadata","text":"env_type - This is the type of interface that will be used to interact with the Agent. Currently, we support docker (default) and local . env_vars - A dict of environment variables and their values that will be present when starting the Agent. docker_volumes - A list of str representing Docker volume mounts if env_type is docker e.g. /local/path:/agent/container/path:ro . logs_config - A list of configs that will be used by the Logs Agent. You will never need to use this directly, but rather via higher level abstractions .","title":"Metadata"},{"location":"ddev/test/","text":"Test framework Environments \u00b6 Most integrations monitor services like databases or web servers, rather than system properties like CPU usage. For such cases, you'll want to spin up an environment and gracefully tear it down when tests finish. We define all environment actions in a fixture called dd_environment that looks semantically like this: @pytest . fixture ( scope = 'session' ) def dd_environment (): try : set_up_env () yield some_default_config finally : tear_down_env () This is not only used for regular tests, but is also the basis of our E2E testing . The start command executes everything before the yield and the stop command executes everything after it. We provide a few utilities for common environment types. Docker \u00b6 The docker_run utility makes it easy to create services using docker-compose . from datadog_checks.dev import docker_run @pytest . fixture ( scope = 'session' ) def dd_environment (): with docker_run ( os . path . join ( HERE , 'docker' , 'compose.yaml' )): yield ... Read the reference for more information. Terraform \u00b6 The terraform_run utility makes it easy to create services from a directory of Terraform files. from datadog_checks.dev.terraform import terraform_run @pytest . fixture ( scope = 'session' ) def dd_environment (): with terraform_run ( os . path . join ( HERE , 'terraform' )): yield ... Currently, we only use this for services that would be too complex to setup with Docker (like OpenStack) or things that cannot be provided by Docker (like vSphere). We provide some ready-to-use cloud templates that are available for referencing by default. Terraform E2E tests are not run in our public CI as that would needlessly slow down builds. Read the reference for more information. Mocker \u00b6 The mocker fixture is provided by the pytest-mock plugin. This fixture automatically restores anything that was mocked at the end of each test and is more ergonomic to use than stacking decorators or nesting context managers. Here's an example from their docs: def test_foo ( mocker ): # all valid calls mocker . patch ( 'os.remove' ) mocker . patch . object ( os , 'listdir' , autospec = True ) mocked_isfile = mocker . patch ( 'os.path.isfile' ) It also has many other nice features, like using pytest introspection when comparing calls. Benchmarks \u00b6 The benchmark fixture is provided by the pytest-benchmark plugin. It enables the profiling of functions with the low-overhead cProfile module. It is quite useful for seeing the approximate time a given check takes to run, as well as gaining insight into any potential performance bottlenecks. You would use it like this: def test_large_payload ( benchmark , dd_run_check ): check = AwesomeCheck ( 'awesome' , {}, [ instance ]) # Run once to get any initialization out of the way. dd_run_check ( check ) benchmark ( dd_run_check , check ) To add benchmarks, define environments in tox.ini with bench somewhere in their names: [tox] ... envlist = ... bench ... [testenv:bench] By default, the test command skips all benchmark environments and tests. To run only benchmark environments and tests use the --bench / -b flag. The results are sorted by tottime , which is the total time spent in the given function (and excluding time made in calls to sub-functions). Logs \u00b6 We provide an easy way to utilize log collection with E2E Docker environments . Pass mount_logs=True to docker_run . This will use the logs example in the integration's config spec . For example, the following defines 2 example log files: - template : logs example : - type : file path : /var/log/apache2/access.log source : apache service : apache - type : file path : /var/log/apache2/error.log source : apache service : apache Alternatives If mount_logs is a sequence of int , only the selected indices (starting at 1) will be used. So, using the Apache example above, to only monitor the error log you would set it to [2] . In lieu of a config spec, for whatever reason, you may set mount_logs to a dict containing the standard logs key. All requested log files are available to reference as environment variables for any Docker calls as DD_LOG_<LOG_CONFIG_INDEX> where the indices start at 1. volumes : - ${DD_LOG_1}:/usr/local/apache2/logs/access_log - ${DD_LOG_2}:/usr/local/apache2/logs/error_log When starting the environment, pass -e DD_LOGS_ENABLED=true to activate the Logs Agent. Reference \u00b6 datadog_checks.dev.docker \u00b6 compose_file_active ( compose_file ) \u00b6 Returns a bool indicating whether or not a compose file has any active services. Source code in 46 47 48 49 50 51 52 53 54 55 56 57 def compose_file_active ( compose_file ): \"\"\" Returns a `bool` indicating whether or not a compose file has any active services. \"\"\" command = [ 'docker-compose' , '-f' , compose_file , 'ps' ] lines = run_command ( command , capture = 'out' , check = True ) . stdout . splitlines () for i , line in enumerate ( lines , 1 ): if set ( line . strip ()) == { '-' }: return len ( lines [ i :]) >= 1 return False docker_run ( compose_file = None , build = False , service_name = None , up = None , down = None , on_error = None , sleep = None , endpoints = None , log_patterns = None , mount_logs = False , conditions = None , env_vars = None , wrappers = None ) \u00b6 A convenient context manager for safely setting up and tearing down Docker environments. compose_file ( str ) - A path to a Docker compose file. A custom tear down is not required when using this. build ( bool ) - Whether or not to build images for when compose_file is provided service_name ( str ) - Optional name for when compose_file is provided up ( callable ) - A custom setup callable down ( callable ) - A custom tear down callable. This is required when using a custom setup. on_error ( callable ) - A callable called in case of an unhandled exception sleep ( float ) - Number of seconds to wait before yielding. This occurs after all conditions are successful. endpoints ( List[str]) ) - Endpoints to verify access for before yielding. Shorthand for adding CheckEndpoints(endpoints) to the conditions argument. log_patterns ( List[str|re.Pattern]) ) - Regular expression patterns to find in Docker logs before yielding. This is only available when compose_file is provided. Shorthand for adding CheckDockerLogs(compose_file, log_patterns) to the conditions argument. mount_logs ( bool ) - Whether or not to mount log files in Agent containers based on example logs configuration conditions ( callable ) - A list of callable objects that will be executed before yielding to check for errors env_vars ( dict ) - A dictionary to update os.environ with during execution wrappers ( List[callable] ) - A list of context managers to use during execution Source code in 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 @contextmanager def docker_run ( compose_file = None , build = False , service_name = None , up = None , down = None , on_error = None , sleep = None , endpoints = None , log_patterns = None , mount_logs = False , conditions = None , env_vars = None , wrappers = None , ): \"\"\" A convenient context manager for safely setting up and tearing down Docker environments. - **compose_file** (_str_) - A path to a Docker compose file. A custom tear down is not required when using this. - **build** (_bool_) - Whether or not to build images for when `compose_file` is provided - **service_name** (_str_) - Optional name for when ``compose_file`` is provided - **up** (_callable_) - A custom setup callable - **down** (_callable_) - A custom tear down callable. This is required when using a custom setup. - **on_error** (_callable_) - A callable called in case of an unhandled exception - **sleep** (_float_) - Number of seconds to wait before yielding. This occurs after all conditions are successful. - **endpoints** (_List[str])_) - Endpoints to verify access for before yielding. Shorthand for adding `CheckEndpoints(endpoints)` to the `conditions` argument. - **log_patterns** (_List[str|re.Pattern])_) - Regular expression patterns to find in Docker logs before yielding. This is only available when `compose_file` is provided. Shorthand for adding `CheckDockerLogs(compose_file, log_patterns)` to the `conditions` argument. - **mount_logs** (_bool_) - Whether or not to mount log files in Agent containers based on example logs configuration - **conditions** (_callable_) - A list of callable objects that will be executed before yielding to check for errors - **env_vars** (_dict_) - A dictionary to update `os.environ` with during execution - **wrappers** (_List[callable]_) - A list of context managers to use during execution \"\"\" if compose_file and up : raise TypeError ( 'You must select either a compose file or a custom setup callable, not both.' ) if compose_file is not None : if not isinstance ( compose_file , string_types ): raise TypeError ( 'The path to the compose file is not a string: {} ' . format ( repr ( compose_file ))) set_up = ComposeFileUp ( compose_file , build = build , service_name = service_name ) if down is not None : tear_down = down else : tear_down = ComposeFileDown ( compose_file ) if on_error is None : on_error = ComposeFileLogs ( compose_file ) else : set_up = up tear_down = down docker_conditions = [] if log_patterns is not None : if compose_file is None : raise ValueError ( 'The `log_patterns` convenience is unavailable when using ' 'a custom setup. Please use a custom condition instead.' ) docker_conditions . append ( CheckDockerLogs ( compose_file , log_patterns )) if conditions is not None : docker_conditions . extend ( conditions ) wrappers = list ( wrappers ) if wrappers is not None else [] if mount_logs : if isinstance ( mount_logs , dict ): wrappers . append ( shared_logs ( mount_logs [ 'logs' ])) # Easy mode, read example config else : # An extra level deep because of the context manager check_root = find_check_root ( depth = 2 ) example_log_configs = _read_example_logs_config ( check_root ) if mount_logs is True : wrappers . append ( shared_logs ( example_log_configs )) elif isinstance ( mount_logs , ( list , set )): wrappers . append ( shared_logs ( example_log_configs , mount_whitelist = mount_logs )) else : raise TypeError ( 'mount_logs: expected True, a list or a set, but got {} ' . format ( type ( mount_logs ) . __name__ ) ) with environment_run ( up = set_up , down = tear_down , on_error = on_error , sleep = sleep , endpoints = endpoints , conditions = docker_conditions , env_vars = env_vars , wrappers = wrappers , ) as result : yield result get_container_ip ( container_id_or_name ) \u00b6 Get a Docker container's IP address from its ID or name. Source code in 31 32 33 34 35 36 37 38 39 40 41 42 43 def get_container_ip ( container_id_or_name ): \"\"\" Get a Docker container's IP address from its ID or name. \"\"\" command = [ 'docker' , 'inspect' , '-f' , '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' , container_id_or_name , ] return run_command ( command , capture = 'out' , check = True ) . stdout . strip () get_docker_hostname () \u00b6 Determine the hostname Docker uses based on the environment, defaulting to localhost . Source code in 24 25 26 27 28 def get_docker_hostname (): \"\"\" Determine the hostname Docker uses based on the environment, defaulting to `localhost`. \"\"\" return urlparse ( os . getenv ( 'DOCKER_HOST' , '' )) . hostname or 'localhost' datadog_checks.dev.terraform \u00b6 terraform_run ( directory , sleep = None , endpoints = None , conditions = None , env_vars = None , wrappers = None ) \u00b6 A convenient context manager for safely setting up and tearing down Terraform environments. directory ( str ) - A path containing Terraform files sleep ( float ) - Number of seconds to wait before yielding. This occurs after all conditions are successful. endpoints ( List[str]) ) - Endpoints to verify access for before yielding. Shorthand for adding CheckEndpoints(endpoints) to the conditions argument. conditions ( callable ) - A list of callable objects that will be executed before yielding to check for errors env_vars ( dict ) - A dictionary to update os.environ with during execution wrappers ( List[callable] ) - A list of context managers to use during execution Source code in 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @contextmanager def terraform_run ( directory , sleep = None , endpoints = None , conditions = None , env_vars = None , wrappers = None ): \"\"\" A convenient context manager for safely setting up and tearing down Terraform environments. - **directory** (_str_) - A path containing Terraform files - **sleep** (_float_) - Number of seconds to wait before yielding. This occurs after all conditions are successful. - **endpoints** (_List[str])_) - Endpoints to verify access for before yielding. Shorthand for adding `CheckEndpoints(endpoints)` to the `conditions` argument. - **conditions** (_callable_) - A list of callable objects that will be executed before yielding to check for errors - **env_vars** (_dict_) - A dictionary to update `os.environ` with during execution - **wrappers** (_List[callable]_) - A list of context managers to use during execution \"\"\" if not which ( 'terraform' ): pytest . skip ( 'Terraform not available' ) set_up = TerraformUp ( directory ) tear_down = TerraformDown ( directory ) with environment_run ( up = set_up , down = tear_down , sleep = sleep , endpoints = endpoints , conditions = conditions , env_vars = env_vars , wrappers = wrappers , ) as result : yield result","title":"Test framework"},{"location":"ddev/test/#environments","text":"Most integrations monitor services like databases or web servers, rather than system properties like CPU usage. For such cases, you'll want to spin up an environment and gracefully tear it down when tests finish. We define all environment actions in a fixture called dd_environment that looks semantically like this: @pytest . fixture ( scope = 'session' ) def dd_environment (): try : set_up_env () yield some_default_config finally : tear_down_env () This is not only used for regular tests, but is also the basis of our E2E testing . The start command executes everything before the yield and the stop command executes everything after it. We provide a few utilities for common environment types.","title":"Environments"},{"location":"ddev/test/#docker","text":"The docker_run utility makes it easy to create services using docker-compose . from datadog_checks.dev import docker_run @pytest . fixture ( scope = 'session' ) def dd_environment (): with docker_run ( os . path . join ( HERE , 'docker' , 'compose.yaml' )): yield ... Read the reference for more information.","title":"Docker"},{"location":"ddev/test/#terraform","text":"The terraform_run utility makes it easy to create services from a directory of Terraform files. from datadog_checks.dev.terraform import terraform_run @pytest . fixture ( scope = 'session' ) def dd_environment (): with terraform_run ( os . path . join ( HERE , 'terraform' )): yield ... Currently, we only use this for services that would be too complex to setup with Docker (like OpenStack) or things that cannot be provided by Docker (like vSphere). We provide some ready-to-use cloud templates that are available for referencing by default. Terraform E2E tests are not run in our public CI as that would needlessly slow down builds. Read the reference for more information.","title":"Terraform"},{"location":"ddev/test/#mocker","text":"The mocker fixture is provided by the pytest-mock plugin. This fixture automatically restores anything that was mocked at the end of each test and is more ergonomic to use than stacking decorators or nesting context managers. Here's an example from their docs: def test_foo ( mocker ): # all valid calls mocker . patch ( 'os.remove' ) mocker . patch . object ( os , 'listdir' , autospec = True ) mocked_isfile = mocker . patch ( 'os.path.isfile' ) It also has many other nice features, like using pytest introspection when comparing calls.","title":"Mocker"},{"location":"ddev/test/#benchmarks","text":"The benchmark fixture is provided by the pytest-benchmark plugin. It enables the profiling of functions with the low-overhead cProfile module. It is quite useful for seeing the approximate time a given check takes to run, as well as gaining insight into any potential performance bottlenecks. You would use it like this: def test_large_payload ( benchmark , dd_run_check ): check = AwesomeCheck ( 'awesome' , {}, [ instance ]) # Run once to get any initialization out of the way. dd_run_check ( check ) benchmark ( dd_run_check , check ) To add benchmarks, define environments in tox.ini with bench somewhere in their names: [tox] ... envlist = ... bench ... [testenv:bench] By default, the test command skips all benchmark environments and tests. To run only benchmark environments and tests use the --bench / -b flag. The results are sorted by tottime , which is the total time spent in the given function (and excluding time made in calls to sub-functions).","title":"Benchmarks"},{"location":"ddev/test/#logs","text":"We provide an easy way to utilize log collection with E2E Docker environments . Pass mount_logs=True to docker_run . This will use the logs example in the integration's config spec . For example, the following defines 2 example log files: - template : logs example : - type : file path : /var/log/apache2/access.log source : apache service : apache - type : file path : /var/log/apache2/error.log source : apache service : apache Alternatives If mount_logs is a sequence of int , only the selected indices (starting at 1) will be used. So, using the Apache example above, to only monitor the error log you would set it to [2] . In lieu of a config spec, for whatever reason, you may set mount_logs to a dict containing the standard logs key. All requested log files are available to reference as environment variables for any Docker calls as DD_LOG_<LOG_CONFIG_INDEX> where the indices start at 1. volumes : - ${DD_LOG_1}:/usr/local/apache2/logs/access_log - ${DD_LOG_2}:/usr/local/apache2/logs/error_log When starting the environment, pass -e DD_LOGS_ENABLED=true to activate the Logs Agent.","title":"Logs"},{"location":"ddev/test/#reference","text":"","title":"Reference"},{"location":"ddev/test/#datadog_checks.dev.docker","text":"","title":"docker"},{"location":"ddev/test/#datadog_checks.dev.docker.compose_file_active","text":"Returns a bool indicating whether or not a compose file has any active services. Source code in 46 47 48 49 50 51 52 53 54 55 56 57 def compose_file_active ( compose_file ): \"\"\" Returns a `bool` indicating whether or not a compose file has any active services. \"\"\" command = [ 'docker-compose' , '-f' , compose_file , 'ps' ] lines = run_command ( command , capture = 'out' , check = True ) . stdout . splitlines () for i , line in enumerate ( lines , 1 ): if set ( line . strip ()) == { '-' }: return len ( lines [ i :]) >= 1 return False","title":"compose_file_active()"},{"location":"ddev/test/#datadog_checks.dev.docker.docker_run","text":"A convenient context manager for safely setting up and tearing down Docker environments. compose_file ( str ) - A path to a Docker compose file. A custom tear down is not required when using this. build ( bool ) - Whether or not to build images for when compose_file is provided service_name ( str ) - Optional name for when compose_file is provided up ( callable ) - A custom setup callable down ( callable ) - A custom tear down callable. This is required when using a custom setup. on_error ( callable ) - A callable called in case of an unhandled exception sleep ( float ) - Number of seconds to wait before yielding. This occurs after all conditions are successful. endpoints ( List[str]) ) - Endpoints to verify access for before yielding. Shorthand for adding CheckEndpoints(endpoints) to the conditions argument. log_patterns ( List[str|re.Pattern]) ) - Regular expression patterns to find in Docker logs before yielding. This is only available when compose_file is provided. Shorthand for adding CheckDockerLogs(compose_file, log_patterns) to the conditions argument. mount_logs ( bool ) - Whether or not to mount log files in Agent containers based on example logs configuration conditions ( callable ) - A list of callable objects that will be executed before yielding to check for errors env_vars ( dict ) - A dictionary to update os.environ with during execution wrappers ( List[callable] ) - A list of context managers to use during execution Source code in 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 @contextmanager def docker_run ( compose_file = None , build = False , service_name = None , up = None , down = None , on_error = None , sleep = None , endpoints = None , log_patterns = None , mount_logs = False , conditions = None , env_vars = None , wrappers = None , ): \"\"\" A convenient context manager for safely setting up and tearing down Docker environments. - **compose_file** (_str_) - A path to a Docker compose file. A custom tear down is not required when using this. - **build** (_bool_) - Whether or not to build images for when `compose_file` is provided - **service_name** (_str_) - Optional name for when ``compose_file`` is provided - **up** (_callable_) - A custom setup callable - **down** (_callable_) - A custom tear down callable. This is required when using a custom setup. - **on_error** (_callable_) - A callable called in case of an unhandled exception - **sleep** (_float_) - Number of seconds to wait before yielding. This occurs after all conditions are successful. - **endpoints** (_List[str])_) - Endpoints to verify access for before yielding. Shorthand for adding `CheckEndpoints(endpoints)` to the `conditions` argument. - **log_patterns** (_List[str|re.Pattern])_) - Regular expression patterns to find in Docker logs before yielding. This is only available when `compose_file` is provided. Shorthand for adding `CheckDockerLogs(compose_file, log_patterns)` to the `conditions` argument. - **mount_logs** (_bool_) - Whether or not to mount log files in Agent containers based on example logs configuration - **conditions** (_callable_) - A list of callable objects that will be executed before yielding to check for errors - **env_vars** (_dict_) - A dictionary to update `os.environ` with during execution - **wrappers** (_List[callable]_) - A list of context managers to use during execution \"\"\" if compose_file and up : raise TypeError ( 'You must select either a compose file or a custom setup callable, not both.' ) if compose_file is not None : if not isinstance ( compose_file , string_types ): raise TypeError ( 'The path to the compose file is not a string: {} ' . format ( repr ( compose_file ))) set_up = ComposeFileUp ( compose_file , build = build , service_name = service_name ) if down is not None : tear_down = down else : tear_down = ComposeFileDown ( compose_file ) if on_error is None : on_error = ComposeFileLogs ( compose_file ) else : set_up = up tear_down = down docker_conditions = [] if log_patterns is not None : if compose_file is None : raise ValueError ( 'The `log_patterns` convenience is unavailable when using ' 'a custom setup. Please use a custom condition instead.' ) docker_conditions . append ( CheckDockerLogs ( compose_file , log_patterns )) if conditions is not None : docker_conditions . extend ( conditions ) wrappers = list ( wrappers ) if wrappers is not None else [] if mount_logs : if isinstance ( mount_logs , dict ): wrappers . append ( shared_logs ( mount_logs [ 'logs' ])) # Easy mode, read example config else : # An extra level deep because of the context manager check_root = find_check_root ( depth = 2 ) example_log_configs = _read_example_logs_config ( check_root ) if mount_logs is True : wrappers . append ( shared_logs ( example_log_configs )) elif isinstance ( mount_logs , ( list , set )): wrappers . append ( shared_logs ( example_log_configs , mount_whitelist = mount_logs )) else : raise TypeError ( 'mount_logs: expected True, a list or a set, but got {} ' . format ( type ( mount_logs ) . __name__ ) ) with environment_run ( up = set_up , down = tear_down , on_error = on_error , sleep = sleep , endpoints = endpoints , conditions = docker_conditions , env_vars = env_vars , wrappers = wrappers , ) as result : yield result","title":"docker_run()"},{"location":"ddev/test/#datadog_checks.dev.docker.get_container_ip","text":"Get a Docker container's IP address from its ID or name. Source code in 31 32 33 34 35 36 37 38 39 40 41 42 43 def get_container_ip ( container_id_or_name ): \"\"\" Get a Docker container's IP address from its ID or name. \"\"\" command = [ 'docker' , 'inspect' , '-f' , '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' , container_id_or_name , ] return run_command ( command , capture = 'out' , check = True ) . stdout . strip ()","title":"get_container_ip()"},{"location":"ddev/test/#datadog_checks.dev.docker.get_docker_hostname","text":"Determine the hostname Docker uses based on the environment, defaulting to localhost . Source code in 24 25 26 27 28 def get_docker_hostname (): \"\"\" Determine the hostname Docker uses based on the environment, defaulting to `localhost`. \"\"\" return urlparse ( os . getenv ( 'DOCKER_HOST' , '' )) . hostname or 'localhost'","title":"get_docker_hostname()"},{"location":"ddev/test/#datadog_checks.dev.terraform","text":"","title":"terraform"},{"location":"ddev/test/#datadog_checks.dev.terraform.terraform_run","text":"A convenient context manager for safely setting up and tearing down Terraform environments. directory ( str ) - A path containing Terraform files sleep ( float ) - Number of seconds to wait before yielding. This occurs after all conditions are successful. endpoints ( List[str]) ) - Endpoints to verify access for before yielding. Shorthand for adding CheckEndpoints(endpoints) to the conditions argument. conditions ( callable ) - A list of callable objects that will be executed before yielding to check for errors env_vars ( dict ) - A dictionary to update os.environ with during execution wrappers ( List[callable] ) - A list of context managers to use during execution Source code in 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @contextmanager def terraform_run ( directory , sleep = None , endpoints = None , conditions = None , env_vars = None , wrappers = None ): \"\"\" A convenient context manager for safely setting up and tearing down Terraform environments. - **directory** (_str_) - A path containing Terraform files - **sleep** (_float_) - Number of seconds to wait before yielding. This occurs after all conditions are successful. - **endpoints** (_List[str])_) - Endpoints to verify access for before yielding. Shorthand for adding `CheckEndpoints(endpoints)` to the `conditions` argument. - **conditions** (_callable_) - A list of callable objects that will be executed before yielding to check for errors - **env_vars** (_dict_) - A dictionary to update `os.environ` with during execution - **wrappers** (_List[callable]_) - A list of context managers to use during execution \"\"\" if not which ( 'terraform' ): pytest . skip ( 'Terraform not available' ) set_up = TerraformUp ( directory ) tear_down = TerraformDown ( directory ) with environment_run ( up = set_up , down = tear_down , sleep = sleep , endpoints = endpoints , conditions = conditions , env_vars = env_vars , wrappers = wrappers , ) as result : yield result","title":"terraform_run()"},{"location":"faq/faq/","text":"FAQ Integration vs Check \u00b6 A Check is any integration whose execution is triggered directly in code by the Datadog Agent . Therefore, all Agent-based integrations written in Python or Go are considered Checks. Why test tests \u00b6 We track the coverage of tests in all cases. See pyca/pynacl !290 and !4280 .","title":"FAQ"},{"location":"faq/faq/#integration-vs-check","text":"A Check is any integration whose execution is triggered directly in code by the Datadog Agent . Therefore, all Agent-based integrations written in Python or Go are considered Checks.","title":"Integration vs Check"},{"location":"faq/faq/#why-test-tests","text":"We track the coverage of tests in all cases. See pyca/pynacl !290 and !4280 .","title":"Why test tests"},{"location":"guidelines/conventions/","text":"Conventions Stateful checks \u00b6 Since Agent v6, every instance of AgentCheck corresponds to a single YAML instance of an integration defined in the instances array of user configuration. As such, the instance argument the check method accepts is redundant and wasteful since you are parsing the same configuration at every run. Parse configuration once and save the results. Do this class AwesomeCheck ( AgentCheck ): def __init__ ( self , name , init_config , instances ): super ( AwesomeCheck , self ) . __init__ ( name , init_config , instances ) self . _server = self . instance . get ( 'server' , '' ) self . _port = int ( self . instance . get ( 'port' , 8080 )) self . _tags = list ( self . instance . get ( 'tags' , [])) self . _tags . append ( 'server: {} ' . format ( self . _server )) self . _tags . append ( 'port: {} ' . format ( self . _port )) def check ( self , _ ): ... Do NOT do this class AwesomeCheck ( AgentCheck ): def check ( self , instance ): server = instance . get ( 'server' , '' ) port = int ( instance . get ( 'port' , 8080 )) tags = list ( instance . get ( 'tags' , [])) tags . append ( 'server: {} ' . format ( server )) tags . append ( 'port: {} ' . format ( port )) ...","title":"Conventions"},{"location":"guidelines/conventions/#stateful-checks","text":"Since Agent v6, every instance of AgentCheck corresponds to a single YAML instance of an integration defined in the instances array of user configuration. As such, the instance argument the check method accepts is redundant and wasteful since you are parsing the same configuration at every run. Parse configuration once and save the results. Do this class AwesomeCheck ( AgentCheck ): def __init__ ( self , name , init_config , instances ): super ( AwesomeCheck , self ) . __init__ ( name , init_config , instances ) self . _server = self . instance . get ( 'server' , '' ) self . _port = int ( self . instance . get ( 'port' , 8080 )) self . _tags = list ( self . instance . get ( 'tags' , [])) self . _tags . append ( 'server: {} ' . format ( self . _server )) self . _tags . append ( 'port: {} ' . format ( self . _port )) def check ( self , _ ): ... Do NOT do this class AwesomeCheck ( AgentCheck ): def check ( self , instance ): server = instance . get ( 'server' , '' ) port = int ( instance . get ( 'port' , 8080 )) tags = list ( instance . get ( 'tags' , [])) tags . append ( 'server: {} ' . format ( server )) tags . append ( 'port: {} ' . format ( port )) ...","title":"Stateful checks"},{"location":"guidelines/pr/","text":"Pull requests","title":"Pull requests"},{"location":"guidelines/style/","text":"Style These are all the checkers used by our style enforcement . black \u00b6 An opinionated formatter, like JavaScript's prettier and Golang's gofmt . isort \u00b6 A tool to sort imports lexicographically, by section, and by type. We use the 5 standard sections: __future__ , stdlib, third party, first party, and local. datadog_checks is configured as a first party namespace. flake8 \u00b6 An easy-to-use wrapper around pycodestyle and pyflakes . We select everything it provides and only ignore a few things to give precedence to other tools. bugbear \u00b6 A flake8 plugin for finding likely bugs and design problems in programs. We enable: B001 : Do not use bare except: , it also catches unexpected events like memory errors, interrupts, system exit, and so on. Prefer except Exception: . B003 : Assigning to os.environ doesn't clear the environment. Subprocesses are going to see outdated variables, in disagreement with the current process. Use os.environ.clear() or the env= argument to Popen. B006 : Do not use mutable data structures for argument defaults. All calls reuse one instance of that data structure, persisting changes between them. B007 : Loop control variable not used within the loop body. If this is intended, start the name with an underscore. B301 : Python 3 does not include .iter* methods on dictionaries. The default behavior is to return iterables. Simply remove the iter prefix from the method. For Python 2 compatibility, also prefer the Python 3 equivalent if you expect that the size of the dict to be small and bounded. The performance regression on Python 2 will be negligible and the code is going to be the clearest. Alternatively, use six.iter* . B305 : .next() is not a thing on Python 3. Use the next() builtin. For Python 2 compatibility, use six.next() . B306 : BaseException.message has been deprecated as of Python 2.6 and is removed in Python 3. Use str(e) to access the user-readable message. Use e.args to access arguments passed to the exception. B902 : Invalid first argument used for method. Use self for instance methods, and cls for class methods. logging-format \u00b6 A flake8 plugin for ensuring a consistent logging format. We enable: G001 : Logging statements should not use string.format() for their first argument G002 : Logging statements should not use % formatting for their first argument G003 : Logging statements should not use + concatenation for their first argument G004 : Logging statements should not use f\"...\" for their first argument (only in Python 3.6+) G010 : Logging statements should not use warn (use warning instead) G100 : Logging statements should not use extra arguments unless whitelisted G201 : Logging statements should not use error(..., exc_info=True) (use exception(...) instead) G202 : Logging statements should not use redundant exc_info=True in exception Mypy \u00b6 A type checker allowing a mix of dynamic and static typing. This is optional for now.","title":"Style"},{"location":"guidelines/style/#black","text":"An opinionated formatter, like JavaScript's prettier and Golang's gofmt .","title":"black"},{"location":"guidelines/style/#isort","text":"A tool to sort imports lexicographically, by section, and by type. We use the 5 standard sections: __future__ , stdlib, third party, first party, and local. datadog_checks is configured as a first party namespace.","title":"isort"},{"location":"guidelines/style/#flake8","text":"An easy-to-use wrapper around pycodestyle and pyflakes . We select everything it provides and only ignore a few things to give precedence to other tools.","title":"flake8"},{"location":"guidelines/style/#bugbear","text":"A flake8 plugin for finding likely bugs and design problems in programs. We enable: B001 : Do not use bare except: , it also catches unexpected events like memory errors, interrupts, system exit, and so on. Prefer except Exception: . B003 : Assigning to os.environ doesn't clear the environment. Subprocesses are going to see outdated variables, in disagreement with the current process. Use os.environ.clear() or the env= argument to Popen. B006 : Do not use mutable data structures for argument defaults. All calls reuse one instance of that data structure, persisting changes between them. B007 : Loop control variable not used within the loop body. If this is intended, start the name with an underscore. B301 : Python 3 does not include .iter* methods on dictionaries. The default behavior is to return iterables. Simply remove the iter prefix from the method. For Python 2 compatibility, also prefer the Python 3 equivalent if you expect that the size of the dict to be small and bounded. The performance regression on Python 2 will be negligible and the code is going to be the clearest. Alternatively, use six.iter* . B305 : .next() is not a thing on Python 3. Use the next() builtin. For Python 2 compatibility, use six.next() . B306 : BaseException.message has been deprecated as of Python 2.6 and is removed in Python 3. Use str(e) to access the user-readable message. Use e.args to access arguments passed to the exception. B902 : Invalid first argument used for method. Use self for instance methods, and cls for class methods.","title":"bugbear"},{"location":"guidelines/style/#logging-format","text":"A flake8 plugin for ensuring a consistent logging format. We enable: G001 : Logging statements should not use string.format() for their first argument G002 : Logging statements should not use % formatting for their first argument G003 : Logging statements should not use + concatenation for their first argument G004 : Logging statements should not use f\"...\" for their first argument (only in Python 3.6+) G010 : Logging statements should not use warn (use warning instead) G100 : Logging statements should not use extra arguments unless whitelisted G201 : Logging statements should not use error(..., exc_info=True) (use exception(...) instead) G202 : Logging statements should not use redundant exc_info=True in exception","title":"logging-format"},{"location":"guidelines/style/#mypy","text":"A type checker allowing a mix of dynamic and static typing. This is optional for now.","title":"Mypy"},{"location":"meta/cd/","text":"CD","title":"CD"},{"location":"meta/ci/","text":"CI","title":"CI"},{"location":"meta/config_specs/","text":"Configuration","title":"Config specs"},{"location":"process/agent_release/","text":"Agent release A new minor version of the Agent is released every 6 weeks (approximately). Each release ships a snapshot of integrations-core . Setup \u00b6 Configure your GitHub and Trello auth. Freeze \u00b6 At midnight (EDT/EST) on the Friday before QA week we freeze, at which point the release manager will release all integrations with pending changes then branch off. Release \u00b6 Make a pull request to release any new integrations , then merge it and pull master Make a pull request to release all changed integrations , then merge it and pull master Branch \u00b6 Create a branch based on master named after the highest version of the Agent being released in the form <MAJOR>.<MINOR>.x Push the branch to GitHub Tag \u00b6 Run: git tag <MAJOR>.<MINOR>.0-rc.1 -m <MAJOR>.<MINOR>.0-rc.1 git push origin <MAJOR>.<MINOR>.0-rc.1 QA week \u00b6 We test all changes to integrations that were introduced since the last release. Create items \u00b6 Create an item for every change in our board using the ddev release testable command. For example: ddev release testable 7.17.1 7.18.0-rc.1 would select all commits that were merged between the Git references. The command will display each change and prompt you to assign a team or skip. Purely documentation changes are automatically skipped. You must assign each item to a team member after creation and ensure no one is assigned to a change that they authored. If you would like to automate this, then add a trello_users_$team table in your configuration , with keys being GitHub usernames and values being their corresponding Trello IDs (not names). You can find current team member information in this document . Release candidates \u00b6 The main Agent release manager will increment and build a new rc every day a bug fix needs to be tested until all QA is complete. Before each build is triggered: Merge any fixes that have been approved, then pull master Release all changed integrations with the exception of datadog_checks_dev For each fix merged, you must cherry-pick to the branch : The commit to master itself The release commit, so the shipped versions match the individually released integrations After all fixes have been cherry-picked: Push the changes to GitHub Tag with the appropriate rc number even if there were no changes Communication \u00b6 Update the #agent-release-sync channel with a daily status. #agent-integrations status: - TODO: X - In progress: X - Issues found: X - Awaiting build: X Release week \u00b6 After QA week ends the code freeze is lifted, even if there are items yet to be tested. The release manager will continue the same process outlined above. Finalize \u00b6 On the day of the final stable release, tag the branch with <MAJOR>.<MINOR>.0 . After the main Agent release manager confirms successful deployment to a few targets, create a branch based on master and run: ddev agent changelog ddev agent integrations Create a pull request and wait for approval before merging.","title":"Agent release"},{"location":"process/agent_release/#setup","text":"Configure your GitHub and Trello auth.","title":"Setup"},{"location":"process/agent_release/#freeze","text":"At midnight (EDT/EST) on the Friday before QA week we freeze, at which point the release manager will release all integrations with pending changes then branch off.","title":"Freeze"},{"location":"process/agent_release/#release","text":"Make a pull request to release any new integrations , then merge it and pull master Make a pull request to release all changed integrations , then merge it and pull master","title":"Release"},{"location":"process/agent_release/#branch","text":"Create a branch based on master named after the highest version of the Agent being released in the form <MAJOR>.<MINOR>.x Push the branch to GitHub","title":"Branch"},{"location":"process/agent_release/#tag","text":"Run: git tag <MAJOR>.<MINOR>.0-rc.1 -m <MAJOR>.<MINOR>.0-rc.1 git push origin <MAJOR>.<MINOR>.0-rc.1","title":"Tag"},{"location":"process/agent_release/#qa-week","text":"We test all changes to integrations that were introduced since the last release.","title":"QA week"},{"location":"process/agent_release/#create-items","text":"Create an item for every change in our board using the ddev release testable command. For example: ddev release testable 7.17.1 7.18.0-rc.1 would select all commits that were merged between the Git references. The command will display each change and prompt you to assign a team or skip. Purely documentation changes are automatically skipped. You must assign each item to a team member after creation and ensure no one is assigned to a change that they authored. If you would like to automate this, then add a trello_users_$team table in your configuration , with keys being GitHub usernames and values being their corresponding Trello IDs (not names). You can find current team member information in this document .","title":"Create items"},{"location":"process/agent_release/#release-candidates","text":"The main Agent release manager will increment and build a new rc every day a bug fix needs to be tested until all QA is complete. Before each build is triggered: Merge any fixes that have been approved, then pull master Release all changed integrations with the exception of datadog_checks_dev For each fix merged, you must cherry-pick to the branch : The commit to master itself The release commit, so the shipped versions match the individually released integrations After all fixes have been cherry-picked: Push the changes to GitHub Tag with the appropriate rc number even if there were no changes","title":"Release candidates"},{"location":"process/agent_release/#communication","text":"Update the #agent-release-sync channel with a daily status. #agent-integrations status: - TODO: X - In progress: X - Issues found: X - Awaiting build: X","title":"Communication"},{"location":"process/agent_release/#release-week","text":"After QA week ends the code freeze is lifted, even if there are items yet to be tested. The release manager will continue the same process outlined above.","title":"Release week"},{"location":"process/agent_release/#finalize","text":"On the day of the final stable release, tag the branch with <MAJOR>.<MINOR>.0 . After the main Agent release manager confirms successful deployment to a few targets, create a branch based on master and run: ddev agent changelog ddev agent integrations Create a pull request and wait for approval before merging.","title":"Finalize"},{"location":"process/integration_release/","text":"Integration release Each Agent integration has its own release cycle. Many integrations are actively developed and released often while some are rarely touched (usually indicating feature-completeness). Versioning \u00b6 All releases adhere to Semantic Versioning . Tags in the form <INTEGRATION_NAME>-<VERSION> are added to the Git repository. Therefore, it's possible to checkout and build the code for a certain version of a specific check. Setup \u00b6 Configure your GitHub auth. Identify changes \u00b6 Note If you already know which integration you'd like to release, skip this section. To see all checks that need to be released, run ddev release show ready . Steps \u00b6 Checkout and pull the most recent version of the master branch. git checkout master git pull Important Not using the latest version of master may cause errors in the build pipeline . Review which PRs were merged in between the latest release and the master branch. ddev release show changes <INTEGRATION> You should ensure that PR titles and changelog labels are correct. Create a release branch from master (suggested naming format is <USERNAME>/release-<INTEGRATION_NAME> ). This has the purpose of opening a PR so others can review the changelog. Important It is critical the branch name is not in the form <USERNAME>/<INTEGRATION_NAME>-<NEW_VERSION> because one of our Gitlab jobs is triggered whenever a Git reference matches that pattern, see !3843 & !3980 . Make the release. ddev release make <INTEGRATION> You may need to touch your Yubikey multiple times. This will automatically: update the version in <INTEGRATION>/datadog_checks/<INTEGRATION>/__about__.py update the changelog update the requirements-agent-release.txt file update in-toto metadata commit the above changes Push your branch to GitHub and create a pull request. Update the title of the PR to something like [Release] Bumped <INTEGRATION> version to <VERSION> . Ask for a review in Slack. Merge the pull request after approval. PyPI \u00b6 If you released datadog_checks_base or datadog_checks_dev then you will need to upload to PyPI for use by integrations-extras . ddev release upload datadog_checks_[base|dev] Metadata \u00b6 You need to run certain jobs if any changes modified integration metadata. See the Declarative Integration Pipeline wiki. Bulk releases \u00b6 To create a release for every integration that has changed, use all as the integration name in the ddev release make step above. ddev release make all You may also pass a comma-separated list of checks to skip using the --exclude option, e.g.: ddev release make all --exclude datadog_checks_dev Warning There is a known GitHub limitation where if an issue has too many labels (100), its state cannot be modified. If you cannot merge the pull request: Run the remove-labels command After merging, manually add back the changelog/no-changelog label Betas \u00b6 Creating pre-releases is the same workflow except you do not open a pull request but rather release directly from a branch. In the ddev release make step set --version to [major|minor|patch],[rc|alpha|beta] . For example, if the current version of an integration is 1.1.3 , the following command will bump it to 1.2.0-rc.1 : ddev release make <INTEGRATION> --version minor,rc After pushing the release commits to GitHub, run: ddev release tag <INTEGRATION> This manually triggers the build pipeline . To increment the version, omit the first part, e.g.: ddev release make <INTEGRATION> --version rc New integrations \u00b6 To bump a new integration to 1.0.0 if it is not already there, run: ddev release make <INTEGRATION> --new To ensure this for all integrations, run: ddev release make all --new If a release was created, run: ddev agent requirements Troubleshooting \u00b6 If you encounter errors when signing with your Yubikey, ensure you ran gpg --import <YOUR_KEY_ID>.gpg.pub . If the build pipeline failed, it is likely that you modified a file in the pull request without re-signing. To resolve this, you'll need to bootstrap metadata for every integration: Checkout and pull the most recent version of the master branch. git checkout master git pull Sign everything. ddev release make all --sign-only You may need to touch your Yubikey multiple times. Push your branch to GitHub. Manually trigger a build. git tag <USERNAME>bootstrap-1.0.0 -m <USERNAME>bootstrap-1.0.0 The tag name is irrelevant, it just needs to look like an integration release. Gitlab doesn't sync deleted tags, so any subsequent manual trigger tags will need to increment the version number. Delete the branch and tag, locally and on GitHub. Releasers \u00b6 For whom it may concern, the following is a list of GPG public key fingerprints known to correspond to developers who, at the time of writing (28-02-2020), can trigger a build by signing in-toto metadata . Christine Chen 57CE 2495 EA48 D456 B9C4 BA4F 66E8 2239 9141 D9D3 36C0 82E7 38C7 B4A1 E169 11C0 D633 59C4 875A 1A9A Dave Coleman 8278 C406 C1BB F1F2 DFBB 5AD6 0AE7 E246 4F8F D375 98A5 37CD CCA2 8DFF B35B 0551 5D50 0742 90F6 422F Mike Garabedian F90C 0097 67F2 4B27 9DC2 C83D A227 6601 6CB4 CF1D 2669 6E67 28D2 0CB0 C1E0 D2BE 6643 5756 8398 9306 Thomas Herv\u00e9 59DB 2532 75A5 BD4E 55C7 C5AA 0678 55A2 8E90 3B3B E2BD 994F 95C0 BC0B B923 1D21 F752 1EC8 F485 90D0 Ofek Lev C295 CF63 B355 DFEB 3316 02F7 F426 A944 35BE 6F99 D009 8861 8057 D2F4 D855 5A62 B472 442C B7D3 AF42 Florimond Manca B023 B02A 0331 9CD8 D19A 4328 83ED 89A4 5548 48FC 0992 11D9 AA67 D21E 7098 7B59 7C7D CB06 C9F2 0C13 Julia Simon 4A54 09A2 3361 109C 047C C76A DC8A 42C2 8B95 0123 129A 26CF A726 3C85 98A6 94B0 8659 1366 CBA1 BF3C Florian Veaux 3109 1C85 5D78 7789 93E5 0348 9BFE 5299 D02F 83E9 7A73 0C5E 48B0 6986 1045 CF8B 8B2D 16D6 5DE4 C95E Alexandre Yang FBC6 3AE0 9D0C A9B4 584C 9D7F 4291 A11A 36EA 52CD F8D9 181D 9309 F8A4 957D 636A 27F8 F48B 18AE 91AA","title":"Integration release"},{"location":"process/integration_release/#versioning","text":"All releases adhere to Semantic Versioning . Tags in the form <INTEGRATION_NAME>-<VERSION> are added to the Git repository. Therefore, it's possible to checkout and build the code for a certain version of a specific check.","title":"Versioning"},{"location":"process/integration_release/#setup","text":"Configure your GitHub auth.","title":"Setup"},{"location":"process/integration_release/#identify-changes","text":"Note If you already know which integration you'd like to release, skip this section. To see all checks that need to be released, run ddev release show ready .","title":"Identify changes"},{"location":"process/integration_release/#steps","text":"Checkout and pull the most recent version of the master branch. git checkout master git pull Important Not using the latest version of master may cause errors in the build pipeline . Review which PRs were merged in between the latest release and the master branch. ddev release show changes <INTEGRATION> You should ensure that PR titles and changelog labels are correct. Create a release branch from master (suggested naming format is <USERNAME>/release-<INTEGRATION_NAME> ). This has the purpose of opening a PR so others can review the changelog. Important It is critical the branch name is not in the form <USERNAME>/<INTEGRATION_NAME>-<NEW_VERSION> because one of our Gitlab jobs is triggered whenever a Git reference matches that pattern, see !3843 & !3980 . Make the release. ddev release make <INTEGRATION> You may need to touch your Yubikey multiple times. This will automatically: update the version in <INTEGRATION>/datadog_checks/<INTEGRATION>/__about__.py update the changelog update the requirements-agent-release.txt file update in-toto metadata commit the above changes Push your branch to GitHub and create a pull request. Update the title of the PR to something like [Release] Bumped <INTEGRATION> version to <VERSION> . Ask for a review in Slack. Merge the pull request after approval.","title":"Steps"},{"location":"process/integration_release/#pypi","text":"If you released datadog_checks_base or datadog_checks_dev then you will need to upload to PyPI for use by integrations-extras . ddev release upload datadog_checks_[base|dev]","title":"PyPI"},{"location":"process/integration_release/#metadata","text":"You need to run certain jobs if any changes modified integration metadata. See the Declarative Integration Pipeline wiki.","title":"Metadata"},{"location":"process/integration_release/#bulk-releases","text":"To create a release for every integration that has changed, use all as the integration name in the ddev release make step above. ddev release make all You may also pass a comma-separated list of checks to skip using the --exclude option, e.g.: ddev release make all --exclude datadog_checks_dev Warning There is a known GitHub limitation where if an issue has too many labels (100), its state cannot be modified. If you cannot merge the pull request: Run the remove-labels command After merging, manually add back the changelog/no-changelog label","title":"Bulk releases"},{"location":"process/integration_release/#betas","text":"Creating pre-releases is the same workflow except you do not open a pull request but rather release directly from a branch. In the ddev release make step set --version to [major|minor|patch],[rc|alpha|beta] . For example, if the current version of an integration is 1.1.3 , the following command will bump it to 1.2.0-rc.1 : ddev release make <INTEGRATION> --version minor,rc After pushing the release commits to GitHub, run: ddev release tag <INTEGRATION> This manually triggers the build pipeline . To increment the version, omit the first part, e.g.: ddev release make <INTEGRATION> --version rc","title":"Betas"},{"location":"process/integration_release/#new-integrations","text":"To bump a new integration to 1.0.0 if it is not already there, run: ddev release make <INTEGRATION> --new To ensure this for all integrations, run: ddev release make all --new If a release was created, run: ddev agent requirements","title":"New integrations"},{"location":"process/integration_release/#troubleshooting","text":"If you encounter errors when signing with your Yubikey, ensure you ran gpg --import <YOUR_KEY_ID>.gpg.pub . If the build pipeline failed, it is likely that you modified a file in the pull request without re-signing. To resolve this, you'll need to bootstrap metadata for every integration: Checkout and pull the most recent version of the master branch. git checkout master git pull Sign everything. ddev release make all --sign-only You may need to touch your Yubikey multiple times. Push your branch to GitHub. Manually trigger a build. git tag <USERNAME>bootstrap-1.0.0 -m <USERNAME>bootstrap-1.0.0 The tag name is irrelevant, it just needs to look like an integration release. Gitlab doesn't sync deleted tags, so any subsequent manual trigger tags will need to increment the version number. Delete the branch and tag, locally and on GitHub.","title":"Troubleshooting"},{"location":"process/integration_release/#releasers","text":"For whom it may concern, the following is a list of GPG public key fingerprints known to correspond to developers who, at the time of writing (28-02-2020), can trigger a build by signing in-toto metadata . Christine Chen 57CE 2495 EA48 D456 B9C4 BA4F 66E8 2239 9141 D9D3 36C0 82E7 38C7 B4A1 E169 11C0 D633 59C4 875A 1A9A Dave Coleman 8278 C406 C1BB F1F2 DFBB 5AD6 0AE7 E246 4F8F D375 98A5 37CD CCA2 8DFF B35B 0551 5D50 0742 90F6 422F Mike Garabedian F90C 0097 67F2 4B27 9DC2 C83D A227 6601 6CB4 CF1D 2669 6E67 28D2 0CB0 C1E0 D2BE 6643 5756 8398 9306 Thomas Herv\u00e9 59DB 2532 75A5 BD4E 55C7 C5AA 0678 55A2 8E90 3B3B E2BD 994F 95C0 BC0B B923 1D21 F752 1EC8 F485 90D0 Ofek Lev C295 CF63 B355 DFEB 3316 02F7 F426 A944 35BE 6F99 D009 8861 8057 D2F4 D855 5A62 B472 442C B7D3 AF42 Florimond Manca B023 B02A 0331 9CD8 D19A 4328 83ED 89A4 5548 48FC 0992 11D9 AA67 D21E 7098 7B59 7C7D CB06 C9F2 0C13 Julia Simon 4A54 09A2 3361 109C 047C C76A DC8A 42C2 8B95 0123 129A 26CF A726 3C85 98A6 94B0 8659 1366 CBA1 BF3C Florian Veaux 3109 1C85 5D78 7789 93E5 0348 9BFE 5299 D02F 83E9 7A73 0C5E 48B0 6986 1045 CF8B 8B2D 16D6 5DE4 C95E Alexandre Yang FBC6 3AE0 9D0C A9B4 584C 9D7F 4291 A11A 36EA 52CD F8D9 181D 9309 F8A4 957D 636A 27F8 F48B 18AE 91AA","title":"Releasers"}]}